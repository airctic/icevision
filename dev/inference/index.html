
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for IceVision.">
      
      
      
        <meta name="author" content="Lucas Goulart Vazquez, Farid Hassainia, and Contributors">
      
      
        <link rel="canonical" href="https://airctic.com/dev/inference/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-7.3.6">
    
    
      
        <title>Inference - IceVision</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../css/termynal.css">
    
      <link rel="stylesheet" href="../css/custom.css">
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="blue">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#inference-using-icevision" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="IceVision" class="md-header__button md-logo" aria-label="IceVision" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            IceVision
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Inference
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/airctic/IceVision" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="IceVision" class="md-nav__button md-logo" aria-label="IceVision" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    IceVision
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/airctic/IceVision" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../install/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Getting Started
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Getting Started" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Getting Started
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../getting_started_object_detection/" class="md-nav__link">
        Object Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../getting_started_instance_segmentation/" class="md-nav__link">
        Instance Segmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../getting_started_semantic_segmentation/" class="md-nav__link">
        Semantic Segmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../getting_started_keypoint_detection/" class="md-nav__link">
        Keypoint Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../custom_parser/" class="md-nav__link">
        Custom Parser
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Inference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Inference
      </a>
      
        


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#install-icevision" class="md-nav__link">
    Install IceVision
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#list-of-images-for-inference" class="md-nav__link">
    List of images for inference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-a-checkpoint-and-creating-the-corresponding-model" class="md-nav__link">
    Loading a checkpoint and creating the corresponding model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#get-model-object" class="md-nav__link">
    Get Model Object
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#single-image-inference" class="md-nav__link">
    Single Image Inference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#batch-inference" class="md-nav__link">
    Batch Inference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-export-inferences-as-coco-annotations" class="md-nav__link">
    How to export inferences as COCO annotations
  </a>
  
    <nav class="md-nav" aria-label="How to export inferences as COCO annotations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preview-predictions-on-original-image" class="md-nav__link">
    Preview predictions on original image:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-save-a-model-and-its-metadata-in-icevision" class="md-nav__link">
    How to save a model and its metadata in IceVision
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-models-already-containing-metadata" class="md-nav__link">
    Loading models already containing metadata
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#happy-learning" class="md-nav__link">
    Happy Learning!
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Other Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Other Tutorials" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Other Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../wandb_efficientdet/" class="md-nav__link">
        Model Tracking Using Wandb
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../negative_samples/" class="md-nav__link">
        How to use negative samples
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../voc_predefined_splits/" class="md-nav__link">
        Fixed Splitter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../progressive_resizing/" class="md-nav__link">
        Progressive resizing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mmdet_custom_config/" class="md-nav__link">
        MMDetection Custom Config
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../plot_top_losses/" class="md-nav__link">
        Model Debugging with Plot Top Losses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ochuman_keypoint_detection/" class="md-nav__link">
        Advanced Keypoint Detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../SAHI_inference/" class="md-nav__link">
        Small Object Detection with SAHI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../using_fiftyone_in_icevision/" class="md-nav__link">
        Using FiftyOne in icevision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../using_swin_transformer_as_backbone/" class="md-nav__link">
        Using Swin Transformer as Backbone
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Transforms
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transforms" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Transforms
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../albumentations/" class="md-nav__link">
        Albumentations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        Models
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          API Documentation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API Documentation" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          API Documentation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../parser/" class="md-nav__link">
        Parser
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../dataset/" class="md-nav__link">
        Dataset
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_3" type="checkbox" id="__nav_7_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7_3">
          Transforms
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transforms" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_3">
          <span class="md-nav__icon md-icon"></span>
          Transforms
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../albumentations_tfms/" class="md-nav__link">
        Albumentations
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_4" type="checkbox" id="__nav_7_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7_4">
          Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Models" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_4">
          <span class="md-nav__icon md-icon"></span>
          Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_4_1" type="checkbox" id="__nav_7_4_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7_4_1">
          Faster RCNN
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Faster RCNN" data-md-level="3">
        <label class="md-nav__title" for="__nav_7_4_1">
          <span class="md-nav__icon md-icon"></span>
          Faster RCNN
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../faster_rcnn/" class="md-nav__link">
        common
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../faster_rcnn_fastai/" class="md-nav__link">
        fastai
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../faster_rcnn_lightning/" class="md-nav__link">
        lightning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_4_2" type="checkbox" id="__nav_7_4_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7_4_2">
          Mask RCNN
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Mask RCNN" data-md-level="3">
        <label class="md-nav__title" for="__nav_7_4_2">
          <span class="md-nav__icon md-icon"></span>
          Mask RCNN
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mask_rcnn/" class="md-nav__link">
        common
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mask_rcnn_fastai/" class="md-nav__link">
        fastai
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mask_rcnn_lightning/" class="md-nav__link">
        lightning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_4_3" type="checkbox" id="__nav_7_4_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7_4_3">
          EfficientDet
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="EfficientDet" data-md-level="3">
        <label class="md-nav__title" for="__nav_7_4_3">
          <span class="md-nav__icon md-icon"></span>
          EfficientDet
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../efficientdet/" class="md-nav__link">
        common
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../efficientdet_fastai/" class="md-nav__link">
        fastai
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../efficientdet_lightning/" class="md-nav__link">
        lightning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data_splits/" class="md-nav__link">
        Data Splitters
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../contributing/" class="md-nav__link">
        Contributing Guide
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../readme_mkdocs/" class="md-nav__link">
        Generating Docs
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../code_of_conduct/" class="md-nav__link">
        Code of Conduct
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#install-icevision" class="md-nav__link">
    Install IceVision
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#list-of-images-for-inference" class="md-nav__link">
    List of images for inference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-a-checkpoint-and-creating-the-corresponding-model" class="md-nav__link">
    Loading a checkpoint and creating the corresponding model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#get-model-object" class="md-nav__link">
    Get Model Object
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#single-image-inference" class="md-nav__link">
    Single Image Inference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#batch-inference" class="md-nav__link">
    Batch Inference
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-export-inferences-as-coco-annotations" class="md-nav__link">
    How to export inferences as COCO annotations
  </a>
  
    <nav class="md-nav" aria-label="How to export inferences as COCO annotations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preview-predictions-on-original-image" class="md-nav__link">
    Preview predictions on original image:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-save-a-model-and-its-metadata-in-icevision" class="md-nav__link">
    How to save a model and its metadata in IceVision
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-models-already-containing-metadata" class="md-nav__link">
    Loading models already containing metadata
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#happy-learning" class="md-nav__link">
    Happy Learning!
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <p><a href="https://colab.research.google.com/github/airctic/icevision/blob/master/notebooks/inference.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1 id="inference-using-icevision">Inference using IceVision</h1>
<p><img alt="" src="https://raw.githubusercontent.com/airctic/icevision/master/images/fridge-objects.png" /></p>
<h2 id="install-icevision">Install IceVision</h2>
<p>The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well
as the fastai and pytorch lightning engines.</p>
<p>Install from pypi...</p>
<div class="highlight"><pre><span></span><code><span class="c1"># # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation</span>
<span class="c1"># !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh</span>

<span class="c1"># # Choose your installation target: cuda11 or cuda10 or cpu</span>
<span class="c1"># !bash icevision_install.sh cuda11</span>
</code></pre></div>
<p>... or from icevision master</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation</span>
<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">airctic</span><span class="o">/</span><span class="n">icevision</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">icevision_install</span><span class="o">.</span><span class="n">sh</span>

<span class="c1"># Choose your installation target: cuda11 or cuda10 or cpu</span>
<span class="err">!</span><span class="n">bash</span> <span class="n">icevision_install</span><span class="o">.</span><span class="n">sh</span> <span class="n">cuda11</span> <span class="n">master</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>--2022-08-12 18:38:33--  https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2820 (2.8K) [text/plain]
Saving to: ‘icevision_install.sh.2’
</code></pre></div>
</div>

<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>icevision_install.s 100%[===================&gt;]   2.75K  --.-KB/s    in 0s      
</code></pre></div>
</div>

<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>2022-08-12 18:38:33 (41.2 MB/s) - ‘icevision_install.sh.2’ saved [2820/2820]
</code></pre></div>
</div>

<div class="highlight"><pre><span></span><code><span class="c1"># Restart kernel after installation</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">Application</span><span class="o">.</span><span class="n">instance</span><span class="p">()</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">do_shutdown</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>Installing icevision + dependencices for cuda11
- Installing torch and its dependencies
Looking in links: https://download.pytorch.org/whl/torch_stable.html
Requirement already satisfied: torch==1.10.0+cu111 in /opt/conda/lib/python3.9/site-packages (1.10.0+cu111)
Requirement already satisfied: torchvision==0.11.1+cu111 in /opt/conda/lib/python3.9/site-packages (0.11.1+cu111)
Requirement already satisfied: torchtext==0.11.0 in /opt/conda/lib/python3.9/site-packages (0.11.0)
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.10.0+cu111) (4.3.0)
Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.1+cu111) (1.23.1)
Requirement already satisfied: pillow!=8.3.0,&gt;=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.1+cu111) (8.4.0)
Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchtext==0.11.0) (2.27.1)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from torchtext==0.11.0) (4.63.0)
Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-&gt;torchtext==0.11.0) (2.0.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.9/site-packages (from requests-&gt;torchtext==0.11.0) (3.3)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests-&gt;torchtext==0.11.0) (1.26.8)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests-&gt;torchtext==0.11.0) (2022.6.15)
[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
[0m- Installing mmcv
[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
[0m- Installing mmdet
[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
[0m- Installing mmseg
[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
[0m- Installing icevision from master
[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
[0m- Installing icedata from master
[33mWARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
[0m[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66)[0m[31m
[0m[31mERROR: No matching distribution found for opencv-python-headless==4.1.2.30[0m[31m
[0micevision installation finished!

{&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True}
</code></pre></div>
</div>
<h2 id="imports">Imports</h2>
<p>All of the IceVision components can be easily imported with a single line.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">icevision.all</span> <span class="kn">import</span> <span class="o">*</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>[1m[1mINFO    [0m[1m[0m - [1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf[0m | [36micevision.visualize.utils[0m:[36mget_default_font[0m:[36m67[0m
[1m[1mINFO    [0m[1m[0m - [1mDownloading mmdet configs[0m | [36micevision.models.mmdet.download_configs[0m:[36mdownload_mmdet_configs[0m:[36m31[0m

0B [00:00, ?B/s]

Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...

[1m[1mINFO    [0m[1m[0m - [1mDownloading mmseg configs[0m | [36micevision.models.mmseg.download_configs[0m:[36mdownload_mmseg_configs[0m:[36m33[0m

  0%|          | 0/334331 [00:00&lt;?, ?B/s]
</code></pre></div>
</div>
<h2 id="list-of-images-for-inference">List of images for inference</h2>
<p>Please store your images in a folder, and populate the <code>path_to_folder</code> variable with the corresponding folder name.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Pick your images folder</span>
<span class="n">path_to_image_folder</span> <span class="o">=</span> <span class="s2">&quot;./images&quot;</span>
<span class="n">img_files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path_to_image_folder</span><span class="p">)</span>
<span class="n">img_files</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">img</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_files</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">img</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>(#5) [Path(&#39;images/boy_skating.png&#39;),Path(&#39;images/clock_shop.jpeg&#39;),Path(&#39;images/donuts.jpeg&#39;),Path(&#39;images/toothbrushes.jpeg&#39;),Path(&#39;images/umbrellas.jpg&#39;)]
</code></pre></div>
</div>

<p><img alt="png" src="../images/inference/inference_12_0.png" /></p>
<h2 id="loading-a-checkpoint-and-creating-the-corresponding-model">Loading a checkpoint and creating the corresponding model</h2>
<p>The checkpoint file can be either a local file or an URL</p>
<div class="highlight"><pre><span></span><code><span class="c1"># checkpoint_path = &#39;checkpoints/fridge-retinanet-save-checkpoint-full.pth&#39;</span>

<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s1">&#39;http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth&#39;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">icevision.models</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># The model is automatically recreated in the evaluation mode. To unset that mode, you only need to pass `eval_mode=Fales`)</span>
<span class="n">checkpoint_and_model</span> <span class="o">=</span> <span class="n">model_from_checkpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> 
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;mmdet.retinanet&#39;</span><span class="p">,</span> 
    <span class="n">backbone_name</span><span class="o">=</span><span class="s1">&#39;resnet50_fpn_1x&#39;</span><span class="p">,</span>
    <span class="n">img_size</span><span class="o">=</span><span class="mi">640</span><span class="p">,</span> 
    <span class="n">is_coco</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth

2022-08-12 19:06:38,025 - mmcv - INFO - initialize ResNet with init_cfg {&#39;type&#39;: &#39;Pretrained&#39;, &#39;checkpoint&#39;: &#39;torchvision://resnet50&#39;}
2022-08-12 19:06:38,029 - mmcv - INFO - load model from: torchvision://resnet50
2022-08-12 19:06:38,030 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50
2022-08-12 19:06:38,124 - mmcv - WARNING - The model and loaded state dict do not match exactly
</code></pre></div>
</div>

<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>unexpected key in source state_dict: fc.weight, fc.bias
</code></pre></div>
</div>

<p><div class="highlight"><pre><span></span><code><span class="c1"># Just logging the info</span>
<span class="n">model_type</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;backbone&quot;</span><span class="p">]</span>
<span class="n">class_map</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;class_map&quot;</span><span class="p">]</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;img_size&quot;</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">model_type</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">backbone</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">class_map</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">img_size</span>
</code></pre></div></p>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>2022-08-12 19:06:38,142 - mmcv - INFO - initialize FPN with init_cfg {&#39;type&#39;: &#39;Xavier&#39;, &#39;layer&#39;: &#39;Conv2d&#39;, &#39;distribution&#39;: &#39;uniform&#39;}
2022-08-12 19:06:38,181 - mmcv - INFO - initialize RetinaHead with init_cfg {&#39;type&#39;: &#39;Normal&#39;, &#39;layer&#39;: &#39;Conv2d&#39;, &#39;std&#39;: 0.01, &#39;override&#39;: {&#39;type&#39;: &#39;Normal&#39;, &#39;name&#39;: &#39;retina_cls&#39;, &#39;std&#39;: 0.01, &#39;bias_prob&#39;: 0.01}}
2022-08-12 19:06:38,223 - mmcv - INFO - 
backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,223 - mmcv - INFO - 
backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,224 - mmcv - INFO - 
backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,224 - mmcv - INFO - 
backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,225 - mmcv - INFO - 
backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,225 - mmcv - INFO - 
backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,226 - mmcv - INFO - 
backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,226 - mmcv - INFO - 
backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,227 - mmcv - INFO - 
backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,227 - mmcv - INFO - 
backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,227 - mmcv - INFO - 
backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,227 - mmcv - INFO - 
backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,228 - mmcv - INFO - 
backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,228 - mmcv - INFO - 
backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,228 - mmcv - INFO - 
backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,229 - mmcv - INFO - 
backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,229 - mmcv - INFO - 
backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,229 - mmcv - INFO - 
backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,230 - mmcv - INFO - 
backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,230 - mmcv - INFO - 
backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,231 - mmcv - INFO - 
backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,231 - mmcv - INFO - 
backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,231 - mmcv - INFO - 
backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,231 - mmcv - INFO - 
backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,232 - mmcv - INFO - 
backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,232 - mmcv - INFO - 
backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,233 - mmcv - INFO - 
backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,233 - mmcv - INFO - 
backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,233 - mmcv - INFO - 
backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,233 - mmcv - INFO - 
backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,234 - mmcv - INFO - 
backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,234 - mmcv - INFO - 
backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,234 - mmcv - INFO - 
backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,235 - mmcv - INFO - 
backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,235 - mmcv - INFO - 
backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,235 - mmcv - INFO - 
backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,235 - mmcv - INFO - 
backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,236 - mmcv - INFO - 
backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,236 - mmcv - INFO - 
backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,236 - mmcv - INFO - 
backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,237 - mmcv - INFO - 
backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,237 - mmcv - INFO - 
backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,237 - mmcv - INFO - 
backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,237 - mmcv - INFO - 
backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,238 - mmcv - INFO - 
backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,238 - mmcv - INFO - 
backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,238 - mmcv - INFO - 
backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,239 - mmcv - INFO - 
backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,239 - mmcv - INFO - 
backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,239 - mmcv - INFO - 
backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,239 - mmcv - INFO - 
backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,240 - mmcv - INFO - 
backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,240 - mmcv - INFO - 
backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,240 - mmcv - INFO - 
backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,241 - mmcv - INFO - 
backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,241 - mmcv - INFO - 
backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,241 - mmcv - INFO - 
backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,242 - mmcv - INFO - 
backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,242 - mmcv - INFO - 
backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,242 - mmcv - INFO - 
backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,243 - mmcv - INFO - 
backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,243 - mmcv - INFO - 
backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,243 - mmcv - INFO - 
backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,243 - mmcv - INFO - 
backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,244 - mmcv - INFO - 
backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,244 - mmcv - INFO - 
backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,244 - mmcv - INFO - 
backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,245 - mmcv - INFO - 
backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,245 - mmcv - INFO - 
backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,245 - mmcv - INFO - 
backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,246 - mmcv - INFO - 
backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,246 - mmcv - INFO - 
backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,246 - mmcv - INFO - 
backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,246 - mmcv - INFO - 
backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,247 - mmcv - INFO - 
backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,247 - mmcv - INFO - 
backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,247 - mmcv - INFO - 
backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,248 - mmcv - INFO - 
backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,248 - mmcv - INFO - 
backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,248 - mmcv - INFO - 
backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,249 - mmcv - INFO - 
backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,249 - mmcv - INFO - 
backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,249 - mmcv - INFO - 
backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,250 - mmcv - INFO - 
backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,250 - mmcv - INFO - 
backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,250 - mmcv - INFO - 
backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,251 - mmcv - INFO - 
backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,251 - mmcv - INFO - 
backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,251 - mmcv - INFO - 
backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,252 - mmcv - INFO - 
backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,252 - mmcv - INFO - 
backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,253 - mmcv - INFO - 
backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,253 - mmcv - INFO - 
backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,254 - mmcv - INFO - 
backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,254 - mmcv - INFO - 
backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,254 - mmcv - INFO - 
backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,255 - mmcv - INFO - 
backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,255 - mmcv - INFO - 
backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,256 - mmcv - INFO - 
backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,256 - mmcv - INFO - 
backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,256 - mmcv - INFO - 
backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,256 - mmcv - INFO - 
backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,257 - mmcv - INFO - 
backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,257 - mmcv - INFO - 
backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,257 - mmcv - INFO - 
backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,258 - mmcv - INFO - 
backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,258 - mmcv - INFO - 
backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,259 - mmcv - INFO - 
backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,259 - mmcv - INFO - 
backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,259 - mmcv - INFO - 
backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,260 - mmcv - INFO - 
backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,260 - mmcv - INFO - 
backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,260 - mmcv - INFO - 
backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,261 - mmcv - INFO - 
backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,261 - mmcv - INFO - 
backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,261 - mmcv - INFO - 
backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,262 - mmcv - INFO - 
backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,262 - mmcv - INFO - 
backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,263 - mmcv - INFO - 
backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,263 - mmcv - INFO - 
backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,263 - mmcv - INFO - 
backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,264 - mmcv - INFO - 
backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,264 - mmcv - INFO - 
backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,264 - mmcv - INFO - 
backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,265 - mmcv - INFO - 
backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,265 - mmcv - INFO - 
backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,265 - mmcv - INFO - 
backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,266 - mmcv - INFO - 
backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,266 - mmcv - INFO - 
backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,267 - mmcv - INFO - 
backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,267 - mmcv - INFO - 
backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,267 - mmcv - INFO - 
backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,268 - mmcv - INFO - 
backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,268 - mmcv - INFO - 
backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,269 - mmcv - INFO - 
backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,269 - mmcv - INFO - 
backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,269 - mmcv - INFO - 
backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,269 - mmcv - INFO - 
backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,270 - mmcv - INFO - 
backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,270 - mmcv - INFO - 
backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,270 - mmcv - INFO - 
backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,271 - mmcv - INFO - 
backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,271 - mmcv - INFO - 
backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,271 - mmcv - INFO - 
backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,272 - mmcv - INFO - 
backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,272 - mmcv - INFO - 
backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,272 - mmcv - INFO - 
backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,273 - mmcv - INFO - 
backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,273 - mmcv - INFO - 
backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,273 - mmcv - INFO - 
backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,274 - mmcv - INFO - 
backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,274 - mmcv - INFO - 
backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,274 - mmcv - INFO - 
backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,274 - mmcv - INFO - 
backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,275 - mmcv - INFO - 
backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,275 - mmcv - INFO - 
backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,275 - mmcv - INFO - 
backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,276 - mmcv - INFO - 
backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,276 - mmcv - INFO - 
backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 19:06:38,276 - mmcv - INFO - 
neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,277 - mmcv - INFO - 
neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,277 - mmcv - INFO - 
neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,277 - mmcv - INFO - 
neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,278 - mmcv - INFO - 
neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,278 - mmcv - INFO - 
neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,279 - mmcv - INFO - 
neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,279 - mmcv - INFO - 
neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,279 - mmcv - INFO - 
neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,280 - mmcv - INFO - 
neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,280 - mmcv - INFO - 
neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,280 - mmcv - INFO - 
neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,281 - mmcv - INFO - 
neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,281 - mmcv - INFO - 
neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,281 - mmcv - INFO - 
neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 19:06:38,281 - mmcv - INFO - 
neck.fpn_convs.4.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,282 - mmcv - INFO - 
bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,282 - mmcv - INFO - 
bbox_head.cls_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,283 - mmcv - INFO - 
bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,283 - mmcv - INFO - 
bbox_head.cls_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,283 - mmcv - INFO - 
bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,283 - mmcv - INFO - 
bbox_head.cls_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,284 - mmcv - INFO - 
bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,284 - mmcv - INFO - 
bbox_head.cls_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,284 - mmcv - INFO - 
bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,285 - mmcv - INFO - 
bbox_head.reg_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,285 - mmcv - INFO - 
bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,285 - mmcv - INFO - 
bbox_head.reg_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,286 - mmcv - INFO - 
bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,286 - mmcv - INFO - 
bbox_head.reg_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,286 - mmcv - INFO - 
bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,287 - mmcv - INFO - 
bbox_head.reg_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 19:06:38,287 - mmcv - INFO - 
bbox_head.retina_cls.weight - torch.Size([720, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

2022-08-12 19:06:38,287 - mmcv - INFO - 
bbox_head.retina_cls.bias - torch.Size([720]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

2022-08-12 19:06:38,288 - mmcv - INFO - 
bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 19:06:38,288 - mmcv - INFO - 
bbox_head.retina_reg.bias - torch.Size([36]): 
NormalInit: mean=0, std=0.01, bias=0 


&lt;module &#39;icevision.models.mmdet.models.retinanet&#39; from &#39;/notebooks/icevision/models/mmdet/models/retinanet/__init__.py&#39;&gt;

&lt;icevision.models.mmdet.models.retinanet.backbones.backbone_config.MMDetRetinanetBackboneConfig at 0x7fad702b70d0&gt;

&lt;ClassMap: {&#39;background&#39;: 0, &#39;person&#39;: 1, &#39;bicycle&#39;: 2, &#39;car&#39;: 3, &#39;motorcycle&#39;: 4, &#39;airplane&#39;: 5, &#39;bus&#39;: 6, &#39;train&#39;: 7, &#39;truck&#39;: 8, &#39;boat&#39;: 9, &#39;traffic light&#39;: 10, &#39;fire hydrant&#39;: 11, &#39;stop sign&#39;: 12, &#39;parking meter&#39;: 13, &#39;bench&#39;: 14, &#39;bird&#39;: 15, &#39;cat&#39;: 16, &#39;dog&#39;: 17, &#39;horse&#39;: 18, &#39;sheep&#39;: 19, &#39;cow&#39;: 20, &#39;elephant&#39;: 21, &#39;bear&#39;: 22, &#39;zebra&#39;: 23, &#39;giraffe&#39;: 24, &#39;backpack&#39;: 25, &#39;umbrella&#39;: 26, &#39;handbag&#39;: 27, &#39;tie&#39;: 28, &#39;suitcase&#39;: 29, &#39;frisbee&#39;: 30, &#39;skis&#39;: 31, &#39;snowboard&#39;: 32, &#39;sports ball&#39;: 33, &#39;kite&#39;: 34, &#39;baseball bat&#39;: 35, &#39;baseball glove&#39;: 36, &#39;skateboard&#39;: 37, &#39;surfboard&#39;: 38, &#39;tennis racket&#39;: 39, &#39;bottle&#39;: 40, &#39;wine glass&#39;: 41, &#39;cup&#39;: 42, &#39;fork&#39;: 43, &#39;knife&#39;: 44, &#39;spoon&#39;: 45, &#39;bowl&#39;: 46, &#39;banana&#39;: 47, &#39;apple&#39;: 48, &#39;sandwich&#39;: 49, &#39;orange&#39;: 50, &#39;broccoli&#39;: 51, &#39;carrot&#39;: 52, &#39;hot dog&#39;: 53, &#39;pizza&#39;: 54, &#39;donut&#39;: 55, &#39;cake&#39;: 56, &#39;chair&#39;: 57, &#39;couch&#39;: 58, &#39;potted plant&#39;: 59, &#39;bed&#39;: 60, &#39;dining table&#39;: 61, &#39;toilet&#39;: 62, &#39;tv&#39;: 63, &#39;laptop&#39;: 64, &#39;mouse&#39;: 65, &#39;remote&#39;: 66, &#39;keyboard&#39;: 67, &#39;cell phone&#39;: 68, &#39;microwave&#39;: 69, &#39;oven&#39;: 70, &#39;toaster&#39;: 71, &#39;sink&#39;: 72, &#39;refrigerator&#39;: 73, &#39;book&#39;: 74, &#39;clock&#39;: 75, &#39;vase&#39;: 76, &#39;scissors&#39;: 77, &#39;teddy bear&#39;: 78, &#39;hair drier&#39;: 79, &#39;toothbrush&#39;: 80}&gt;

640
</code></pre></div>
</div>
<h2 id="get-model-object">Get Model Object</h2>
<p><code>model_from_checkpoint(checkpoint_path)</code> returns a dictionary: <code>checkpoint_and_model</code>. The model object is stored in <code>checkpoint_and_model["model"]</code>. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Get model object</span>
<span class="c1"># The model is automatically set in the evaluation mode</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span>

<span class="c1"># Check device</span>
<span class="n">device</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
<span class="n">device</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Transforms</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;img_size&quot;</span><span class="p">]</span>
<span class="n">valid_tfms</span> <span class="o">=</span> <span class="n">tfms</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">Adapter</span><span class="p">([</span><span class="o">*</span><span class="n">tfms</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">resize_and_pad</span><span class="p">(</span><span class="n">img_size</span><span class="p">),</span> <span class="n">tfms</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">Normalize</span><span class="p">()])</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>device(type=&#39;cpu&#39;)
</code></pre></div>
</div>
<h2 id="single-image-inference">Single Image Inference</h2>
<p>The <code>end2end_detect()</code> not only compute predictions for a single image but also automatically adjust predicted boxes to the original image size</p>
<div class="highlight"><pre><span></span><code><span class="n">img</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_files</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">pred_dict</span>  <span class="o">=</span> <span class="n">model_type</span><span class="o">.</span><span class="n">end2end_detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">valid_tfms</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">class_map</span><span class="o">=</span><span class="n">class_map</span><span class="p">,</span> <span class="n">detection_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">pred_dict</span><span class="p">[</span><span class="s1">&#39;img&#39;</span><span class="p">]</span>
</code></pre></div>
<p><img alt="png" src="../images/inference/inference_25_0.png" /></p>
<h2 id="batch-inference">Batch Inference</h2>
<p>The following option shows to do generate inference for a set of images. The latter is processed in batches.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Create a dataset</span>
<span class="n">imgs_array</span> <span class="o">=</span> <span class="p">[</span><span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">fname</span><span class="p">))</span> <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">img_files</span><span class="p">]</span>
<span class="n">infer_ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_images</span><span class="p">(</span><span class="n">imgs_array</span><span class="p">,</span> <span class="n">valid_tfms</span><span class="p">,</span> <span class="n">class_map</span><span class="o">=</span><span class="n">class_map</span><span class="p">)</span>

<span class="c1"># Batch Inference</span>
<span class="n">infer_dl</span> <span class="o">=</span> <span class="n">model_type</span><span class="o">.</span><span class="n">infer_dl</span><span class="p">(</span><span class="n">infer_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model_type</span><span class="o">.</span><span class="n">predict_from_dl</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">infer_dl</span><span class="p">,</span> <span class="n">keep_images</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">detection_threshold</span><span class="o">=</span><span class="mf">0.33</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># You may need to un-comment this in certain environments to preview images:</span>
<span class="c1"># %matplotlib inline</span>

<span class="n">show_preds</span><span class="p">(</span><span class="n">preds</span><span class="o">=</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>  0%|          | 0/2 [00:00&lt;?, ?it/s]
</code></pre></div>
</div>

<p><img alt="png" src="../images/inference/inference_28_0.png" /></p>
<h2 id="how-to-export-inferences-as-coco-annotations">How to export inferences as COCO annotations</h2>
<p>These will match the dimensions of the original images. This could be useful if you'd like to create preliminary pseudo-annotations for unlabeled data:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">icevision.data.convert_records_to_coco_style</span> <span class="kn">import</span> <span class="n">export_batch_inferences_as_coco_annotations</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">info</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;2022&quot;</span><span class="p">,</span>
    <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Exported from IceVision&quot;</span><span class="p">,</span>
    <span class="s2">&quot;contributor&quot;</span><span class="p">:</span> <span class="s2">&quot;Awesome contributor&quot;</span><span class="p">,</span>
    <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://lazyannotator.fun&quot;</span><span class="p">,</span>
    <span class="s2">&quot;date_created&quot;</span><span class="p">:</span> <span class="s2">&quot;2022-08-05T20:13:09+00:00&quot;</span>
<span class="p">}</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">licenses</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Creative Commons Attribution 4.0&quot;</span><span class="p">,</span>
        <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">export_batch_inferences_as_coco_annotations</span><span class="p">(</span>
    <span class="n">preds</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span>
    <span class="n">img_files</span><span class="o">=</span><span class="n">img_files</span><span class="p">,</span>
    <span class="n">transforms</span><span class="o">=</span><span class="n">valid_tfms</span><span class="p">,</span>
    <span class="n">class_map</span><span class="o">=</span><span class="n">class_map</span><span class="p">,</span>
    <span class="n">output_filepath</span><span class="o">=</span><span class="s2">&quot;../inferences_for_pseudo_labels.json&quot;</span><span class="p">,</span>
    <span class="n">info</span><span class="o">=</span><span class="n">info</span><span class="p">,</span>
    <span class="n">licenses</span><span class="o">=</span><span class="n">licenses</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>New COCO annotation file saved to ../inferences_for_pseudo_labels.json
</code></pre></div>
</div>
<h3 id="preview-predictions-on-original-image">Preview predictions on original image:</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Index of image you&#39;d like to check</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">this_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">record</span> <span class="o">=</span> <span class="n">this_pred</span><span class="o">.</span><span class="n">pred</span>

<span class="c1"># Draw that image</span>
<span class="n">pred_img</span> <span class="o">=</span> <span class="n">draw_record</span><span class="p">(</span>
        <span class="n">record</span><span class="o">=</span><span class="n">this_pred</span><span class="p">,</span>
        <span class="n">class_map</span><span class="o">=</span><span class="n">class_map</span><span class="p">,</span>
        <span class="n">display_label</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">display_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">display_bbox</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">font_path</span><span class="o">=</span><span class="n">get_default_font</span><span class="p">(),</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">label_color</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;#FF59D6&quot;</span><span class="p">),</span>
        <span class="n">return_as_pil_img</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">pred_img</span>
</code></pre></div>
<p><img alt="png" src="../images/inference/inference_35_0.png" /></p>
<h2 id="how-to-save-a-model-and-its-metadata-in-icevision">How to save a model and its metadata in IceVision</h2>
<p>When saving a model weights, we could also store the model metadata that are retrieved by the <code>model_from_checkpoint(checkpoint_path)</code> method</p>
<div class="highlight"><pre><span></span><code><span class="c1"># How to save a model and its metadata</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s1">&#39;coco-retinanet-checkpoint-full.pth&#39;</span>

<span class="n">save_icevision_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                        <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;mmdet.retinanet&#39;</span><span class="p">,</span> 
                        <span class="n">backbone_name</span><span class="o">=</span><span class="s1">&#39;resnet50_fpn_1x&#39;</span><span class="p">,</span>
                        <span class="n">classes</span> <span class="o">=</span>  <span class="n">class_map</span><span class="o">.</span><span class="n">get_classes</span><span class="p">(),</span> 
                        <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span> 
                        <span class="n">filename</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">,</span>
                        <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;icevision_version&#39;</span><span class="p">:</span> <span class="s1">&#39;0.9.1&#39;</span><span class="p">})</span>
</code></pre></div>
<h2 id="loading-models-already-containing-metadata">Loading models already containing metadata</h2>
<p>If you have saved your model weights with the model metadata, you only need to call <code>model_from_checkpoint(checkpoint_path)</code>: No other arguments (<code>model_name, backbone_name, classes, img_size</code>) are needed. All the information is already embedded in the checkpoint file.</p>
<div class="highlight"><pre><span></span><code><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s1">&#39;https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth&#39;</span>
<span class="n">checkpoint_and_model</span> <span class="o">=</span> <span class="n">model_from_checkpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
</code></pre></div>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>load checkpoint from http path: https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth

Downloading: &quot;https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth&quot; to /root/.cache/torch/hub/checkpoints/fridge-retinanet-checkpoint-full.pth

  0%|          | 0.00/139M [00:00&lt;?, ?B/s]

2022-08-12 18:56:49,593 - mmcv - INFO - initialize ResNet with init_cfg {&#39;type&#39;: &#39;Pretrained&#39;, &#39;checkpoint&#39;: &#39;torchvision://resnet50&#39;}
2022-08-12 18:56:49,594 - mmcv - INFO - load model from: torchvision://resnet50
2022-08-12 18:56:49,594 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50
2022-08-12 18:56:49,687 - mmcv - WARNING - The model and loaded state dict do not match exactly
</code></pre></div>
</div>

<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>unexpected key in source state_dict: fc.weight, fc.bias
</code></pre></div>
</div>

<p><div class="highlight"><pre><span></span><code><span class="c1"># Just logging the info</span>
<span class="n">model_type</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">]</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;backbone&quot;</span><span class="p">]</span>
<span class="n">class_map</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;class_map&quot;</span><span class="p">]</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;img_size&quot;</span><span class="p">]</span>
<span class="n">model_type</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">class_map</span><span class="p">,</span> <span class="n">img_size</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Inference</span>

<span class="c1"># Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span>

<span class="c1"># Transforms</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="n">checkpoint_and_model</span><span class="p">[</span><span class="s2">&quot;img_size&quot;</span><span class="p">]</span>
<span class="n">valid_tfms</span> <span class="o">=</span> <span class="n">tfms</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">Adapter</span><span class="p">([</span><span class="o">*</span><span class="n">tfms</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">resize_and_pad</span><span class="p">(</span><span class="n">img_size</span><span class="p">),</span> <span class="n">tfms</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">Normalize</span><span class="p">()])</span>

<span class="c1"># Pick your images folder</span>
<span class="n">path_to_image_folder</span> <span class="o">=</span> <span class="s2">&quot;../samples/fridge/odFridgeObjects/images&quot;</span>
<span class="n">img_files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path_to_image_folder</span><span class="p">)</span>

<span class="c1"># Create a dataset with appropriate images </span>
<span class="n">imgs_array</span> <span class="o">=</span> <span class="p">[</span><span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">fname</span><span class="p">))</span> <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">img_files</span><span class="p">]</span>
<span class="n">infer_ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_images</span><span class="p">(</span><span class="n">imgs_array</span><span class="p">,</span> <span class="n">valid_tfms</span><span class="p">,</span> <span class="n">class_map</span><span class="o">=</span><span class="n">class_map</span><span class="p">)</span>

<span class="c1"># Batch Inference</span>
<span class="n">infer_dl</span> <span class="o">=</span> <span class="n">model_type</span><span class="o">.</span><span class="n">infer_dl</span><span class="p">(</span><span class="n">infer_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model_type</span><span class="o">.</span><span class="n">predict_from_dl</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">infer_dl</span><span class="p">,</span> <span class="n">keep_images</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">show_preds</span><span class="p">(</span><span class="n">preds</span><span class="o">=</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></p>
<div class="k-default-codeblock">
<div class="highlight"><pre><span></span><code>2022-08-12 18:56:49,704 - mmcv - INFO - initialize FPN with init_cfg {&#39;type&#39;: &#39;Xavier&#39;, &#39;layer&#39;: &#39;Conv2d&#39;, &#39;distribution&#39;: &#39;uniform&#39;}
2022-08-12 18:56:49,744 - mmcv - INFO - initialize RetinaHead with init_cfg {&#39;type&#39;: &#39;Normal&#39;, &#39;layer&#39;: &#39;Conv2d&#39;, &#39;std&#39;: 0.01, &#39;override&#39;: {&#39;type&#39;: &#39;Normal&#39;, &#39;name&#39;: &#39;retina_cls&#39;, &#39;std&#39;: 0.01, &#39;bias_prob&#39;: 0.01}}
2022-08-12 18:56:49,772 - mmcv - INFO - 
backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,774 - mmcv - INFO - 
backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,776 - mmcv - INFO - 
backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,777 - mmcv - INFO - 
backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,779 - mmcv - INFO - 
backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,781 - mmcv - INFO - 
backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,781 - mmcv - INFO - 
backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,781 - mmcv - INFO - 
backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,783 - mmcv - INFO - 
backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,784 - mmcv - INFO - 
backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,784 - mmcv - INFO - 
backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,786 - mmcv - INFO - 
backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,787 - mmcv - INFO - 
backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,787 - mmcv - INFO - 
backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,788 - mmcv - INFO - 
backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,788 - mmcv - INFO - 
backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,788 - mmcv - INFO - 
backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,788 - mmcv - INFO - 
backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,791 - mmcv - INFO - 
backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,792 - mmcv - INFO - 
backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,792 - mmcv - INFO - 
backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,792 - mmcv - INFO - 
backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,793 - mmcv - INFO - 
backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,793 - mmcv - INFO - 
backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,793 - mmcv - INFO - 
backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,793 - mmcv - INFO - 
backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,794 - mmcv - INFO - 
backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,794 - mmcv - INFO - 
backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,794 - mmcv - INFO - 
backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,794 - mmcv - INFO - 
backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,795 - mmcv - INFO - 
backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,795 - mmcv - INFO - 
backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,795 - mmcv - INFO - 
backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,796 - mmcv - INFO - 
backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,796 - mmcv - INFO - 
backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,796 - mmcv - INFO - 
backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,796 - mmcv - INFO - 
backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,797 - mmcv - INFO - 
backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,797 - mmcv - INFO - 
backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,797 - mmcv - INFO - 
backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,798 - mmcv - INFO - 
backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,798 - mmcv - INFO - 
backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,798 - mmcv - INFO - 
backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,798 - mmcv - INFO - 
backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,799 - mmcv - INFO - 
backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,799 - mmcv - INFO - 
backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,799 - mmcv - INFO - 
backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,799 - mmcv - INFO - 
backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,800 - mmcv - INFO - 
backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,800 - mmcv - INFO - 
backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,800 - mmcv - INFO - 
backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,800 - mmcv - INFO - 
backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,801 - mmcv - INFO - 
backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,801 - mmcv - INFO - 
backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,801 - mmcv - INFO - 
backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,802 - mmcv - INFO - 
backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,802 - mmcv - INFO - 
backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,802 - mmcv - INFO - 
backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,803 - mmcv - INFO - 
backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,803 - mmcv - INFO - 
backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,803 - mmcv - INFO - 
backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,803 - mmcv - INFO - 
backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,804 - mmcv - INFO - 
backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,804 - mmcv - INFO - 
backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,804 - mmcv - INFO - 
backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,804 - mmcv - INFO - 
backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,805 - mmcv - INFO - 
backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,805 - mmcv - INFO - 
backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,805 - mmcv - INFO - 
backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,806 - mmcv - INFO - 
backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,806 - mmcv - INFO - 
backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,806 - mmcv - INFO - 
backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,807 - mmcv - INFO - 
backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,807 - mmcv - INFO - 
backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,807 - mmcv - INFO - 
backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,807 - mmcv - INFO - 
backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,808 - mmcv - INFO - 
backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,808 - mmcv - INFO - 
backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,808 - mmcv - INFO - 
backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,809 - mmcv - INFO - 
backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,809 - mmcv - INFO - 
backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,809 - mmcv - INFO - 
backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,809 - mmcv - INFO - 
backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,810 - mmcv - INFO - 
backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,810 - mmcv - INFO - 
backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,810 - mmcv - INFO - 
backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,811 - mmcv - INFO - 
backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,811 - mmcv - INFO - 
backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,812 - mmcv - INFO - 
backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,812 - mmcv - INFO - 
backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,812 - mmcv - INFO - 
backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,813 - mmcv - INFO - 
backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,813 - mmcv - INFO - 
backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,813 - mmcv - INFO - 
backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,814 - mmcv - INFO - 
backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,814 - mmcv - INFO - 
backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,814 - mmcv - INFO - 
backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,815 - mmcv - INFO - 
backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,815 - mmcv - INFO - 
backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,816 - mmcv - INFO - 
backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,816 - mmcv - INFO - 
backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,816 - mmcv - INFO - 
backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,817 - mmcv - INFO - 
backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,817 - mmcv - INFO - 
backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,817 - mmcv - INFO - 
backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,817 - mmcv - INFO - 
backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,818 - mmcv - INFO - 
backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,819 - mmcv - INFO - 
backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,819 - mmcv - INFO - 
backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,819 - mmcv - INFO - 
backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,819 - mmcv - INFO - 
backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,820 - mmcv - INFO - 
backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,820 - mmcv - INFO - 
backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,820 - mmcv - INFO - 
backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,821 - mmcv - INFO - 
backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,821 - mmcv - INFO - 
backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,821 - mmcv - INFO - 
backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,821 - mmcv - INFO - 
backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,822 - mmcv - INFO - 
backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,822 - mmcv - INFO - 
backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,823 - mmcv - INFO - 
backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,823 - mmcv - INFO - 
backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,823 - mmcv - INFO - 
backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,823 - mmcv - INFO - 
backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,824 - mmcv - INFO - 
backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,824 - mmcv - INFO - 
backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,824 - mmcv - INFO - 
backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,826 - mmcv - INFO - 
backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,826 - mmcv - INFO - 
backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,826 - mmcv - INFO - 
backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,827 - mmcv - INFO - 
backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,827 - mmcv - INFO - 
backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,827 - mmcv - INFO - 
backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,827 - mmcv - INFO - 
backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,828 - mmcv - INFO - 
backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,828 - mmcv - INFO - 
backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,828 - mmcv - INFO - 
backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,829 - mmcv - INFO - 
backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,829 - mmcv - INFO - 
backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,829 - mmcv - INFO - 
backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,830 - mmcv - INFO - 
backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,830 - mmcv - INFO - 
backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,830 - mmcv - INFO - 
backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,830 - mmcv - INFO - 
backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,831 - mmcv - INFO - 
backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,831 - mmcv - INFO - 
backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,831 - mmcv - INFO - 
backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,832 - mmcv - INFO - 
backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,832 - mmcv - INFO - 
backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,833 - mmcv - INFO - 
backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,833 - mmcv - INFO - 
backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,833 - mmcv - INFO - 
backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,834 - mmcv - INFO - 
backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,834 - mmcv - INFO - 
backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,834 - mmcv - INFO - 
backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,835 - mmcv - INFO - 
backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,835 - mmcv - INFO - 
backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,835 - mmcv - INFO - 
backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,835 - mmcv - INFO - 
backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

2022-08-12 18:56:49,836 - mmcv - INFO - 
neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,836 - mmcv - INFO - 
neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,836 - mmcv - INFO - 
neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,837 - mmcv - INFO - 
neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,837 - mmcv - INFO - 
neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,837 - mmcv - INFO - 
neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,838 - mmcv - INFO - 
neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,838 - mmcv - INFO - 
neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,838 - mmcv - INFO - 
neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,839 - mmcv - INFO - 
neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,839 - mmcv - INFO - 
neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,839 - mmcv - INFO - 
neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,839 - mmcv - INFO - 
neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,840 - mmcv - INFO - 
neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,840 - mmcv - INFO - 
neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

2022-08-12 18:56:49,840 - mmcv - INFO - 
neck.fpn_convs.4.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,841 - mmcv - INFO - 
bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,841 - mmcv - INFO - 
bbox_head.cls_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,842 - mmcv - INFO - 
bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,842 - mmcv - INFO - 
bbox_head.cls_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,843 - mmcv - INFO - 
bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,843 - mmcv - INFO - 
bbox_head.cls_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,843 - mmcv - INFO - 
bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,844 - mmcv - INFO - 
bbox_head.cls_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,844 - mmcv - INFO - 
bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,844 - mmcv - INFO - 
bbox_head.reg_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,844 - mmcv - INFO - 
bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,845 - mmcv - INFO - 
bbox_head.reg_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,845 - mmcv - INFO - 
bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,845 - mmcv - INFO - 
bbox_head.reg_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,846 - mmcv - INFO - 
bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,846 - mmcv - INFO - 
bbox_head.reg_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

2022-08-12 18:56:49,846 - mmcv - INFO - 
bbox_head.retina_cls.weight - torch.Size([36, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

2022-08-12 18:56:49,847 - mmcv - INFO - 
bbox_head.retina_cls.bias - torch.Size([36]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

2022-08-12 18:56:49,847 - mmcv - INFO - 
bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

2022-08-12 18:56:49,847 - mmcv - INFO - 
bbox_head.retina_reg.bias - torch.Size([36]): 
NormalInit: mean=0, std=0.01, bias=0 


(&lt;module &#39;icevision.models.mmdet.models.retinanet&#39; from &#39;/notebooks/icevision/models/mmdet/models/retinanet/__init__.py&#39;&gt;,
 &lt;icevision.models.mmdet.models.retinanet.backbones.backbone_config.MMDetRetinanetBackboneConfig at 0x7fad702b70d0&gt;,
 &lt;ClassMap: {&#39;background&#39;: 0, &#39;carton&#39;: 1, &#39;milk_bottle&#39;: 2, &#39;can&#39;: 3, &#39;water_bottle&#39;: 4}&gt;,
 384)

  0%|          | 0/2 [00:00&lt;?, ?it/s]
</code></pre></div>
</div>

<p><img alt="png" src="../images/inference/inference_42_0.png" /></p>
<h2 id="happy-learning">Happy Learning!</h2>
<p>If you need any assistance, feel free to join our <a href="https://discord.gg/JDBeZYK">forum</a>.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../custom_parser/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Custom Parser" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Custom Parser
            </div>
          </div>
        </a>
      
      
        
        <a href="../wandb_efficientdet/" class="md-footer__link md-footer__link--next" aria-label="Next: Model Tracking Using Wandb" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Model Tracking Using Wandb
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            airctic.com
          </div>
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b1047164.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@8.4.6/dist/mermaid.min.js"></script>
      
        <script src="../js/termynal.js"></script>
      
        <script src="../js/custom.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/@widgetbot/crate@3"></script>
      
        <script src="../js/crate.js"></script>
      
    
  </body>
</html>