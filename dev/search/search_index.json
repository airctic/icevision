{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Join our Forum IceVision is the first agnostic computer vision framework to offer a curated collection with hundreds of high-quality pre-trained models from torchvision, MMLabs, and soon Pytorch Image Models. It orchestrates the end-to-end deep learning workflow allowing to train networks with easy-to-use robust high-performance libraries such as Pytorch-Lightning and Fastai IceVision Unique Features: Data curation/cleaning with auto-fix Access to an exploratory data analysis dashboard Pluggable transforms for better model generalization Access to hundreds of neural net models Access to multiple training loop libraries Multi-task training to efficiently combine object detection, segmentation, and classification models Quick Example: How to train the Fridge Objects Dataset Happy Learning! If you need any assistance, feel free to: Join our Forum","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"Join our Forum IceVision is the first agnostic computer vision framework to offer a curated collection with hundreds of high-quality pre-trained models from torchvision, MMLabs, and soon Pytorch Image Models. It orchestrates the end-to-end deep learning workflow allowing to train networks with easy-to-use robust high-performance libraries such as Pytorch-Lightning and Fastai IceVision Unique Features: Data curation/cleaning with auto-fix Access to an exploratory data analysis dashboard Pluggable transforms for better model generalization Access to hundreds of neural net models Access to multiple training loop libraries Multi-task training to efficiently combine object detection, segmentation, and classification models","title":""},{"location":"#quick-example-how-to-train-the-fridge-objects-dataset","text":"","title":"Quick Example: How to train the Fridge Objects Dataset"},{"location":"#happy-learning","text":"If you need any assistance, feel free to: Join our Forum","title":"Happy Learning!"},{"location":"IceApp_coco/","text":"IceVision Deployment App: COCO Dataset This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"IceApp coco"},{"location":"IceApp_coco/#icevision-deployment-app-coco-dataset","text":"This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  COCO Dataset"},{"location":"IceApp_coco/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_coco/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_coco/#loading-trained-model","text":"class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_coco/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_coco/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_coco/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_coco/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_masks/","text":"IceVision Deployment App: PennFudan Dataset This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0] Defining the get_masks method: called by gr.Interface(fn=get_masks, ...) def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface # Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"IceApp masks"},{"location":"IceApp_masks/#icevision-deployment-app-pennfudan-dataset","text":"This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  PennFudan Dataset"},{"location":"IceApp_masks/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_masks/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_masks/#loading-trained-model","text":"class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_masks/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_masks/#defining-the-get_masks-method-called-by-grinterfacefnget_masks","text":"def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the get_masks method: called by gr.Interface(fn=get_masks, ...)"},{"location":"IceApp_masks/#gradio-user-interface","text":"# Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_masks/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_pets/","text":"IceVision Deployment App: PETS Dataset This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"IceApp pets"},{"location":"IceApp_pets/#icevision-deployment-app-pets-dataset","text":"This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App: PETS Dataset"},{"location":"IceApp_pets/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_pets/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_pets/#loading-trained-model","text":"class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_pets/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_pets/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_pets/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_pets/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"SAHI_inference/","text":"IceVision + SAHI: addressing low performance in small object detection This notebook showcases the newly added IceVision + SAHI integration. You can find more detailed info about this work in this blog post . Installing Icevision and dependencies + SAHI # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master Install SAHI ! pip install sahi - q # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... Loading the Fridge dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) train_records , valid_records = parser . parse () 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] Defining augmentations and datasets image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = ( image_size , image_size ), presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad (( image_size , image_size )), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Choosing model # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x #model_type = models.mmdet.faster_rcnn #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.retinanet #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.ssd #backbone = model_type.backbones.ssd512 elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite1 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . medium # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size print ( model_type , backbone , extra_args ) model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) Getting dataloaders, defining metrics and instantiate fastai learner train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 8 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 8 , shuffle = False ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Finding best learning rate learn . lr_find () /usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=0.0004786300996784121) Training the model learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.809848 3.245110 0.219802 00:22 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 2.858475 2.487748 0.435076 00:24 1 2.506602 1.631549 0.439791 00:19 2 2.196014 1.348319 0.443677 00:19 3 1.966267 1.245886 0.692616 00:19 4 1.783822 1.063900 0.778038 00:19 5 1.637959 0.931765 0.855932 00:19 6 1.499139 0.803554 0.919840 00:19 7 1.391289 0.777090 0.922477 00:19 8 1.303855 0.758904 0.918359 00:19 9 1.213133 0.675470 0.942594 00:19 10 1.139793 0.716553 0.924616 00:19 11 1.084275 0.671022 0.935786 00:19 12 1.037781 0.623612 0.948895 00:19 13 0.991282 0.619961 0.960125 00:19 14 0.950265 0.616058 0.961337 00:19 15 0.918986 0.610729 0.958616 00:19 16 0.886407 0.608031 0.955717 00:19 17 0.858739 0.600173 0.956490 00:19 18 0.835620 0.598794 0.954467 00:19 19 0.810680 0.598279 0.956181 00:19 Downloading sample image ! wget -- no - check - certificate - O small_fridge . jpg 'https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS' --2021-11-30 18:12:37-- https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS Resolving docs.google.com (docs.google.com)... 172.217.214.101, 172.217.214.113, 172.217.214.139, ... Connecting to docs.google.com (docs.google.com)|172.217.214.101|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download [following] Warning: wildcards not supported in HTTP. --2021-11-30 18:12:37-- https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download Resolving doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)... 142.250.159.132, 2607:f8b0:4001:c58::84 Connecting to doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)|142.250.159.132|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 48117 (47K) [image/jpeg] Saving to: \u2018small_fridge.jpg\u2019 small_fridge.jpg 100%[===================>] 46.99K --.-KB/s in 0.001s 2021-11-30 18:12:37 (91.5 MB/s) - \u2018small_fridge.jpg\u2019 saved [48117/48117] PIL . Image . open ( \"small_fridge.jpg\" ) . resize (( 500 , 300 )) Running inference without SAHI No bbox detected! img = PIL . Image . open ( \"small_fridge.jpg\" ) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = parser . class_map , detection_threshold = 0.4 ) pred_dict [ 'img' ] Running inference with SAHI Check out how almost all objects (too small for a one-shot prediction) are detected using the sliding window approach SAHI offers. from icevision.models.inference_sahi import IceSahiModel sahimodel = IceSahiModel ( model_type = model_type , model = model , class_map = parser . class_map , tfms = valid_tfms , confidence_threshold = 0.4 ) pred = sahimodel . get_sliced_prediction ( \"small_fridge.jpg\" , keep_sahi_format = False , return_img = True , slice_height = 128 , slice_width = 128 , overlap_height_ratio = 0.2 , overlap_width_ratio = 0.2 , ) pred [ \"img\" ] Number of slices: 91","title":"Small Object Detection with SAHI"},{"location":"SAHI_inference/#icevision-sahi-addressing-low-performance-in-small-object-detection","text":"This notebook showcases the newly added IceVision + SAHI integration. You can find more detailed info about this work in this blog post .","title":"IceVision + SAHI: addressing low performance in small object detection"},{"location":"SAHI_inference/#installing-icevision-and-dependencies-sahi","text":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master Install SAHI ! pip install sahi - q # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing Icevision and dependencies + SAHI"},{"location":"SAHI_inference/#imports","text":"from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...","title":"Imports"},{"location":"SAHI_inference/#loading-the-fridge-dataset","text":"url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) train_records , valid_records = parser . parse () 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s]","title":"Loading the Fridge dataset"},{"location":"SAHI_inference/#defining-augmentations-and-datasets","text":"image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = ( image_size , image_size ), presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad (( image_size , image_size )), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Defining augmentations and datasets"},{"location":"SAHI_inference/#choosing-model","text":"# Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x #model_type = models.mmdet.faster_rcnn #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.retinanet #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.ssd #backbone = model_type.backbones.ssd512 elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite1 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . medium # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size print ( model_type , backbone , extra_args ) model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Choosing model"},{"location":"SAHI_inference/#getting-dataloaders-defining-metrics-and-instantiate-fastai-learner","text":"train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 8 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 8 , shuffle = False ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked))","title":"Getting dataloaders, defining metrics and instantiate fastai learner"},{"location":"SAHI_inference/#finding-best-learning-rate","text":"learn . lr_find () /usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=0.0004786300996784121)","title":"Finding best learning rate"},{"location":"SAHI_inference/#training-the-model","text":"learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.809848 3.245110 0.219802 00:22 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 2.858475 2.487748 0.435076 00:24 1 2.506602 1.631549 0.439791 00:19 2 2.196014 1.348319 0.443677 00:19 3 1.966267 1.245886 0.692616 00:19 4 1.783822 1.063900 0.778038 00:19 5 1.637959 0.931765 0.855932 00:19 6 1.499139 0.803554 0.919840 00:19 7 1.391289 0.777090 0.922477 00:19 8 1.303855 0.758904 0.918359 00:19 9 1.213133 0.675470 0.942594 00:19 10 1.139793 0.716553 0.924616 00:19 11 1.084275 0.671022 0.935786 00:19 12 1.037781 0.623612 0.948895 00:19 13 0.991282 0.619961 0.960125 00:19 14 0.950265 0.616058 0.961337 00:19 15 0.918986 0.610729 0.958616 00:19 16 0.886407 0.608031 0.955717 00:19 17 0.858739 0.600173 0.956490 00:19 18 0.835620 0.598794 0.954467 00:19 19 0.810680 0.598279 0.956181 00:19","title":"Training the model"},{"location":"SAHI_inference/#downloading-sample-image","text":"! wget -- no - check - certificate - O small_fridge . jpg 'https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS' --2021-11-30 18:12:37-- https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS Resolving docs.google.com (docs.google.com)... 172.217.214.101, 172.217.214.113, 172.217.214.139, ... Connecting to docs.google.com (docs.google.com)|172.217.214.101|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download [following] Warning: wildcards not supported in HTTP. --2021-11-30 18:12:37-- https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download Resolving doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)... 142.250.159.132, 2607:f8b0:4001:c58::84 Connecting to doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)|142.250.159.132|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 48117 (47K) [image/jpeg] Saving to: \u2018small_fridge.jpg\u2019 small_fridge.jpg 100%[===================>] 46.99K --.-KB/s in 0.001s 2021-11-30 18:12:37 (91.5 MB/s) - \u2018small_fridge.jpg\u2019 saved [48117/48117] PIL . Image . open ( \"small_fridge.jpg\" ) . resize (( 500 , 300 ))","title":"Downloading sample image"},{"location":"SAHI_inference/#running-inference-without-sahi","text":"No bbox detected! img = PIL . Image . open ( \"small_fridge.jpg\" ) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = parser . class_map , detection_threshold = 0.4 ) pred_dict [ 'img' ]","title":"Running inference without SAHI"},{"location":"SAHI_inference/#running-inference-with-sahi","text":"Check out how almost all objects (too small for a one-shot prediction) are detected using the sliding window approach SAHI offers. from icevision.models.inference_sahi import IceSahiModel sahimodel = IceSahiModel ( model_type = model_type , model = model , class_map = parser . class_map , tfms = valid_tfms , confidence_threshold = 0.4 ) pred = sahimodel . get_sliced_prediction ( \"small_fridge.jpg\" , keep_sahi_format = False , return_img = True , slice_height = 128 , slice_width = 128 , overlap_height_ratio = 0.2 , overlap_width_ratio = 0.2 , ) pred [ \"img\" ] Number of slices: 91","title":"Running inference with SAHI"},{"location":"YOLOv5/","text":"YOLOv5 from Ultralytics This notebook showcases the newly added YOLOv5 suite of models to the IceVision library. Installing IceVision, IceData and yolov5-icevision yolov5-icevision is a tiny IceVision wrapper around the official YOLOv5 repo from Ultralytics, to make the code pip-installable. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 Imports from icevision.all import * import icedata \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m69\u001b[0m Loading the Fridge dataset data_dir = icedata . fridge . load_data () class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) train_records , valid_records = parser . parse () presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) len ( train_ds ), len ( valid_ds ) 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m136\u001b[0m (102, 26) YOLOv5 dataloaders model_type = models . ultralytics . yolov5 train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 2 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 2 , shuffle = False ) Showing what's inside a batch dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 2 , shuffle = False ) batch = first ( dl ) model_type . show_batch ( batch , ncols = 4 ) YOLOv5 model You can choose between a small , medium and large backbone. backbone = model_type . backbones . small ( pretrained = True ) #backbone = model_type.backbones.medium(pretrained=True) #backbone = model_type.backbones.large(pretrained=True) #backbone = model_type.backbones.extra_large(pretrained=True) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size , device = torch . device ( \"cuda\" )) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt to /root/.icevision/yolo/yolov5s.pt... 0%| | 0.00/14.1M [00:00<?, ?B/s] Training a fastai Learner metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(lr_min=0.0033113110810518267, lr_steep=9.12010818865383e-07) learn . fit_one_cycle ( 50 , 3e-4 ) <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='5' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress> 10.00% [5/50 00:16<02:28] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='0' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress> 0.00% [0/7 00:00<00:00] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 5 0.661492 0.513050 0.257225 00:03 6 0.622621 0.455875 0.482885 00:03 7 0.595726 0.436317 0.271832 00:03 8 0.563349 0.419788 0.236814 00:03 9 0.539367 0.387893 0.348432 00:03 10 0.515677 0.375338 0.283068 00:03 11 0.496582 0.369159 0.413665 00:03 12 0.474011 0.326403 0.472587 00:03 13 0.456676 0.332162 0.553740 00:03 14 0.434918 0.298136 0.627820 00:03 15 0.414545 0.272040 0.652365 00:03 16 0.395862 0.329013 0.391859 00:03 17 0.383311 0.271511 0.635272 00:03 18 0.367031 0.303586 0.322467 00:03 19 0.354010 0.271726 0.497521 00:03 20 0.340034 0.226416 0.700743 00:03 21 0.326033 0.225464 0.619431 00:03 22 0.310925 0.203816 0.769183 00:03 23 0.295661 0.189258 0.769431 00:03 24 0.281358 0.195591 0.681807 00:03 25 0.269978 0.190864 0.731312 00:03 26 0.260026 0.189974 0.700495 00:03 27 0.252257 0.180860 0.738119 00:03 28 0.247666 0.165262 0.762871 00:03 29 0.237585 0.172969 0.737624 00:03 30 0.228174 0.156880 0.806559 00:03 31 0.219802 0.149674 0.856312 00:03 32 0.214853 0.147571 0.800248 00:03 33 0.211070 0.139506 0.875248 00:03 34 0.203329 0.139021 0.843936 00:03 35 0.195134 0.126844 0.856312 00:03 36 0.188641 0.123761 0.881312 00:03 37 0.183303 0.128407 0.875000 00:03 38 0.177788 0.113928 0.868936 00:03 39 0.171578 0.126054 0.900248 00:03 40 0.168272 0.113003 0.868936 00:03 41 0.166278 0.113589 0.906559 00:03 42 0.161366 0.109419 0.893936 00:03 43 0.161012 0.107286 0.925248 00:03 44 0.155416 0.107902 0.925248 00:03 45 0.150046 0.104697 0.918936 00:03 46 0.145804 0.104805 0.918936 00:03 47 0.141888 0.104281 0.918936 00:03 48 0.137711 0.103718 0.918936 00:03 49 0.135240 0.103554 0.918936 00:03 ### Showing training results model_type . show_results ( model , valid_ds , detection_threshold = 0.25 ) ![png](images/YOLOv5/YOLOv5_21_0.png) ## Inference pipeline From a dataset... preds = model_type . predict ( model , valid_ds , detection_threshold = 0.25 ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) ![png](images/YOLOv5/YOLOv5_23_0.png) ... and from a Dataloader infer_dl = model_type . infer_dl ( valid_ds , batch_size = 1 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , detection_threshold = 0.25 , keep_images = True ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) 0%| | 0/26 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_25_1.png) ## Inspecting model predictions with `plot_top_losses` What are the images the model is having a hard time time with? #by = \"loss_total\" #by = \"loss_yolo\" by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_yolo\" : 0.5 , }, } sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = by ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_yolo']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m219\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_27_3.png) ## Training a PyTorch Lightning model class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 3e-4 ) backbone = model_type . backbones . medium ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 30 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5m.pt to /root/.icevision/yolo/yolov5m.pt... 0%| | 0.00/41.1M [00:00<?, ?B/s] GPU available: True, used: True TPU available: None, using: 0 TPU cores | Name | Type | Params -------------------------------- 0 | model | Model | 21.1 M -------------------------------- 21.1 M Trainable params 0 Non-trainable params 21.1 M Total params 84.290 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] 1 ## Happy Learning! If you need any assistance, feel free to join our [forum](https://discord.gg/JDBeZYK).","title":"YOLOv5"},{"location":"YOLOv5/#yolov5-from-ultralytics","text":"This notebook showcases the newly added YOLOv5 suite of models to the IceVision library.","title":"YOLOv5 from Ultralytics"},{"location":"YOLOv5/#installing-icevision-icedata-and-yolov5-icevision","text":"yolov5-icevision is a tiny IceVision wrapper around the official YOLOv5 repo from Ultralytics, to make the code pip-installable. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11","title":"Installing IceVision, IceData and yolov5-icevision"},{"location":"YOLOv5/#imports","text":"from icevision.all import * import icedata \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m69\u001b[0m","title":"Imports"},{"location":"YOLOv5/#loading-the-fridge-dataset","text":"data_dir = icedata . fridge . load_data () class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) train_records , valid_records = parser . parse () presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) len ( train_ds ), len ( valid_ds ) 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m136\u001b[0m (102, 26)","title":"Loading the Fridge dataset"},{"location":"YOLOv5/#yolov5-dataloaders","text":"model_type = models . ultralytics . yolov5 train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 2 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 2 , shuffle = False ) Showing what's inside a batch dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 2 , shuffle = False ) batch = first ( dl ) model_type . show_batch ( batch , ncols = 4 )","title":"YOLOv5 dataloaders"},{"location":"YOLOv5/#yolov5-model","text":"You can choose between a small , medium and large backbone. backbone = model_type . backbones . small ( pretrained = True ) #backbone = model_type.backbones.medium(pretrained=True) #backbone = model_type.backbones.large(pretrained=True) #backbone = model_type.backbones.extra_large(pretrained=True) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size , device = torch . device ( \"cuda\" )) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt to /root/.icevision/yolo/yolov5s.pt... 0%| | 0.00/14.1M [00:00<?, ?B/s]","title":"YOLOv5 model"},{"location":"YOLOv5/#training-a-fastai-learner","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(lr_min=0.0033113110810518267, lr_steep=9.12010818865383e-07) learn . fit_one_cycle ( 50 , 3e-4 ) <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='5' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress> 10.00% [5/50 00:16<02:28] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='0' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress> 0.00% [0/7 00:00<00:00] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 5 0.661492 0.513050 0.257225 00:03 6 0.622621 0.455875 0.482885 00:03 7 0.595726 0.436317 0.271832 00:03 8 0.563349 0.419788 0.236814 00:03 9 0.539367 0.387893 0.348432 00:03 10 0.515677 0.375338 0.283068 00:03 11 0.496582 0.369159 0.413665 00:03 12 0.474011 0.326403 0.472587 00:03 13 0.456676 0.332162 0.553740 00:03 14 0.434918 0.298136 0.627820 00:03 15 0.414545 0.272040 0.652365 00:03 16 0.395862 0.329013 0.391859 00:03 17 0.383311 0.271511 0.635272 00:03 18 0.367031 0.303586 0.322467 00:03 19 0.354010 0.271726 0.497521 00:03 20 0.340034 0.226416 0.700743 00:03 21 0.326033 0.225464 0.619431 00:03 22 0.310925 0.203816 0.769183 00:03 23 0.295661 0.189258 0.769431 00:03 24 0.281358 0.195591 0.681807 00:03 25 0.269978 0.190864 0.731312 00:03 26 0.260026 0.189974 0.700495 00:03 27 0.252257 0.180860 0.738119 00:03 28 0.247666 0.165262 0.762871 00:03 29 0.237585 0.172969 0.737624 00:03 30 0.228174 0.156880 0.806559 00:03 31 0.219802 0.149674 0.856312 00:03 32 0.214853 0.147571 0.800248 00:03 33 0.211070 0.139506 0.875248 00:03 34 0.203329 0.139021 0.843936 00:03 35 0.195134 0.126844 0.856312 00:03 36 0.188641 0.123761 0.881312 00:03 37 0.183303 0.128407 0.875000 00:03 38 0.177788 0.113928 0.868936 00:03 39 0.171578 0.126054 0.900248 00:03 40 0.168272 0.113003 0.868936 00:03 41 0.166278 0.113589 0.906559 00:03 42 0.161366 0.109419 0.893936 00:03 43 0.161012 0.107286 0.925248 00:03 44 0.155416 0.107902 0.925248 00:03 45 0.150046 0.104697 0.918936 00:03 46 0.145804 0.104805 0.918936 00:03 47 0.141888 0.104281 0.918936 00:03 48 0.137711 0.103718 0.918936 00:03 49 0.135240 0.103554 0.918936 00:03 ### Showing training results model_type . show_results ( model , valid_ds , detection_threshold = 0.25 ) ![png](images/YOLOv5/YOLOv5_21_0.png) ## Inference pipeline From a dataset... preds = model_type . predict ( model , valid_ds , detection_threshold = 0.25 ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) ![png](images/YOLOv5/YOLOv5_23_0.png) ... and from a Dataloader infer_dl = model_type . infer_dl ( valid_ds , batch_size = 1 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , detection_threshold = 0.25 , keep_images = True ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) 0%| | 0/26 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_25_1.png) ## Inspecting model predictions with `plot_top_losses` What are the images the model is having a hard time time with? #by = \"loss_total\" #by = \"loss_yolo\" by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_yolo\" : 0.5 , }, } sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = by ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_yolo']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m219\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_27_3.png) ## Training a PyTorch Lightning model class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 3e-4 ) backbone = model_type . backbones . medium ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 30 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5m.pt to /root/.icevision/yolo/yolov5m.pt... 0%| | 0.00/41.1M [00:00<?, ?B/s] GPU available: True, used: True TPU available: None, using: 0 TPU cores | Name | Type | Params -------------------------------- 0 | model | Model | 21.1 M -------------------------------- 21.1 M Trainable params 0 Non-trainable params 21.1 M Total params 84.290 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] 1 ## Happy Learning! If you need any assistance, feel free to join our [forum](https://discord.gg/JDBeZYK).","title":"Training a fastai Learner"},{"location":"about/","text":"Hall of Fame This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"About"},{"location":"about/#hall-of-fame","text":"This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"Hall of Fame"},{"location":"albumentations/","text":"Transforms Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases. Usage In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Albumentations"},{"location":"albumentations/#transforms","text":"Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases.","title":"Transforms"},{"location":"albumentations/#usage","text":"In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Usage"},{"location":"albumentations_tfms/","text":"[source] aug_tfms icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit_x = ( - 0.0625 , 0.0625 ), shift_limit_y = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 15 , 15 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 10 , 10 ), g_shift_limit = ( - 10 , 10 ), b_shift_limit = ( - 10 , 10 ) ), lightning = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (width, height). presize Optional[Union[int, Tuple[int, int]]] : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.geometric.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lightning Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source] Adapter icevision . tfms . albumentations . Adapter ( tfms ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Albumentations"},{"location":"albumentations_tfms/#aug_tfms","text":"icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit_x = ( - 0.0625 , 0.0625 ), shift_limit_y = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 15 , 15 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 10 , 10 ), g_shift_limit = ( - 10 , 10 ), b_shift_limit = ( - 10 , 10 ) ), lightning = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (width, height). presize Optional[Union[int, Tuple[int, int]]] : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.geometric.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lightning Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source]","title":"aug_tfms"},{"location":"albumentations_tfms/#adapter","text":"icevision . tfms . albumentations . Adapter ( tfms ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Adapter"},{"location":"backbones/","text":"Backbones IceVision supports several backbones for model. Thanks to IceVision Unified API, backbone are invoked in a very similar way. model = model_type . model ( backbone = backbone (), num_classes = len ( parser . class_map ))","title":"Backbones"},{"location":"backbones/#backbones","text":"IceVision supports several backbones for model. Thanks to IceVision Unified API, backbone are invoked in a very similar way. model = model_type . model ( backbone = backbone (), num_classes = len ( parser . class_map ))","title":"Backbones"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog . 0.11.0 The following PRs have been merged since the last version. ai-fast-track - Updating mmcv installation to torch 1.10.0 (#972) - Upgrade to torch 1.10 and torchvision 0.11 (#970) - Pass both map_location, and logger to downstream methods (#968) - Bumped torch and torchision versions (#961) - Update CHANGELOG.md for Release 0.11.0 (#959) - Adding an installation script for cuda and cpu (#956) - fixed yaml issue in doc generation CI/CD (#952) - Upgrade mk-docs-build.yml in the CI/CD (#951) - Update mmcv to 1.3.14 and mmdet to 2.17.0 in CI/CD (#949) - Update notebooks installation (#940) - Fix Colab script (#938) - Fixed Colab installation script (#937) - Update installation to torch 1.9 and dependencies (#935) - Inference - automatically recreate model trained with COCO (#929) - Simplify save and load model checkpoints (#924) - Update installation to torch 1.9 + dependencies (#919) - Added MMDetection VFNet Support. (#906) - Make MMDetection config object accessible to users (#904) - Adding progressive resizing support (#902) - Fix mmdet weights path issue (#900) - add docker-compose instructions (#898) - Added script for icevision inference installation (#893) - Added kwargs and label_border_color to end2end_detect() (#891) - Fix icevision installation in Colab (#887) - added kwargs to the EfficientDet model() method (#883) fstroth - (WIP) Fix masks for instance segmentation (#967) - (Refactor) Removed the coco function. (#964) - (Feature) init coco and via parser with a dict instead of the filepath (#963) - (Feature) Added way to output metrics for pytorchlightning during training (#960) - Fix for CHANGLOG.md update script. (#958) - Script for automatically updating CHANGELOG.md (#957) - (Update) Updated code to run with albumentations version 1.0.3. (#927) - Radiographic images (#912) potipot - Fix show pred (#930) - Fix inference on rectangular efficientdet input (#910) FraPochetti - adding docker support (#895) - Colab Install Script: fixing link to icevision master (#888) jaeeolma - Empty mask fix (#933) bogdan-evtushenko - Add support for yolox from mmdetection. (#932) drscotthawley - casting both caption parts as str (#922) lgvaz - Unet3 (#907) nicjac - Fixed PIL size bug in ImageRecordComponent (#889) (#894) Thank you to all contributers: @ai-fast-track, @fstroth, @potipot, @FraPochetti, @jaeeolma, @bogdan-evtushenko, @drscotthawley, @lgvaz, @nicjac Unreleased - 0.10.0a1 Main dependencies updated torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8 Unreleased - 0.9.0a1 Added Low level parsing workflow with RecordCollection Semantic segmentation support with fastai Changed Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore. [0.8.1] Added end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers. Changed Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights. [0.8.0] Supports pytorch 1.8 Added iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models Changed Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4) [0.7.0] BREAKING: API Refactor Added Metrics for mmdetection models Changed Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl [0.6.0b1] Added mmdetection models Changed Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record Deleted [0.5.2] Added aggregate_records_objects function Changed Added label_field to VIA parser to allow for alternate region_attribute names [0.5.0] Added Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample Changed improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable [0.4.0] Added retinanet: model, dataloaders, predict, ... Changed Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models [0.3.0] Added pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing Changed show_preds visual improvement [0.2.2] Added Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time Changed Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles [0.2.1] Changed updated matplotlib and ipykernel minimum version for colab compatibility [0.2.0] IMPORTANT Switched from poetry to setuptools Added Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data Changed Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height Deleted Removed CombinedParser , all parsing can be done with the standard Parser [0.1.6] Added Efficientdet now support empty annotations Changed Returns float instead of dict on FastaiMetricAdapter [0.1.5] Changed Updates fastai2 to the final release version [0.1.4] Added soft import icedata in icevision.all show_pbar parameter to COCOMetric Changed Deleted [0.1.3] Changed Effdet as direct dependency [0.1.2] Added show_results function for each model Changed Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision Deleted Removed datasets module to instead use the new icedata package [0.0.9] Added batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds Changed Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids [0.0.0-pre-release] Added CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class Changed All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map Removed MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule Links","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog .","title":"Changelog"},{"location":"changelog/#0110","text":"The following PRs have been merged since the last version. ai-fast-track - Updating mmcv installation to torch 1.10.0 (#972) - Upgrade to torch 1.10 and torchvision 0.11 (#970) - Pass both map_location, and logger to downstream methods (#968) - Bumped torch and torchision versions (#961) - Update CHANGELOG.md for Release 0.11.0 (#959) - Adding an installation script for cuda and cpu (#956) - fixed yaml issue in doc generation CI/CD (#952) - Upgrade mk-docs-build.yml in the CI/CD (#951) - Update mmcv to 1.3.14 and mmdet to 2.17.0 in CI/CD (#949) - Update notebooks installation (#940) - Fix Colab script (#938) - Fixed Colab installation script (#937) - Update installation to torch 1.9 and dependencies (#935) - Inference - automatically recreate model trained with COCO (#929) - Simplify save and load model checkpoints (#924) - Update installation to torch 1.9 + dependencies (#919) - Added MMDetection VFNet Support. (#906) - Make MMDetection config object accessible to users (#904) - Adding progressive resizing support (#902) - Fix mmdet weights path issue (#900) - add docker-compose instructions (#898) - Added script for icevision inference installation (#893) - Added kwargs and label_border_color to end2end_detect() (#891) - Fix icevision installation in Colab (#887) - added kwargs to the EfficientDet model() method (#883) fstroth - (WIP) Fix masks for instance segmentation (#967) - (Refactor) Removed the coco function. (#964) - (Feature) init coco and via parser with a dict instead of the filepath (#963) - (Feature) Added way to output metrics for pytorchlightning during training (#960) - Fix for CHANGLOG.md update script. (#958) - Script for automatically updating CHANGELOG.md (#957) - (Update) Updated code to run with albumentations version 1.0.3. (#927) - Radiographic images (#912) potipot - Fix show pred (#930) - Fix inference on rectangular efficientdet input (#910) FraPochetti - adding docker support (#895) - Colab Install Script: fixing link to icevision master (#888) jaeeolma - Empty mask fix (#933) bogdan-evtushenko - Add support for yolox from mmdetection. (#932) drscotthawley - casting both caption parts as str (#922) lgvaz - Unet3 (#907) nicjac - Fixed PIL size bug in ImageRecordComponent (#889) (#894) Thank you to all contributers: @ai-fast-track, @fstroth, @potipot, @FraPochetti, @jaeeolma, @bogdan-evtushenko, @drscotthawley, @lgvaz, @nicjac","title":"0.11.0"},{"location":"changelog/#unreleased-0100a1","text":"","title":"Unreleased - 0.10.0a1"},{"location":"changelog/#main-dependencies-updated","text":"torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8","title":"Main dependencies updated"},{"location":"changelog/#unreleased-090a1","text":"","title":"Unreleased - 0.9.0a1"},{"location":"changelog/#added","text":"Low level parsing workflow with RecordCollection Semantic segmentation support with fastai","title":"Added"},{"location":"changelog/#changed","text":"Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore.","title":"Changed"},{"location":"changelog/#081","text":"","title":"[0.8.1]"},{"location":"changelog/#added_1","text":"end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers.","title":"Added"},{"location":"changelog/#changed_1","text":"Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights.","title":"Changed"},{"location":"changelog/#080","text":"Supports pytorch 1.8","title":"[0.8.0]"},{"location":"changelog/#added_2","text":"iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models","title":"Added"},{"location":"changelog/#changed_2","text":"Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4)","title":"Changed"},{"location":"changelog/#070","text":"BREAKING: API Refactor","title":"[0.7.0]"},{"location":"changelog/#added_3","text":"Metrics for mmdetection models","title":"Added"},{"location":"changelog/#changed_3","text":"Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl","title":"Changed"},{"location":"changelog/#060b1","text":"","title":"[0.6.0b1]"},{"location":"changelog/#added_4","text":"mmdetection models","title":"Added"},{"location":"changelog/#changed_4","text":"Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record","title":"Changed"},{"location":"changelog/#deleted","text":"","title":"Deleted"},{"location":"changelog/#052","text":"","title":"[0.5.2]"},{"location":"changelog/#added_5","text":"aggregate_records_objects function","title":"Added"},{"location":"changelog/#changed_5","text":"Added label_field to VIA parser to allow for alternate region_attribute names","title":"Changed"},{"location":"changelog/#050","text":"","title":"[0.5.0]"},{"location":"changelog/#added_6","text":"Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample","title":"Added"},{"location":"changelog/#changed_6","text":"improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable","title":"Changed"},{"location":"changelog/#040","text":"","title":"[0.4.0]"},{"location":"changelog/#added_7","text":"retinanet: model, dataloaders, predict, ...","title":"Added"},{"location":"changelog/#changed_7","text":"Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models","title":"Changed"},{"location":"changelog/#030","text":"","title":"[0.3.0]"},{"location":"changelog/#added_8","text":"pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing","title":"Added"},{"location":"changelog/#changed_8","text":"show_preds visual improvement","title":"Changed"},{"location":"changelog/#022","text":"","title":"[0.2.2]"},{"location":"changelog/#added_9","text":"Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time","title":"Added"},{"location":"changelog/#changed_9","text":"Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles","title":"Changed"},{"location":"changelog/#021","text":"","title":"[0.2.1]"},{"location":"changelog/#changed_10","text":"updated matplotlib and ipykernel minimum version for colab compatibility","title":"Changed"},{"location":"changelog/#020","text":"","title":"[0.2.0]"},{"location":"changelog/#important","text":"Switched from poetry to setuptools","title":"IMPORTANT"},{"location":"changelog/#added_10","text":"Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data","title":"Added"},{"location":"changelog/#changed_11","text":"Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height","title":"Changed"},{"location":"changelog/#deleted_1","text":"Removed CombinedParser , all parsing can be done with the standard Parser","title":"Deleted"},{"location":"changelog/#016","text":"","title":"[0.1.6]"},{"location":"changelog/#added_11","text":"Efficientdet now support empty annotations","title":"Added"},{"location":"changelog/#changed_12","text":"Returns float instead of dict on FastaiMetricAdapter","title":"Changed"},{"location":"changelog/#015","text":"","title":"[0.1.5]"},{"location":"changelog/#changed_13","text":"Updates fastai2 to the final release version","title":"Changed"},{"location":"changelog/#014","text":"","title":"[0.1.4]"},{"location":"changelog/#added_12","text":"soft import icedata in icevision.all show_pbar parameter to COCOMetric","title":"Added"},{"location":"changelog/#changed_14","text":"","title":"Changed"},{"location":"changelog/#deleted_2","text":"","title":"Deleted"},{"location":"changelog/#013","text":"","title":"[0.1.3]"},{"location":"changelog/#changed_15","text":"Effdet as direct dependency","title":"Changed"},{"location":"changelog/#012","text":"","title":"[0.1.2]"},{"location":"changelog/#added_13","text":"show_results function for each model","title":"Added"},{"location":"changelog/#changed_16","text":"Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision","title":"Changed"},{"location":"changelog/#deleted_3","text":"Removed datasets module to instead use the new icedata package","title":"Deleted"},{"location":"changelog/#009","text":"","title":"[0.0.9]"},{"location":"changelog/#added_14","text":"batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds","title":"Added"},{"location":"changelog/#changed_17","text":"Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids","title":"Changed"},{"location":"changelog/#000-pre-release","text":"","title":"[0.0.0-pre-release]"},{"location":"changelog/#added_15","text":"CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class","title":"Added"},{"location":"changelog/#changed_18","text":"All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map","title":"Changed"},{"location":"changelog/#removed","text":"MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule","title":"Removed"},{"location":"changelog/#links","text":"","title":"Links"},{"location":"changelog_backup/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog . Unreleased - 0.10.0a1 Main dependencies updated torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8 Unreleased - 0.9.0a1 Added Low level parsing workflow with RecordCollection Semantic segmentation support with fastai Changed Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore. [0.8.1] Added end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers. Changed Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights. [0.8.0] Supports pytorch 1.8 Added iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models Changed Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4) [0.7.0] BREAKING: API Refactor Added Metrics for mmdetection models Changed Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl [0.6.0b1] Added mmdetection models Changed Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record Deleted [0.5.2] Added aggregate_records_objects function Changed Added label_field to VIA parser to allow for alternate region_attribute names [0.5.0] Added Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample Changed improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable [0.4.0] Added retinanet: model, dataloaders, predict, ... Changed Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models [0.3.0] Added pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing Changed show_preds visual improvement [0.2.2] Added Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time Changed Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles [0.2.1] Changed updated matplotlib and ipykernel minimum version for colab compatibility [0.2.0] IMPORTANT Switched from poetry to setuptools Added Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data Changed Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height Deleted Removed CombinedParser , all parsing can be done with the standard Parser [0.1.6] Added Efficientdet now support empty annotations Changed Returns float instead of dict on FastaiMetricAdapter [0.1.5] Changed Updates fastai2 to the final release version [0.1.4] Added soft import icedata in icevision.all show_pbar parameter to COCOMetric Changed Deleted [0.1.3] Changed Effdet as direct dependency [0.1.2] Added show_results function for each model Changed Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision Deleted Removed datasets module to instead use the new icedata package [0.0.9] Added batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds Changed Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids [0.0.0-pre-release] Added CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class Changed All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map Removed MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule Links","title":"Changelog"},{"location":"changelog_backup/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog .","title":"Changelog"},{"location":"changelog_backup/#unreleased-0100a1","text":"","title":"Unreleased - 0.10.0a1"},{"location":"changelog_backup/#main-dependencies-updated","text":"torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8","title":"Main dependencies updated"},{"location":"changelog_backup/#unreleased-090a1","text":"","title":"Unreleased - 0.9.0a1"},{"location":"changelog_backup/#added","text":"Low level parsing workflow with RecordCollection Semantic segmentation support with fastai","title":"Added"},{"location":"changelog_backup/#changed","text":"Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore.","title":"Changed"},{"location":"changelog_backup/#081","text":"","title":"[0.8.1]"},{"location":"changelog_backup/#added_1","text":"end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers.","title":"Added"},{"location":"changelog_backup/#changed_1","text":"Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights.","title":"Changed"},{"location":"changelog_backup/#080","text":"Supports pytorch 1.8","title":"[0.8.0]"},{"location":"changelog_backup/#added_2","text":"iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models","title":"Added"},{"location":"changelog_backup/#changed_2","text":"Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4)","title":"Changed"},{"location":"changelog_backup/#070","text":"BREAKING: API Refactor","title":"[0.7.0]"},{"location":"changelog_backup/#added_3","text":"Metrics for mmdetection models","title":"Added"},{"location":"changelog_backup/#changed_3","text":"Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl","title":"Changed"},{"location":"changelog_backup/#060b1","text":"","title":"[0.6.0b1]"},{"location":"changelog_backup/#added_4","text":"mmdetection models","title":"Added"},{"location":"changelog_backup/#changed_4","text":"Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record","title":"Changed"},{"location":"changelog_backup/#deleted","text":"","title":"Deleted"},{"location":"changelog_backup/#052","text":"","title":"[0.5.2]"},{"location":"changelog_backup/#added_5","text":"aggregate_records_objects function","title":"Added"},{"location":"changelog_backup/#changed_5","text":"Added label_field to VIA parser to allow for alternate region_attribute names","title":"Changed"},{"location":"changelog_backup/#050","text":"","title":"[0.5.0]"},{"location":"changelog_backup/#added_6","text":"Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample","title":"Added"},{"location":"changelog_backup/#changed_6","text":"improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable","title":"Changed"},{"location":"changelog_backup/#040","text":"","title":"[0.4.0]"},{"location":"changelog_backup/#added_7","text":"retinanet: model, dataloaders, predict, ...","title":"Added"},{"location":"changelog_backup/#changed_7","text":"Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models","title":"Changed"},{"location":"changelog_backup/#030","text":"","title":"[0.3.0]"},{"location":"changelog_backup/#added_8","text":"pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing","title":"Added"},{"location":"changelog_backup/#changed_8","text":"show_preds visual improvement","title":"Changed"},{"location":"changelog_backup/#022","text":"","title":"[0.2.2]"},{"location":"changelog_backup/#added_9","text":"Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time","title":"Added"},{"location":"changelog_backup/#changed_9","text":"Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles","title":"Changed"},{"location":"changelog_backup/#021","text":"","title":"[0.2.1]"},{"location":"changelog_backup/#changed_10","text":"updated matplotlib and ipykernel minimum version for colab compatibility","title":"Changed"},{"location":"changelog_backup/#020","text":"","title":"[0.2.0]"},{"location":"changelog_backup/#important","text":"Switched from poetry to setuptools","title":"IMPORTANT"},{"location":"changelog_backup/#added_10","text":"Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data","title":"Added"},{"location":"changelog_backup/#changed_11","text":"Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height","title":"Changed"},{"location":"changelog_backup/#deleted_1","text":"Removed CombinedParser , all parsing can be done with the standard Parser","title":"Deleted"},{"location":"changelog_backup/#016","text":"","title":"[0.1.6]"},{"location":"changelog_backup/#added_11","text":"Efficientdet now support empty annotations","title":"Added"},{"location":"changelog_backup/#changed_12","text":"Returns float instead of dict on FastaiMetricAdapter","title":"Changed"},{"location":"changelog_backup/#015","text":"","title":"[0.1.5]"},{"location":"changelog_backup/#changed_13","text":"Updates fastai2 to the final release version","title":"Changed"},{"location":"changelog_backup/#014","text":"","title":"[0.1.4]"},{"location":"changelog_backup/#added_12","text":"soft import icedata in icevision.all show_pbar parameter to COCOMetric","title":"Added"},{"location":"changelog_backup/#changed_14","text":"","title":"Changed"},{"location":"changelog_backup/#deleted_2","text":"","title":"Deleted"},{"location":"changelog_backup/#013","text":"","title":"[0.1.3]"},{"location":"changelog_backup/#changed_15","text":"Effdet as direct dependency","title":"Changed"},{"location":"changelog_backup/#012","text":"","title":"[0.1.2]"},{"location":"changelog_backup/#added_13","text":"show_results function for each model","title":"Added"},{"location":"changelog_backup/#changed_16","text":"Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision","title":"Changed"},{"location":"changelog_backup/#deleted_3","text":"Removed datasets module to instead use the new icedata package","title":"Deleted"},{"location":"changelog_backup/#009","text":"","title":"[0.0.9]"},{"location":"changelog_backup/#added_14","text":"batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds","title":"Added"},{"location":"changelog_backup/#changed_17","text":"Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids","title":"Changed"},{"location":"changelog_backup/#000-pre-release","text":"","title":"[0.0.0-pre-release]"},{"location":"changelog_backup/#added_15","text":"CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class","title":"Added"},{"location":"changelog_backup/#changed_18","text":"All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map","title":"Changed"},{"location":"changelog_backup/#removed","text":"MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule","title":"Removed"},{"location":"changelog_backup/#links","text":"","title":"Links"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contribution Guide We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps Step 1: Forking and Installing IceVision \u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, locally install the package: pip install -e \".[all,dev]\" Then, set up pre-commit hooks using: pre-commit install Step 2: Stay in Sync with the original (upstream) repo Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master Step 3: Creating a new branch git checkout -b feature-name git branch master * feature_name: Step 4: Make changes, and commit your file changes Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files cd to/icevision/folder black . git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary. Step 5: Submitting a Pull Request 1. Create a pull request git Upload your local branch to your remote GitHub repo (github.com/username/icevision) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request 2. Confirm PR was created: Ensure your PR is listed here 3. Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before> Reviewing Your PR Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well. Resolving Conflicts In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts. Feature Requests and questions For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Contributing Guide"},{"location":"contributing/#contribution-guide","text":"We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps","title":"Contribution Guide"},{"location":"contributing/#step-1-forking-and-installing-icevision","text":"\u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, locally install the package: pip install -e \".[all,dev]\" Then, set up pre-commit hooks using: pre-commit install","title":"Step 1: Forking and Installing IceVision"},{"location":"contributing/#step-2-stay-in-sync-with-the-original-upstream-repo","text":"Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master","title":"Step 2: Stay in Sync with the original (upstream) repo"},{"location":"contributing/#step-3-creating-a-new-branch","text":"git checkout -b feature-name git branch master * feature_name:","title":"Step 3: Creating a new branch"},{"location":"contributing/#step-4-make-changes-and-commit-your-file-changes","text":"Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files cd to/icevision/folder black . git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary.","title":"Step 4: Make changes, and commit your file changes"},{"location":"contributing/#step-5-submitting-a-pull-request","text":"","title":"Step 5: Submitting a Pull Request"},{"location":"contributing/#1-create-a-pull-request-git","text":"Upload your local branch to your remote GitHub repo (github.com/username/icevision) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request","title":"1. Create a pull request git"},{"location":"contributing/#2-confirm-pr-was-created","text":"Ensure your PR is listed here","title":"2. Confirm PR was created:"},{"location":"contributing/#3-updating-a-pr","text":"Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before>","title":"3.  Updating a PR:"},{"location":"contributing/#reviewing-your-pr","text":"Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well.","title":"Reviewing Your PR"},{"location":"contributing/#resolving-conflicts","text":"In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts.","title":"Resolving Conflicts"},{"location":"contributing/#feature-requests-and-questions","text":"For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Feature Requests and questions"},{"location":"custom_parser/","text":"Custom Parser - Simple Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'} Imports As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd Download dataset We're going to be using a small sample of the chess dataset, the full dataset is offered by roboflow here data_url = \"https://github.com/airctic/chess_sample/archive/master.zip\" data_dir = icedata . load_data ( data_url , 'chess_sample' ) / 'chess_sample-master' Understand the data format In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. df = pd . read_csv ( data_dir / \"annotations.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> filename width height label xmin ymin xmax ymax 0 0.jpg 416 416 black-bishop 280 227 310 284 1 0.jpg 416 416 black-king 311 110 345 195 2 0.jpg 416 416 black-queen 237 85 262 159 3 0.jpg 416 416 black-rook 331 277 366 333 4 0.jpg 416 416 black-rook 235 3 255 51 At first glance, we can make the following assumptions: Multiple rows with the same filename, width, height A label for each row A bbox [xmin, ymin, xmax, ymax] for each row Once we know what our data provides we can create our custom Parser . Create the Parser The first step is to create a template record for our specific type of dataset, in this case we're doing standard object detection: template_record = ObjectDetectionRecord () Now use the method generate_template that will print out all the necessary steps we have to implement. Parser . generate_template ( template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_filepath(<Union[str, Path]>) record.set_img_size(<ImgSize>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) record.detection.add_bboxes(<Sequence[BBox]>) We can copy the template and use it as our starting point. Let's go over each of the methods we have to define: __init__ : What happens here is completely up to you, normally we have to pass some reference to our data, data_dir in our case. __iter__ : This tells our parser how to iterate over our data, each item returned here will be passed to parse_fields as o . In our case we call df.itertuples to iterate over all df rows. __len__ : How many items will be iterating over. imageid : Should return a Hashable ( int , str , etc). In our case we want all the dataset items that have the same filename to be unified in the same record. parse_fields : Here is where the attributes of the record are collected, the template will suggest what methods we need to call on the record and what parameters it expects. The parameter o it receives is the item returned by __iter__ . Important Be sure to pass the correct type on all record methods! class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : for o in self . df . itertuples (): yield o def __len__ ( self ) -> int : return len ( self . df ) def record_id ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record , is_new ): if is_new : record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detection . set_class_map ( self . class_map ) record . detection . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detection . add_labels ([ o . label ]) Let's randomly split the data and parser with Parser.parse : parser = ChessParser ( template_record , data_dir ) train_records , valid_records = parser . parse () 0%| | 0/109 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7 [00:00<?, ?it/s] Let's take a look at one record: show_record ( train_records [ 0 ], display_label = False , figsize = ( 14 , 10 )) train_records [ 0 ] BaseRecord # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Filepath: /root/.icevision/data/chess_sample/chess_sample-master/images/6.jpg - Img: None - Image size ImgSize(width=416, height=416) - Record ID: 6.jpg detection: - Class Map: <ClassMap: {'background': 0, 'black-bishop': 1, 'black-king': 2, 'black-queen': 3, 'black-rook': 4, 'black-pawn': 5, 'black-knight': 6, 'white-queen': 7, 'white-rook': 8, 'white-king': 9, 'white-bishop': 10, 'white-knight': 11, 'white-pawn': 12}> - Labels: [8, 9, 12, 12, 12, 12, 2, 4, 3, 5, 5, 5] - BBoxes: [<BBox (xmin:170, ymin:79, xmax:195, ymax:132)>, <BBox (xmin:131, ymin:129, xmax:162, ymax:213)>, <BBox (xmin:167, ymin:119, xmax:189, ymax:165)>, <BBox (xmin:136, ymin:52, xmax:158, ymax:97)>, <BBox (xmin:101, ymin:15, xmax:123, ymax:59)>, <BBox (xmin:94, ymin:256, xmax:118, ymax:304)>, <BBox (xmin:257, ymin:268, xmax:291, ymax:354)>, <BBox (xmin:335, ymin:297, xmax:368, ymax:355)>, <BBox (xmin:234, ymin:97, xmax:260, ymax:173)>, <BBox (xmin:273, ymin:84, xmax:291, ymax:128)>, <BBox (xmin:204, ymin:118, xmax:223, ymax:162)>, <BBox (xmin:248, ymin:204, xmax:270, ymax:251)>] Models We've selected a few of the many options below. You can easily pick which libraries, models, and backbones you like to use. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } elif selection == 3 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 4 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 5 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) (<module 'icevision.models.mmdet.models.vfnet' from '/usr/local/lib/python3.7/dist-packages/icevision/models/mmdet/models/vfnet/__init__.py'>, <icevision.models.mmdet.models.vfnet.backbones.resnet_fpn.MMDetVFNETBackboneConfig at 0x7f3d3eefcf50>, {}) Training metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=3.0199516913853586e-05) learn . fine_tune ( 40 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.581739 1.825835 0.196081 00:01 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 1.581900 1.786651 0.216952 00:01 1 1.608743 1.763155 0.224355 00:01 2 1.631124 1.756258 0.224446 00:01 3 1.605482 1.744663 0.221207 00:01 4 1.601004 1.733463 0.225954 00:01 5 1.602483 1.737170 0.227496 00:01 6 1.585189 1.726894 0.230818 00:01 7 1.569807 1.705676 0.242893 00:01 8 1.568512 1.702688 0.239849 00:01 9 1.554865 1.681125 0.247037 00:01 10 1.555655 1.684015 0.297935 00:01 11 1.543609 1.681997 0.301196 00:01 12 1.534735 1.690655 0.310736 00:01 13 1.524568 1.689521 0.314591 00:01 14 1.516276 1.659921 0.330271 00:01 15 1.511769 1.629157 0.348593 00:01 16 1.502966 1.596284 0.362406 00:01 17 1.497759 1.586747 0.371476 00:01 18 1.497266 1.607103 0.369360 00:01 19 1.492118 1.642410 0.333980 00:01 20 1.489123 1.666914 0.333943 00:01 21 1.487922 1.678961 0.331371 00:01 22 1.483647 1.671102 0.349929 00:01 23 1.480486 1.642387 0.365551 00:01 24 1.472842 1.604754 0.377337 00:01 25 1.466778 1.569934 0.398319 00:01 26 1.457183 1.547147 0.410581 00:01 27 1.452692 1.534132 0.419814 00:01 28 1.447047 1.526892 0.423409 00:01 29 1.443192 1.525016 0.422199 00:01 30 1.444314 1.528092 0.420855 00:01 31 1.442283 1.533094 0.419651 00:01 32 1.434897 1.537819 0.417674 00:01 33 1.431285 1.542786 0.418381 00:01 34 1.424665 1.546462 0.418103 00:01 35 1.418521 1.548954 0.416256 00:01 36 1.414101 1.550630 0.416183 00:01 37 1.407963 1.551399 0.416183 00:01 38 1.402073 1.551711 0.416183 00:01 39 1.393795 1.551780 0.416183 00:01 Training using Pytorch Lightning # class LightModel(model_type.lightning.ModelAdapter): # def configure_optimizers(self): # return Adam(self.parameters(), lr=1e-4) # light_model = LightModel(model, metrics=metrics) # trainer = pl.Trainer(max_epochs=20, gpus=1) # trainer.fit(light_model, train_dl, valid_dl) Showing the results model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Batch Inference (Prediction) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/1 [00:00<?, ?it/s] Conclusion This notebook shows ho to create a custom parser to process data stored in format different than the popular COCO or VOV formats. The parsed data feed any IceVision models that could be trained by either Fastai or Pytorch-Lightning training loops. Happy Learning! If you need any assistance, feel free to join our forum .","title":"Custom Parser"},{"location":"custom_parser/#custom-parser-simple","text":"","title":"Custom Parser - Simple"},{"location":"custom_parser/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'}","title":"Installing IceVision and IceData"},{"location":"custom_parser/#imports","text":"As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd","title":"Imports"},{"location":"custom_parser/#download-dataset","text":"We're going to be using a small sample of the chess dataset, the full dataset is offered by roboflow here data_url = \"https://github.com/airctic/chess_sample/archive/master.zip\" data_dir = icedata . load_data ( data_url , 'chess_sample' ) / 'chess_sample-master'","title":"Download dataset"},{"location":"custom_parser/#understand-the-data-format","text":"In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. df = pd . read_csv ( data_dir / \"annotations.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> filename width height label xmin ymin xmax ymax 0 0.jpg 416 416 black-bishop 280 227 310 284 1 0.jpg 416 416 black-king 311 110 345 195 2 0.jpg 416 416 black-queen 237 85 262 159 3 0.jpg 416 416 black-rook 331 277 366 333 4 0.jpg 416 416 black-rook 235 3 255 51 At first glance, we can make the following assumptions: Multiple rows with the same filename, width, height A label for each row A bbox [xmin, ymin, xmax, ymax] for each row Once we know what our data provides we can create our custom Parser .","title":"Understand the data format"},{"location":"custom_parser/#create-the-parser","text":"The first step is to create a template record for our specific type of dataset, in this case we're doing standard object detection: template_record = ObjectDetectionRecord () Now use the method generate_template that will print out all the necessary steps we have to implement. Parser . generate_template ( template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_filepath(<Union[str, Path]>) record.set_img_size(<ImgSize>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) record.detection.add_bboxes(<Sequence[BBox]>) We can copy the template and use it as our starting point. Let's go over each of the methods we have to define: __init__ : What happens here is completely up to you, normally we have to pass some reference to our data, data_dir in our case. __iter__ : This tells our parser how to iterate over our data, each item returned here will be passed to parse_fields as o . In our case we call df.itertuples to iterate over all df rows. __len__ : How many items will be iterating over. imageid : Should return a Hashable ( int , str , etc). In our case we want all the dataset items that have the same filename to be unified in the same record. parse_fields : Here is where the attributes of the record are collected, the template will suggest what methods we need to call on the record and what parameters it expects. The parameter o it receives is the item returned by __iter__ . Important Be sure to pass the correct type on all record methods! class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : for o in self . df . itertuples (): yield o def __len__ ( self ) -> int : return len ( self . df ) def record_id ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record , is_new ): if is_new : record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detection . set_class_map ( self . class_map ) record . detection . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detection . add_labels ([ o . label ]) Let's randomly split the data and parser with Parser.parse : parser = ChessParser ( template_record , data_dir ) train_records , valid_records = parser . parse () 0%| | 0/109 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7 [00:00<?, ?it/s] Let's take a look at one record: show_record ( train_records [ 0 ], display_label = False , figsize = ( 14 , 10 )) train_records [ 0 ] BaseRecord # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Filepath: /root/.icevision/data/chess_sample/chess_sample-master/images/6.jpg - Img: None - Image size ImgSize(width=416, height=416) - Record ID: 6.jpg detection: - Class Map: <ClassMap: {'background': 0, 'black-bishop': 1, 'black-king': 2, 'black-queen': 3, 'black-rook': 4, 'black-pawn': 5, 'black-knight': 6, 'white-queen': 7, 'white-rook': 8, 'white-king': 9, 'white-bishop': 10, 'white-knight': 11, 'white-pawn': 12}> - Labels: [8, 9, 12, 12, 12, 12, 2, 4, 3, 5, 5, 5] - BBoxes: [<BBox (xmin:170, ymin:79, xmax:195, ymax:132)>, <BBox (xmin:131, ymin:129, xmax:162, ymax:213)>, <BBox (xmin:167, ymin:119, xmax:189, ymax:165)>, <BBox (xmin:136, ymin:52, xmax:158, ymax:97)>, <BBox (xmin:101, ymin:15, xmax:123, ymax:59)>, <BBox (xmin:94, ymin:256, xmax:118, ymax:304)>, <BBox (xmin:257, ymin:268, xmax:291, ymax:354)>, <BBox (xmin:335, ymin:297, xmax:368, ymax:355)>, <BBox (xmin:234, ymin:97, xmax:260, ymax:173)>, <BBox (xmin:273, ymin:84, xmax:291, ymax:128)>, <BBox (xmin:204, ymin:118, xmax:223, ymax:162)>, <BBox (xmin:248, ymin:204, xmax:270, ymax:251)>]","title":"Create the Parser"},{"location":"custom_parser/#models","text":"We've selected a few of the many options below. You can easily pick which libraries, models, and backbones you like to use. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } elif selection == 3 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 4 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 5 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) (<module 'icevision.models.mmdet.models.vfnet' from '/usr/local/lib/python3.7/dist-packages/icevision/models/mmdet/models/vfnet/__init__.py'>, <icevision.models.mmdet.models.vfnet.backbones.resnet_fpn.MMDetVFNETBackboneConfig at 0x7f3d3eefcf50>, {})","title":"Models"},{"location":"custom_parser/#training","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Training"},{"location":"custom_parser/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=3.0199516913853586e-05) learn . fine_tune ( 40 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.581739 1.825835 0.196081 00:01 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 1.581900 1.786651 0.216952 00:01 1 1.608743 1.763155 0.224355 00:01 2 1.631124 1.756258 0.224446 00:01 3 1.605482 1.744663 0.221207 00:01 4 1.601004 1.733463 0.225954 00:01 5 1.602483 1.737170 0.227496 00:01 6 1.585189 1.726894 0.230818 00:01 7 1.569807 1.705676 0.242893 00:01 8 1.568512 1.702688 0.239849 00:01 9 1.554865 1.681125 0.247037 00:01 10 1.555655 1.684015 0.297935 00:01 11 1.543609 1.681997 0.301196 00:01 12 1.534735 1.690655 0.310736 00:01 13 1.524568 1.689521 0.314591 00:01 14 1.516276 1.659921 0.330271 00:01 15 1.511769 1.629157 0.348593 00:01 16 1.502966 1.596284 0.362406 00:01 17 1.497759 1.586747 0.371476 00:01 18 1.497266 1.607103 0.369360 00:01 19 1.492118 1.642410 0.333980 00:01 20 1.489123 1.666914 0.333943 00:01 21 1.487922 1.678961 0.331371 00:01 22 1.483647 1.671102 0.349929 00:01 23 1.480486 1.642387 0.365551 00:01 24 1.472842 1.604754 0.377337 00:01 25 1.466778 1.569934 0.398319 00:01 26 1.457183 1.547147 0.410581 00:01 27 1.452692 1.534132 0.419814 00:01 28 1.447047 1.526892 0.423409 00:01 29 1.443192 1.525016 0.422199 00:01 30 1.444314 1.528092 0.420855 00:01 31 1.442283 1.533094 0.419651 00:01 32 1.434897 1.537819 0.417674 00:01 33 1.431285 1.542786 0.418381 00:01 34 1.424665 1.546462 0.418103 00:01 35 1.418521 1.548954 0.416256 00:01 36 1.414101 1.550630 0.416183 00:01 37 1.407963 1.551399 0.416183 00:01 38 1.402073 1.551711 0.416183 00:01 39 1.393795 1.551780 0.416183 00:01","title":"Training using fastai"},{"location":"custom_parser/#training-using-pytorch-lightning","text":"# class LightModel(model_type.lightning.ModelAdapter): # def configure_optimizers(self): # return Adam(self.parameters(), lr=1e-4) # light_model = LightModel(model, metrics=metrics) # trainer = pl.Trainer(max_epochs=20, gpus=1) # trainer.fit(light_model, train_dl, valid_dl)","title":"Training using Pytorch Lightning"},{"location":"custom_parser/#showing-the-results","text":"model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Showing the results"},{"location":"custom_parser/#batch-inference-prediction","text":"infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/1 [00:00<?, ?it/s]","title":"Batch Inference (Prediction)"},{"location":"custom_parser/#conclusion","text":"This notebook shows ho to create a custom parser to process data stored in format different than the popular COCO or VOV formats. The parsed data feed any IceVision models that could be trained by either Fastai or Pytorch-Lightning training loops.","title":"Conclusion"},{"location":"custom_parser/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"data_splits/","text":"[source] DataSplitter icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source] RandomSplitter icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source] FixedSplitter icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source] SingleSplitSplitter icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"Data Splitters"},{"location":"data_splits/#datasplitter","text":"icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source]","title":"DataSplitter"},{"location":"data_splits/#randomsplitter","text":"icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source]","title":"RandomSplitter"},{"location":"data_splits/#fixedsplitter","text":"icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source]","title":"FixedSplitter"},{"location":"data_splits/#singlesplitsplitter","text":"icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"SingleSplitSplitter"},{"location":"dataset/","text":"[source] Dataset icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm Optional[icevision.tfms.transform.Transform] : Transforms to be applied to each item. [source] from_images Dataset . from_images ( images , tfm = None , class_map = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"Dataset"},{"location":"dataset/#dataset","text":"icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm Optional[icevision.tfms.transform.Transform] : Transforms to be applied to each item. [source]","title":"Dataset"},{"location":"dataset/#from_images","text":"Dataset . from_images ( images , tfm = None , class_map = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"from_images"},{"location":"deployment/","text":"Deployment We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Deployment"},{"location":"deployment/#deployment","text":"We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Deployment"},{"location":"efficientdet/","text":"[source] model icevision . models . ross . efficientdet . model . model ( backbone , num_classes , img_size , ** kwargs ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments backbone icevision.models.ross.efficientdet.utils.EfficientDetBackboneConfig : Specifies the backbone to use create the model. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 128. Returns A PyTorch model. [source] train_dl icevision . models . ross . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . ross . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . ross . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . ross . efficientdet . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . ross . efficientdet . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . ross . efficientdet . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"efficientdet/#model","text":"icevision . models . ross . efficientdet . model . model ( backbone , num_classes , img_size , ** kwargs ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments backbone icevision.models.ross.efficientdet.utils.EfficientDetBackboneConfig : Specifies the backbone to use create the model. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 128. Returns A PyTorch model. [source]","title":"model"},{"location":"efficientdet/#train_dl","text":"icevision . models . ross . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"efficientdet/#valid_dl","text":"icevision . models . ross . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"efficientdet/#infer_dl","text":"icevision . models . ross . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"efficientdet/#build_train_batch","text":"icevision . models . ross . efficientdet . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"efficientdet/#build_valid_batch","text":"icevision . models . ross . efficientdet . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"efficientdet/#build_infer_batch","text":"icevision . models . ross . efficientdet . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"efficientdet_fastai/","text":"[source] learner icevision . models . ross . efficientdet . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"efficientdet_fastai/#learner","text":"icevision . models . ross . efficientdet . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"efficientdet_lightning/","text":"[source] ModelAdapter icevision . models . ross . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"efficientdet_lightning/#modeladapter","text":"icevision . models . ross . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"faster_rcnn/","text":"[source] model icevision . models . torchvision . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbones.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained : Argument passed to fastercnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source] train_dl icevision . models . torchvision . faster_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . torchvision . faster_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . torchvision . faster_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . torchvision . faster_rcnn . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . torchvision . faster_rcnn . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . torchvision . faster_rcnn . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"faster_rcnn/#model","text":"icevision . models . torchvision . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbones.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained : Argument passed to fastercnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source]","title":"model"},{"location":"faster_rcnn/#train_dl","text":"icevision . models . torchvision . faster_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"faster_rcnn/#valid_dl","text":"icevision . models . torchvision . faster_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"faster_rcnn/#infer_dl","text":"icevision . models . torchvision . faster_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"faster_rcnn/#build_train_batch","text":"icevision . models . torchvision . faster_rcnn . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"faster_rcnn/#build_valid_batch","text":"icevision . models . torchvision . faster_rcnn . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"faster_rcnn/#build_infer_batch","text":"icevision . models . torchvision . faster_rcnn . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"faster_rcnn_fastai/","text":"[source] learner icevision . models . torchvision . faster_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"faster_rcnn_fastai/#learner","text":"icevision . models . torchvision . faster_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"faster_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . torchvision . faster_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"faster_rcnn_lightning/#modeladapter","text":"icevision . models . torchvision . faster_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"getting_started_instance_segmentation/","text":"Getting Started with Instance Segmentation using IceVision Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the pennfudan folder in icedata. Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions # IceVision - IceData - MMDetection - YOLO v5 Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Model To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section Choose a model and backbone We use MMDet here. When you want to use the torch vision version the COCOMetric will not be correct at the moment due to a problem in the bounding box conversion. # Just change the value of selection to try another model selection = 1 if selection == 0 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x if selection == 1 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . mask_rcnn_swin_t_p4_w7_fpn_1x_coco Datasets : Pennfudan Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s] Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) DataLoader # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 , shuffle = False ) valid_batch = first ( valid_dl ) infer_batch = first ( infer_dl ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Model Now that we determined the number of classes ( num_classes ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = icedata . pennfudan . NUM_CLASSES ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . mask )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(valley=9.120108734350652e-05) learn . fine_tune ( 5 , 3e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss COCOMetric time 0 1.518704 0.520843 0.610489 00:11 1 0.799316 0.388823 0.689349 00:10 epoch train_loss valid_loss COCOMetric time 0 0.373752 0.356427 0.690000 00:11 1 0.362768 0.354911 0.721681 00:11 2 0.345709 0.342564 0.722313 00:11 3 0.327585 0.345134 0.733165 00:11 4 0.324897 0.341429 0.732759 00:11 Training using Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 5e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Show Results model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial batch , records = first ( valid_dl ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ], ncols = 3 ) 0%| | 0/9 [00:00<?, ?it/s] Happy Learning! If you need any assistance, feel free to join our forum .","title":"Instance Segmentation"},{"location":"getting_started_instance_segmentation/#getting-started-with-instance-segmentation-using-icevision","text":"","title":"Getting Started with Instance Segmentation using IceVision"},{"location":"getting_started_instance_segmentation/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the pennfudan folder in icedata.","title":"Introduction"},{"location":"getting_started_instance_segmentation/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions # IceVision - IceData - MMDetection - YOLO v5 Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"getting_started_instance_segmentation/#imports","text":"from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"getting_started_instance_segmentation/#model","text":"To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section","title":"Model"},{"location":"getting_started_instance_segmentation/#choose-a-model-and-backbone","text":"We use MMDet here. When you want to use the torch vision version the COCOMetric will not be correct at the moment due to a problem in the bounding box conversion. # Just change the value of selection to try another model selection = 1 if selection == 0 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x if selection == 1 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . mask_rcnn_swin_t_p4_w7_fpn_1x_coco","title":"Choose a model and backbone"},{"location":"getting_started_instance_segmentation/#datasets-pennfudan","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s]","title":"Datasets : Pennfudan"},{"location":"getting_started_instance_segmentation/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Displaying the same image with different transforms"},{"location":"getting_started_instance_segmentation/#dataloader","text":"# DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 , shuffle = False ) valid_batch = first ( valid_dl ) infer_batch = first ( infer_dl ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"DataLoader"},{"location":"getting_started_instance_segmentation/#model_1","text":"Now that we determined the number of classes ( num_classes ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = icedata . pennfudan . NUM_CLASSES )","title":"Model"},{"location":"getting_started_instance_segmentation/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . mask )]","title":"Metrics"},{"location":"getting_started_instance_segmentation/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"getting_started_instance_segmentation/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(valley=9.120108734350652e-05) learn . fine_tune ( 5 , 3e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss COCOMetric time 0 1.518704 0.520843 0.610489 00:11 1 0.799316 0.388823 0.689349 00:10 epoch train_loss valid_loss COCOMetric time 0 0.373752 0.356427 0.690000 00:11 1 0.362768 0.354911 0.721681 00:11 2 0.345709 0.342564 0.722313 00:11 3 0.327585 0.345134 0.733165 00:11 4 0.324897 0.341429 0.732759 00:11","title":"Training using fastai"},{"location":"getting_started_instance_segmentation/#training-using-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 5e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"getting_started_instance_segmentation/#show-results","text":"model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Show Results"},{"location":"getting_started_instance_segmentation/#inference","text":"","title":"Inference"},{"location":"getting_started_instance_segmentation/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial batch , records = first ( valid_dl ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ], ncols = 3 ) 0%| | 0/9 [00:00<?, ?it/s]","title":"Predicting a batch of images"},{"location":"getting_started_instance_segmentation/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"getting_started_keypoint_detection/","text":"Getting Started with Keypoint Detection using IceVision Introduction This tutorial walk you through the different steps of training the biwi dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the biwi dataset as well as its corresponding parser check out the biwi folder in icedata. Installing IceVision and IceData # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * Model To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section Choose a model and backbone TorchVision model_type = models . torchvision . keypoint_rcnn backbone = model_type . backbones . resnet34_fpn () Datasets : Biwi Biwi dataset is tiny dataset that contains 200 images of 1 class Nose . We will use icedata to download the dataset and get it with pre-defined transforms for training and validation. Note: If you want a more challenging dataset take a look at OCHuman . # Loading Data data_dir = icedata . biwi . load_data () train_ds , valid_ds = icedata . biwi . dataset ( data_dir ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) DataLoader # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Model Now that we determined the number of classes ( num_classes ) and number of keypoints ( num_keypoints ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone , num_keypoints = 1 , num_classes = icedata . biwi . NUM_CLASSES ) Metrics Metrics for keypoint are a work in progress # metrics = [COCOMetric(metric_type=COCOMetricType.keypoint)] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . lr_find () SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 10 , 3e-5 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 8.645806 7.013292 00:28 epoch train_loss valid_loss time 0 6.684147 6.487582 00:26 1 6.277197 5.731036 00:26 2 5.836541 5.114465 00:26 3 5.408250 4.583161 00:20 4 4.996461 4.225419 00:23 5 4.657006 3.987719 00:21 6 4.335581 3.832931 00:20 7 4.114164 3.862228 00:20 8 3.902603 3.697594 00:20 9 3.768973 3.694008 00:21 Training using Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 3e-5 ) light_model = LightModel ( model ) trainer = pl . Trainer ( max_epochs = 2 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Show Results model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Keypoint Detection"},{"location":"getting_started_keypoint_detection/#getting-started-with-keypoint-detection-using-icevision","text":"","title":"Getting Started with Keypoint Detection using IceVision"},{"location":"getting_started_keypoint_detection/#introduction","text":"This tutorial walk you through the different steps of training the biwi dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the biwi dataset as well as its corresponding parser check out the biwi folder in icedata.","title":"Introduction"},{"location":"getting_started_keypoint_detection/#installing-icevision-and-icedata","text":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"getting_started_keypoint_detection/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"getting_started_keypoint_detection/#model","text":"To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section","title":"Model"},{"location":"getting_started_keypoint_detection/#choose-a-model-and-backbone","text":"TorchVision model_type = models . torchvision . keypoint_rcnn backbone = model_type . backbones . resnet34_fpn ()","title":"Choose a model and backbone"},{"location":"getting_started_keypoint_detection/#datasets-biwi","text":"Biwi dataset is tiny dataset that contains 200 images of 1 class Nose . We will use icedata to download the dataset and get it with pre-defined transforms for training and validation. Note: If you want a more challenging dataset take a look at OCHuman . # Loading Data data_dir = icedata . biwi . load_data () train_ds , valid_ds = icedata . biwi . dataset ( data_dir )","title":"Datasets : Biwi"},{"location":"getting_started_keypoint_detection/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Displaying the same image with different transforms"},{"location":"getting_started_keypoint_detection/#dataloader","text":"# DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"DataLoader"},{"location":"getting_started_keypoint_detection/#model_1","text":"Now that we determined the number of classes ( num_classes ) and number of keypoints ( num_keypoints ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone , num_keypoints = 1 , num_classes = icedata . biwi . NUM_CLASSES )","title":"Model"},{"location":"getting_started_keypoint_detection/#metrics","text":"Metrics for keypoint are a work in progress # metrics = [COCOMetric(metric_type=COCOMetricType.keypoint)]","title":"Metrics"},{"location":"getting_started_keypoint_detection/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"getting_started_keypoint_detection/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . lr_find () SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 10 , 3e-5 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 8.645806 7.013292 00:28 epoch train_loss valid_loss time 0 6.684147 6.487582 00:26 1 6.277197 5.731036 00:26 2 5.836541 5.114465 00:26 3 5.408250 4.583161 00:20 4 4.996461 4.225419 00:23 5 4.657006 3.987719 00:21 6 4.335581 3.832931 00:20 7 4.114164 3.862228 00:20 8 3.902603 3.697594 00:20 9 3.768973 3.694008 00:21","title":"Training using fastai"},{"location":"getting_started_keypoint_detection/#training-using-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 3e-5 ) light_model = LightModel ( model ) trainer = pl . Trainer ( max_epochs = 2 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"getting_started_keypoint_detection/#show-results","text":"model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Show Results"},{"location":"getting_started_keypoint_detection/#inference","text":"","title":"Inference"},{"location":"getting_started_keypoint_detection/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ])","title":"Predicting a batch of images"},{"location":"getting_started_keypoint_detection/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"getting_started_object_detection/","text":"Getting Started with Object Detection using IceVision Introduction IceVision is a Framework for object detection and deep learning that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets. It enables you to work with multiple training engines, including fastai , and pytorch-lightning . It enables you to work with some of the best deep learning libraries including mmdetection , Ross Wightman's efficientdet implementation and model library, torchvision , and ultralytics Yolo . It enables you to select from many possible models and backbones from these libraries. IceVision lets you switch between them with ease. This means that you can pick the engine, library, model, and data format that work for you now and easily change them in the future. You can experiment with with them to see which ones meet your requirements. In this tutorial, you will learn how to 1. Install IceVision. This will include the IceData package that provides easy access to several sample datasets, as well as the engines and libraries that IceVision works with. 2. Download and prepare a dataset to work with. 3. Select an object detection library, model, and backbone. 4. Instantiate the model, and then train it with both the fastai and pytorch lightning engines. 5. And finally, use the model to identify objects in images. The notebook is set up so that you can easily select different libraries, models, and backbones to try. Install IceVision and IceData The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Download and prepare a dataset Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) Parse the dataset The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible autofix capability that identifies common errors in annotation files, reports, and often corrects them automatically. The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed. The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations. # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map Creating datasets with agumentations and transforms Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizintal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Understanding the transforms The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) Select a library, model, and backbone In order to create a model, we need to: * Choose one of the libraries supported by IceVision * Choose one of the models supported by the library * Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Creating a model Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone could be specified by: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how easy it is to try new libraries, models, and backbones. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . swin_t_p4_w7_fpn_1x_coco # extra_args['cfg_options'] = { # 'model.roi_head.bbox_head.loss_bbox.loss_weight': 2, # 'model.roi_head.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 3 : model_type = models . mmdet . ssd backbone = model_type . backbones . ssd300 if selection == 4 : model_type = models . mmdet . yolox backbone = model_type . backbones . yolox_s_8x8 if selection == 5 : model_type = models . mmdet . yolof backbone = model_type . backbones . yolof_r50_c5_8x8_1x_coco if selection == 6 : model_type = models . mmdet . detr backbone = model_type . backbones . r50_8x2_150e_coco if selection == 7 : model_type = models . mmdet . deformable_detr backbone = model_type . backbones . twostage_refine_r50_16x2_50e_coco if selection == 8 : model_type = models . mmdet . fsaf backbone = model_type . backbones . x101_64x4d_fpn_1x_coco elif selection == 9 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 10 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 11 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args backbone . __dict__ Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) Data Loader The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Metrics The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning . Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) learn . fine_tune ( 20 , 0.00158 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 4.282796 3.556162 0.000000 00:03 epoch train_loss valid_loss COCOMetric time 0 2.906284 1.651251 0.514744 00:03 1 2.120166 1.340260 0.636351 00:03 2 1.760227 1.048941 0.744560 00:03 3 1.543842 1.071704 0.850686 00:03 4 1.387548 0.964828 0.843832 00:03 5 1.284623 0.858511 0.878575 00:03 6 1.216737 0.871548 0.878225 00:03 7 1.137540 0.832400 0.880371 00:03 8 1.083292 0.753999 0.909935 00:03 9 1.021289 0.762208 0.885468 00:03 10 0.957823 0.682070 0.904922 00:03 11 0.914076 0.667000 0.900920 00:03 12 0.868626 0.731849 0.892283 00:03 13 0.825539 0.636049 0.908815 00:03 14 0.784374 0.613358 0.928341 00:03 15 0.750682 0.584259 0.929360 00:03 16 0.721707 0.569473 0.929833 00:03 17 0.701580 0.573457 0.930522 00:03 18 0.676955 0.569403 0.931394 00:03 19 0.659083 0.566195 0.929665 00:03 Training using Pytorch Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Using the model - inference and showing results The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Prediction Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s] Happy Learning! If you need any assistance, feel free to join our forum .","title":"Object Detection"},{"location":"getting_started_object_detection/#getting-started-with-object-detection-using-icevision","text":"","title":"Getting Started with Object Detection using IceVision"},{"location":"getting_started_object_detection/#introduction","text":"IceVision is a Framework for object detection and deep learning that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets. It enables you to work with multiple training engines, including fastai , and pytorch-lightning . It enables you to work with some of the best deep learning libraries including mmdetection , Ross Wightman's efficientdet implementation and model library, torchvision , and ultralytics Yolo . It enables you to select from many possible models and backbones from these libraries. IceVision lets you switch between them with ease. This means that you can pick the engine, library, model, and data format that work for you now and easily change them in the future. You can experiment with with them to see which ones meet your requirements. In this tutorial, you will learn how to 1. Install IceVision. This will include the IceData package that provides easy access to several sample datasets, as well as the engines and libraries that IceVision works with. 2. Download and prepare a dataset to work with. 3. Select an object detection library, model, and backbone. 4. Instantiate the model, and then train it with both the fastai and pytorch lightning engines. 5. And finally, use the model to identify objects in images. The notebook is set up so that you can easily select different libraries, models, and backbones to try.","title":"Introduction"},{"location":"getting_started_object_detection/#install-icevision-and-icedata","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision and IceData"},{"location":"getting_started_object_detection/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"getting_started_object_detection/#download-and-prepare-a-dataset","text":"Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir )","title":"Download and prepare a dataset"},{"location":"getting_started_object_detection/#parse-the-dataset","text":"The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible autofix capability that identifies common errors in annotation files, reports, and often corrects them automatically. The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed. The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations. # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map","title":"Parse the dataset"},{"location":"getting_started_object_detection/#creating-datasets-with-agumentations-and-transforms","text":"Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizintal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Creating datasets with agumentations and transforms"},{"location":"getting_started_object_detection/#understanding-the-transforms","text":"The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Understanding the transforms"},{"location":"getting_started_object_detection/#select-a-library-model-and-backbone","text":"In order to create a model, we need to: * Choose one of the libraries supported by IceVision * Choose one of the models supported by the library * Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library.","title":"Select a library, model, and backbone"},{"location":"getting_started_object_detection/#creating-a-model","text":"Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone could be specified by: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how easy it is to try new libraries, models, and backbones. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . swin_t_p4_w7_fpn_1x_coco # extra_args['cfg_options'] = { # 'model.roi_head.bbox_head.loss_bbox.loss_weight': 2, # 'model.roi_head.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 3 : model_type = models . mmdet . ssd backbone = model_type . backbones . ssd300 if selection == 4 : model_type = models . mmdet . yolox backbone = model_type . backbones . yolox_s_8x8 if selection == 5 : model_type = models . mmdet . yolof backbone = model_type . backbones . yolof_r50_c5_8x8_1x_coco if selection == 6 : model_type = models . mmdet . detr backbone = model_type . backbones . r50_8x2_150e_coco if selection == 7 : model_type = models . mmdet . deformable_detr backbone = model_type . backbones . twostage_refine_r50_16x2_50e_coco if selection == 8 : model_type = models . mmdet . fsaf backbone = model_type . backbones . x101_64x4d_fpn_1x_coco elif selection == 9 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 10 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 11 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args backbone . __dict__ Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Creating a model"},{"location":"getting_started_object_detection/#data-loader","text":"The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"Data Loader"},{"location":"getting_started_object_detection/#metrics","text":"The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"getting_started_object_detection/#training","text":"IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning .","title":"Training"},{"location":"getting_started_object_detection/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) learn . fine_tune ( 20 , 0.00158 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 4.282796 3.556162 0.000000 00:03 epoch train_loss valid_loss COCOMetric time 0 2.906284 1.651251 0.514744 00:03 1 2.120166 1.340260 0.636351 00:03 2 1.760227 1.048941 0.744560 00:03 3 1.543842 1.071704 0.850686 00:03 4 1.387548 0.964828 0.843832 00:03 5 1.284623 0.858511 0.878575 00:03 6 1.216737 0.871548 0.878225 00:03 7 1.137540 0.832400 0.880371 00:03 8 1.083292 0.753999 0.909935 00:03 9 1.021289 0.762208 0.885468 00:03 10 0.957823 0.682070 0.904922 00:03 11 0.914076 0.667000 0.900920 00:03 12 0.868626 0.731849 0.892283 00:03 13 0.825539 0.636049 0.908815 00:03 14 0.784374 0.613358 0.928341 00:03 15 0.750682 0.584259 0.929360 00:03 16 0.721707 0.569473 0.929833 00:03 17 0.701580 0.573457 0.930522 00:03 18 0.676955 0.569403 0.931394 00:03 19 0.659083 0.566195 0.929665 00:03","title":"Training using fastai"},{"location":"getting_started_object_detection/#training-using-pytorch-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Pytorch Lightning"},{"location":"getting_started_object_detection/#using-the-model-inference-and-showing-results","text":"The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Using the model - inference and showing results"},{"location":"getting_started_object_detection/#prediction","text":"Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s]","title":"Prediction"},{"location":"getting_started_object_detection/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"getting_started_semantic_segmentation/","text":"Getting Started with Semantic Segmentation using IceVision Install # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 Imports from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Getting and parsing the data data_url = 'https://s3.amazonaws.com/fast-ai-sample/camvid_tiny.tgz' data_dir = icedata . load_data ( data_url , 'camvid_tiny' ) / 'camvid_tiny' codes = np . loadtxt ( data_dir / 'codes.txt' , dtype = str ) class_map = ClassMap ( list ( codes )) images_dir = data_dir / 'images' labels_dir = data_dir / 'labels' image_files = get_image_files ( images_dir ) records = RecordCollection ( SemanticSegmentationRecord ) for image_file in pbar ( image_files ): record = records . get_by_record_id ( image_file . stem ) if record . is_new : record . set_filepath ( image_file ) record . set_img_size ( get_img_size ( image_file )) record . segmentation . set_class_map ( class_map ) mask_file = SemanticMaskFile ( labels_dir / f ' { image_file . stem } _P.png' ) record . segmentation . set_mask ( mask_file ) records = records . autofix () train_records , valid_records = records . make_splits ( RandomSplitter ([ 0.8 , 0.2 ])) sample_records = random . choices ( records , k = 3 ) show_records ( sample_records , ncols = 3 ) 0%| | 0/100 [00:00<?, ?it/s] 0%| | 0/100 [00:00<?, ?it/s] Transforms and datasets presize , size = 512 , 384 presize , size = ImgSize ( presize , int ( presize * .75 )), ImgSize ( size , int ( size * .75 )) aug_tfms = tfms . A . aug_tfms ( presize = presize , size = size , pad = None , crop_fn = partial ( tfms . A . RandomCrop , p = 0.5 ), shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 2 ), ) train_tfms = tfms . A . Adapter ([ * aug_tfms , tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ tfms . A . resize ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) ds_samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( ds_samples , ncols = 3 ) UNET model and dataloaders model_type = models . fastai . unet train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) backbone = model_type . backbones . resnet34 () model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes , img_size = size ) Defining and training the fastai learner def accuracy_camvid ( pred , target ): # ignores void pixels keep_idxs = target != class_map . get_by_name ( 'Void' ) target = target [ keep_idxs ] pred = pred . argmax ( dim = 1 )[ keep_idxs ] return ( pred == target ) . float () . mean () learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = [ accuracy_camvid ]) learn . lr_find () SuggestedLRs(valley=9.120108734350652e-05) learn . fine_tune ( 10 , 1e-4 ) epoch train_loss valid_loss accuracy_camvid time 0 3.392160 2.630378 0.255401 00:09 epoch train_loss valid_loss accuracy_camvid time 0 2.602331 2.325062 0.440709 00:07 1 2.381318 1.799831 0.530444 00:07 2 2.142349 1.332237 0.668531 00:07 3 1.917440 1.123698 0.693745 00:07 4 1.747154 0.993772 0.751023 00:07 5 1.598631 0.996451 0.762109 00:07 6 1.491493 0.948187 0.774137 00:07 7 1.395746 0.869413 0.793335 00:07 8 1.311806 0.876654 0.794053 00:07 9 1.256938 0.868124 0.796351 00:07 model_type . show_results ( model , valid_ds , num_samples = 2 ) Inference preds = model_type . predict ( model , valid_ds ) show_preds ( preds = preds [: 3 ]) infer_dl = model_type . infer_dl ([ valid_ds [ 0 ]], batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_sample ( preds [ 0 ] . pred ) 0%| | 0/1 [00:00<?, ?it/s]","title":"Semantic Segmentation"},{"location":"getting_started_semantic_segmentation/#getting-started-with-semantic-segmentation-using-icevision","text":"","title":"Getting Started with Semantic Segmentation using IceVision"},{"location":"getting_started_semantic_segmentation/#install","text":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11","title":"Install"},{"location":"getting_started_semantic_segmentation/#imports","text":"from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"getting_started_semantic_segmentation/#getting-and-parsing-the-data","text":"data_url = 'https://s3.amazonaws.com/fast-ai-sample/camvid_tiny.tgz' data_dir = icedata . load_data ( data_url , 'camvid_tiny' ) / 'camvid_tiny' codes = np . loadtxt ( data_dir / 'codes.txt' , dtype = str ) class_map = ClassMap ( list ( codes )) images_dir = data_dir / 'images' labels_dir = data_dir / 'labels' image_files = get_image_files ( images_dir ) records = RecordCollection ( SemanticSegmentationRecord ) for image_file in pbar ( image_files ): record = records . get_by_record_id ( image_file . stem ) if record . is_new : record . set_filepath ( image_file ) record . set_img_size ( get_img_size ( image_file )) record . segmentation . set_class_map ( class_map ) mask_file = SemanticMaskFile ( labels_dir / f ' { image_file . stem } _P.png' ) record . segmentation . set_mask ( mask_file ) records = records . autofix () train_records , valid_records = records . make_splits ( RandomSplitter ([ 0.8 , 0.2 ])) sample_records = random . choices ( records , k = 3 ) show_records ( sample_records , ncols = 3 ) 0%| | 0/100 [00:00<?, ?it/s] 0%| | 0/100 [00:00<?, ?it/s]","title":"Getting and parsing the data"},{"location":"getting_started_semantic_segmentation/#transforms-and-datasets","text":"presize , size = 512 , 384 presize , size = ImgSize ( presize , int ( presize * .75 )), ImgSize ( size , int ( size * .75 )) aug_tfms = tfms . A . aug_tfms ( presize = presize , size = size , pad = None , crop_fn = partial ( tfms . A . RandomCrop , p = 0.5 ), shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 2 ), ) train_tfms = tfms . A . Adapter ([ * aug_tfms , tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ tfms . A . resize ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) ds_samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( ds_samples , ncols = 3 )","title":"Transforms and datasets"},{"location":"getting_started_semantic_segmentation/#unet-model-and-dataloaders","text":"model_type = models . fastai . unet train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) backbone = model_type . backbones . resnet34 () model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes , img_size = size )","title":"UNET model and dataloaders"},{"location":"getting_started_semantic_segmentation/#defining-and-training-the-fastai-learner","text":"def accuracy_camvid ( pred , target ): # ignores void pixels keep_idxs = target != class_map . get_by_name ( 'Void' ) target = target [ keep_idxs ] pred = pred . argmax ( dim = 1 )[ keep_idxs ] return ( pred == target ) . float () . mean () learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = [ accuracy_camvid ]) learn . lr_find () SuggestedLRs(valley=9.120108734350652e-05) learn . fine_tune ( 10 , 1e-4 ) epoch train_loss valid_loss accuracy_camvid time 0 3.392160 2.630378 0.255401 00:09 epoch train_loss valid_loss accuracy_camvid time 0 2.602331 2.325062 0.440709 00:07 1 2.381318 1.799831 0.530444 00:07 2 2.142349 1.332237 0.668531 00:07 3 1.917440 1.123698 0.693745 00:07 4 1.747154 0.993772 0.751023 00:07 5 1.598631 0.996451 0.762109 00:07 6 1.491493 0.948187 0.774137 00:07 7 1.395746 0.869413 0.793335 00:07 8 1.311806 0.876654 0.794053 00:07 9 1.256938 0.868124 0.796351 00:07 model_type . show_results ( model , valid_ds , num_samples = 2 )","title":"Defining and training the fastai learner"},{"location":"getting_started_semantic_segmentation/#inference","text":"preds = model_type . predict ( model , valid_ds ) show_preds ( preds = preds [: 3 ]) infer_dl = model_type . infer_dl ([ valid_ds [ 0 ]], batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_sample ( preds [ 0 ] . pred ) 0%| | 0/1 [00:00<?, ?it/s]","title":"Inference"},{"location":"inference/","text":"Inference using IceVision Install IceVision The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database. \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/farid/.icevision/mmdetection_configs/mmdetection_configs-2.10.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m List of images for inference Please store your images in a folder, and populate the path_to_folder variable with the corresponding folder name. # Pick your images folder path_to_image_folder = \"./images\" img_files = get_image_files ( path_to_image_folder ) # img_files img = PIL . Image . open ( img_files [ 4 ]) img Loading a checkpoint and creating the corresponding model The checkpoint file can either a local file or an URL # checkpoint_path = 'checkpoints/fridge-retinanet-save-checkpoint-full.pth' checkpoint_path = 'http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth' from icevision.models import * # The model is automatically recreated in the evaluation mode. To unset that mode, you only need to pass `eval_mode=Fales`) checkpoint_and_model = model_from_checkpoint ( checkpoint_path , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , img_size = 640 , is_coco = True ) # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size Use load_from_http loader /home/farid/anaconda3/envs/icevision/lib/python3.8/site-packages/mmcv/cnn/utils/weight_init.py:99: UserWarning: init_cfg without layer key, if you do not define override key either, this init_cfg will do nothing warnings.warn( (<module 'icevision.models.mmdet.models.retinanet' from '/home/farid/repos/airctic/icevision/icevision/models/mmdet/models/retinanet/__init__.py'>, <icevision.models.mmdet.models.retinanet.backbones.resnet_fpn.MMDetRetinanetBackboneConfig at 0x7fa2024c34f0>, <ClassMap: {'background': 0, 'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 12, 'parking meter': 13, 'bench': 14, 'bird': 15, 'cat': 16, 'dog': 17, 'horse': 18, 'sheep': 19, 'cow': 20, 'elephant': 21, 'bear': 22, 'zebra': 23, 'giraffe': 24, 'backpack': 25, 'umbrella': 26, 'handbag': 27, 'tie': 28, 'suitcase': 29, 'frisbee': 30, 'skis': 31, 'snowboard': 32, 'sports ball': 33, 'kite': 34, 'baseball bat': 35, 'baseball glove': 36, 'skateboard': 37, 'surfboard': 38, 'tennis racket': 39, 'bottle': 40, 'wine glass': 41, 'cup': 42, 'fork': 43, 'knife': 44, 'spoon': 45, 'bowl': 46, 'banana': 47, 'apple': 48, 'sandwich': 49, 'orange': 50, 'broccoli': 51, 'carrot': 52, 'hot dog': 53, 'pizza': 54, 'donut': 55, 'cake': 56, 'chair': 57, 'couch': 58, 'potted plant': 59, 'bed': 60, 'dining table': 61, 'toilet': 62, 'tv': 63, 'laptop': 64, 'mouse': 65, 'remote': 66, 'keyboard': 67, 'cell phone': 68, 'microwave': 69, 'oven': 70, 'toaster': 71, 'sink': 72, 'refrigerator': 73, 'book': 74, 'clock': 75, 'vase': 76, 'scissors': 77, 'teddy bear': 78, 'hair drier': 79, 'toothbrush': 80}>, 640) Get Model Object model_from_checkpoint(checkpoint_path) returns a dictionary: checkpoint_and_model . The model object is stored in checkpoint_and_model[\"model\"] . # Get model object # The model is automatically set in the evaluation mode model = checkpoint_and_model [ \"model\" ] # Check device device = next ( model . parameters ()) . device device # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) device(type='cpu') Single Image Inference The end2end_detect() not only compute predictions for a single image but also automatically adjust predicted boxes to the original image size img = PIL . Image . open ( img_files [ 4 ]) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = class_map , detection_threshold = 0.5 ) pred_dict [ 'img' ] Batch Inference The following option shows to do generate inference for a set of images. The latter is processed in batches. # Create a dataset imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [ 0 : 5 ]) 0%| | 0/2 [00:00<?, ?it/s] How to save a model and its metadata in IceVision When saving a model weights, we could also store the model metadata that are retrieved by the model_from_checkpoint(checkpoint_path) method # How to save a model and its metadata checkpoint_path = 'coco-retinanet-checkpoint-full.pth' save_icevision_checkpoint ( model , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , classes = class_map . get_classes (), img_size = img_size , filename = checkpoint_path , meta = { 'icevision_version' : '0.9.1' }) Loading models already containing metadata If you have saved your model weights with the model metadata, you only need to call model_from_checkpoint(checkpoint_path) : No other arguments ( model_name, backbone_name, classes, img_size ) are needed. All the information is already embedded in the checkpoint file. checkpoint_path = 'https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth' checkpoint_and_model = model_from_checkpoint ( checkpoint_path ) # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size # Inference # Model model = checkpoint_and_model [ \"model\" ] # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) # Pick your images folder path_to_image_folder = \"../samples/fridge/odFridgeObjects/images\" img_files = get_image_files ( path_to_image_folder ) # Create a dataset with appropriate images imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [ 0 : 2 ]) Use load_from_http loader (<module 'icevision.models.mmdet.models.retinanet' from '/home/farid/repos/airctic/icevision/icevision/models/mmdet/models/retinanet/__init__.py'>, <icevision.models.mmdet.models.retinanet.backbones.resnet_fpn.MMDetRetinanetBackboneConfig at 0x7fa2024c34f0>, <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>, 384) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Inference"},{"location":"inference/#inference-using-icevision","text":"","title":"Inference using IceVision"},{"location":"inference/#install-icevision","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision"},{"location":"inference/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import * The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database. \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/farid/.icevision/mmdetection_configs/mmdetection_configs-2.10.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"inference/#list-of-images-for-inference","text":"Please store your images in a folder, and populate the path_to_folder variable with the corresponding folder name. # Pick your images folder path_to_image_folder = \"./images\" img_files = get_image_files ( path_to_image_folder ) # img_files img = PIL . Image . open ( img_files [ 4 ]) img","title":"List of images for inference"},{"location":"inference/#loading-a-checkpoint-and-creating-the-corresponding-model","text":"The checkpoint file can either a local file or an URL # checkpoint_path = 'checkpoints/fridge-retinanet-save-checkpoint-full.pth' checkpoint_path = 'http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth' from icevision.models import * # The model is automatically recreated in the evaluation mode. To unset that mode, you only need to pass `eval_mode=Fales`) checkpoint_and_model = model_from_checkpoint ( checkpoint_path , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , img_size = 640 , is_coco = True ) # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size Use load_from_http loader /home/farid/anaconda3/envs/icevision/lib/python3.8/site-packages/mmcv/cnn/utils/weight_init.py:99: UserWarning: init_cfg without layer key, if you do not define override key either, this init_cfg will do nothing warnings.warn( (<module 'icevision.models.mmdet.models.retinanet' from '/home/farid/repos/airctic/icevision/icevision/models/mmdet/models/retinanet/__init__.py'>, <icevision.models.mmdet.models.retinanet.backbones.resnet_fpn.MMDetRetinanetBackboneConfig at 0x7fa2024c34f0>, <ClassMap: {'background': 0, 'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 12, 'parking meter': 13, 'bench': 14, 'bird': 15, 'cat': 16, 'dog': 17, 'horse': 18, 'sheep': 19, 'cow': 20, 'elephant': 21, 'bear': 22, 'zebra': 23, 'giraffe': 24, 'backpack': 25, 'umbrella': 26, 'handbag': 27, 'tie': 28, 'suitcase': 29, 'frisbee': 30, 'skis': 31, 'snowboard': 32, 'sports ball': 33, 'kite': 34, 'baseball bat': 35, 'baseball glove': 36, 'skateboard': 37, 'surfboard': 38, 'tennis racket': 39, 'bottle': 40, 'wine glass': 41, 'cup': 42, 'fork': 43, 'knife': 44, 'spoon': 45, 'bowl': 46, 'banana': 47, 'apple': 48, 'sandwich': 49, 'orange': 50, 'broccoli': 51, 'carrot': 52, 'hot dog': 53, 'pizza': 54, 'donut': 55, 'cake': 56, 'chair': 57, 'couch': 58, 'potted plant': 59, 'bed': 60, 'dining table': 61, 'toilet': 62, 'tv': 63, 'laptop': 64, 'mouse': 65, 'remote': 66, 'keyboard': 67, 'cell phone': 68, 'microwave': 69, 'oven': 70, 'toaster': 71, 'sink': 72, 'refrigerator': 73, 'book': 74, 'clock': 75, 'vase': 76, 'scissors': 77, 'teddy bear': 78, 'hair drier': 79, 'toothbrush': 80}>, 640)","title":"Loading a checkpoint and creating the corresponding model"},{"location":"inference/#get-model-object","text":"model_from_checkpoint(checkpoint_path) returns a dictionary: checkpoint_and_model . The model object is stored in checkpoint_and_model[\"model\"] . # Get model object # The model is automatically set in the evaluation mode model = checkpoint_and_model [ \"model\" ] # Check device device = next ( model . parameters ()) . device device # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) device(type='cpu')","title":"Get Model Object"},{"location":"inference/#single-image-inference","text":"The end2end_detect() not only compute predictions for a single image but also automatically adjust predicted boxes to the original image size img = PIL . Image . open ( img_files [ 4 ]) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = class_map , detection_threshold = 0.5 ) pred_dict [ 'img' ]","title":"Single Image Inference"},{"location":"inference/#batch-inference","text":"The following option shows to do generate inference for a set of images. The latter is processed in batches. # Create a dataset imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [ 0 : 5 ]) 0%| | 0/2 [00:00<?, ?it/s]","title":"Batch Inference"},{"location":"inference/#how-to-save-a-model-and-its-metadata-in-icevision","text":"When saving a model weights, we could also store the model metadata that are retrieved by the model_from_checkpoint(checkpoint_path) method # How to save a model and its metadata checkpoint_path = 'coco-retinanet-checkpoint-full.pth' save_icevision_checkpoint ( model , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , classes = class_map . get_classes (), img_size = img_size , filename = checkpoint_path , meta = { 'icevision_version' : '0.9.1' })","title":"How to save a model and its metadata in IceVision"},{"location":"inference/#loading-models-already-containing-metadata","text":"If you have saved your model weights with the model metadata, you only need to call model_from_checkpoint(checkpoint_path) : No other arguments ( model_name, backbone_name, classes, img_size ) are needed. All the information is already embedded in the checkpoint file. checkpoint_path = 'https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth' checkpoint_and_model = model_from_checkpoint ( checkpoint_path ) # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size # Inference # Model model = checkpoint_and_model [ \"model\" ] # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) # Pick your images folder path_to_image_folder = \"../samples/fridge/odFridgeObjects/images\" img_files = get_image_files ( path_to_image_folder ) # Create a dataset with appropriate images imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [ 0 : 2 ]) Use load_from_http loader (<module 'icevision.models.mmdet.models.retinanet' from '/home/farid/repos/airctic/icevision/icevision/models/mmdet/models/retinanet/__init__.py'>, <icevision.models.mmdet.models.retinanet.backbones.resnet_fpn.MMDetRetinanetBackboneConfig at 0x7fa2024c34f0>, <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>, 384)","title":"Loading models already containing metadata"},{"location":"inference/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"install/","text":"Important We currently only support Linux/MacOS installations Note Please do not forget to install the other optional dependencies if you would like to use them: MMCV+MMDetection, and/or YOLOv5 A- Installation using pip Option 1: Installing from pypi repository [Stable Version] To install icevision package together with almost all dependencies: $ pip install icevision [ all ] Option 2: Installing an editable package locally [For Developers] Note This method is used by developers who are usually either: actively contributing to icevision project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icevision latest version. Then, clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icevision.git $ cd icevision $ pip install -e . [ all,dev ] $ pre-commit install Option 3: Installing a non-editable package from GitHub: To install the icevision package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icevision latest version (from the master branch) $ pip install git+https://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade B- Installation using conda Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called icevision $ conda create -n icevision python = 3 .8 anaconda $ conda activate icevision $ pip install icevision [ all ] Optional dependencies MMDetection Installation We need to provide the appropriate version of the mmcv-full package as well as the cuda and the torch versions. Here are some examples for both the CUDA and the CPU versions Torch and CUDA version For the torch version use torch.__version__ and replace the last number with 0. For the cuda version use: torch.version.cuda . Example: TORCH_VERSION = torch1.8.0 ; CUDA_VERSION = cu101 CUDA-Version Installation Example $ pip install mmcv-full == \"1.3.3\" -f https://download.openmmlab.com/mmcv/dist/CUDA_VERSION/TORCH_VERSION/index.html --upgrade $ pip install mmdet CPU-Version Installation $ pip install mmcv-full == \"1.3.3+torch.1.8.0+cpu\" -f https://download.openmmlab.com/mmcv/dist/index.html --upgrade $ pip install mmdet Troubleshooting MMCV is not installing with cuda support If you are installing MMCV from the wheel like described above and still are having problems with CUDA you will probably have to compile it locally. Do that by running: pip install mmcv-full If you encounter the following error it means you will have to install CUDA manually (the one that comes with conda installation will not do). OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root. Try installing it with: sudo apt install nvidia-cuda-toolkit Check the installation by running: nvcc --version Error: Failed building wheel for pycocotools If you encounter the following error, when installation process is building wheel for pycocotools: unable to execute 'gcc': No such file or directory error: command 'gcc' failed with exit status 1 Try installing gcc with: sudo apt install gcc Check the installation by running: gcc --version It should return something similar: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. After that try installing icevision again.","title":"Installation"},{"location":"install/#a-installation-using-pip","text":"","title":"A- Installation using pip"},{"location":"install/#option-1-installing-from-pypi-repository-stable-version","text":"To install icevision package together with almost all dependencies: $ pip install icevision [ all ]","title":"Option 1: Installing from pypi repository [Stable Version]"},{"location":"install/#option-2-installing-an-editable-package-locally-for-developers","text":"Note This method is used by developers who are usually either: actively contributing to icevision project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icevision latest version. Then, clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icevision.git $ cd icevision $ pip install -e . [ all,dev ] $ pre-commit install","title":"Option 2: Installing an editable package locally [For Developers]"},{"location":"install/#option-3-installing-a-non-editable-package-from-github","text":"To install the icevision package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icevision latest version (from the master branch) $ pip install git+https://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade","title":"Option 3: Installing a non-editable package from GitHub:"},{"location":"install/#b-installation-using-conda","text":"Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called icevision $ conda create -n icevision python = 3 .8 anaconda $ conda activate icevision $ pip install icevision [ all ]","title":"B- Installation using conda"},{"location":"install/#optional-dependencies","text":"","title":"Optional dependencies"},{"location":"install/#mmdetection-installation","text":"We need to provide the appropriate version of the mmcv-full package as well as the cuda and the torch versions. Here are some examples for both the CUDA and the CPU versions Torch and CUDA version For the torch version use torch.__version__ and replace the last number with 0. For the cuda version use: torch.version.cuda . Example: TORCH_VERSION = torch1.8.0 ; CUDA_VERSION = cu101","title":"MMDetection Installation"},{"location":"install/#cuda-version-installation-example","text":"$ pip install mmcv-full == \"1.3.3\" -f https://download.openmmlab.com/mmcv/dist/CUDA_VERSION/TORCH_VERSION/index.html --upgrade $ pip install mmdet","title":"CUDA-Version Installation Example"},{"location":"install/#cpu-version-installation","text":"$ pip install mmcv-full == \"1.3.3+torch.1.8.0+cpu\" -f https://download.openmmlab.com/mmcv/dist/index.html --upgrade $ pip install mmdet","title":"CPU-Version Installation"},{"location":"install/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"install/#mmcv-is-not-installing-with-cuda-support","text":"If you are installing MMCV from the wheel like described above and still are having problems with CUDA you will probably have to compile it locally. Do that by running: pip install mmcv-full If you encounter the following error it means you will have to install CUDA manually (the one that comes with conda installation will not do). OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root. Try installing it with: sudo apt install nvidia-cuda-toolkit Check the installation by running: nvcc --version","title":"MMCV is not installing with cuda support"},{"location":"install/#error-failed-building-wheel-for-pycocotools","text":"If you encounter the following error, when installation process is building wheel for pycocotools: unable to execute 'gcc': No such file or directory error: command 'gcc' failed with exit status 1 Try installing gcc with: sudo apt install gcc Check the installation by running: gcc --version It should return something similar: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. After that try installing icevision again.","title":"Error: Failed building wheel for pycocotools"},{"location":"mask_rcnn/","text":"[source] model icevision . models . torchvision . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbones.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source] train_dl icevision . models . torchvision . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . torchvision . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . torchvision . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . torchvision . mask_rcnn . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . torchvision . mask_rcnn . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . torchvision . mask_rcnn . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"mask_rcnn/#model","text":"icevision . models . torchvision . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbones.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source]","title":"model"},{"location":"mask_rcnn/#train_dl","text":"icevision . models . torchvision . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"mask_rcnn/#valid_dl","text":"icevision . models . torchvision . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"mask_rcnn/#infer_dl","text":"icevision . models . torchvision . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"mask_rcnn/#build_train_batch","text":"icevision . models . torchvision . mask_rcnn . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"mask_rcnn/#build_valid_batch","text":"icevision . models . torchvision . mask_rcnn . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"mask_rcnn/#build_infer_batch","text":"icevision . models . torchvision . mask_rcnn . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"mask_rcnn_fastai/","text":"[source] learner icevision . models . torchvision . mask_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"mask_rcnn_fastai/#learner","text":"icevision . models . torchvision . mask_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"mask_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . torchvision . mask_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for mask_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"mask_rcnn_lightning/#modeladapter","text":"icevision . models . torchvision . mask_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for mask_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"mmdet_custom_config/","text":"MMDetection Custom Config When creating an MMDetection model , the model config object is stored in model.cfg and the pretrained weight file path is stored in model.weights_path . In order to update model config attribute, you need to create an cfg_options dictionary: Example: Changing loss weights for loss_bbox and loss_cls You can pass the cfg_options argument when creation a model model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x 'cfg_options' = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 } # Passing cfg_options to the `model()` method to update loss weights model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options ) Install IceVision and IceData The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * Download and prepare a dataset Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) Parse the dataset # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map Creating datasets with agumentations and transforms # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) Creating a model This shows how to customize a retinanet model by using the cfg_options object: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True, cfg_options=cfg_options) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how to easily customize your model. # Just the config options you would like to update selection = 0 extra_args = {} # Example: Changing both loss weights: loss_bbox, loss_cls if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 , } # Example: cahnging anchor boxes ratios: elif selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.anchor_generator.ratios' : [ 1.0 ] } cfg_options {'model.bbox_head.loss_bbox.loss_weight': 2, 'model.bbox_head.loss_cls.loss_weight': 0.8} Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model and pass the `cfg_options` dictionary model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options ) print ( model . bbox_head . loss_bbox . loss_weight ) print ( model . bbox_head . loss_cls . loss_weight ) print ( model . bbox_head . anchor_generator . ratios ) print ( model . cfg . model . bbox_head . loss_cls . loss_weight ) print ( model . cfg . model . bbox_head . loss_bbox . loss_weight ) print ( model . cfg . model . bbox_head . anchor_generator . ratios ) 2 0.8 tensor([0.50000, 1.00000, 2.00000]) 0.8 2 [0.5, 1.0, 2.0] Print config settings # You have access to model's weights_path model . weights_path # You also have access to the whole config object model . cfg . __dict__ # Double checking model new attributes for `loss_cls` model . cfg . model . bbox_head . loss_cls # Double checkingmodel new attributes for `anchor_generator` model . cfg . model . bbox_head . anchor_generator Path('checkpoints/retinanet/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth') {'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 0.8, 'type': 'FocalLoss', 'use_sigmoid': True} {'octave_base_scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} Data Loader The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) Metrics The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning . Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 20 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.229304 0.983039 0.092624 00:06 epoch train_loss valid_loss COCOMetric time 0 0.899269 0.741807 0.156066 00:05 1 0.810302 0.558525 0.365440 00:05 2 0.728987 0.518454 0.478187 00:05 3 0.663082 0.367077 0.609001 00:06 4 0.604693 0.365837 0.717380 00:05 5 0.555109 0.308284 0.837230 00:05 6 0.507140 0.260290 0.871975 00:05 7 0.464969 0.248942 0.881214 00:05 8 0.429737 0.238713 0.864140 00:05 9 0.398711 0.215525 0.916083 00:05 10 0.372950 0.206479 0.911718 00:05 11 0.356441 0.191768 0.909661 00:05 12 0.343991 0.215923 0.911696 00:05 13 0.331874 0.218360 0.887766 00:05 14 0.313435 0.194519 0.908697 00:05 15 0.299692 0.212282 0.900371 00:05 16 0.286332 0.189399 0.916208 00:05 17 0.282384 0.195011 0.911806 00:05 18 0.275653 0.200540 0.904800 00:05 19 0.273256 0.200690 0.904800 00:05 Training using Pytorch Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Using the model - inference and showing results The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Prediction NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) Happy Learning! If you need any assistance, feel free to join our forum .","title":"MMDetection Custom Config"},{"location":"mmdet_custom_config/#mmdetection-custom-config","text":"When creating an MMDetection model , the model config object is stored in model.cfg and the pretrained weight file path is stored in model.weights_path . In order to update model config attribute, you need to create an cfg_options dictionary: Example: Changing loss weights for loss_bbox and loss_cls You can pass the cfg_options argument when creation a model model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x 'cfg_options' = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 } # Passing cfg_options to the `model()` method to update loss weights model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options )","title":"MMDetection Custom Config"},{"location":"mmdet_custom_config/#install-icevision-and-icedata","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision and IceData"},{"location":"mmdet_custom_config/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import *","title":"Imports"},{"location":"mmdet_custom_config/#download-and-prepare-a-dataset","text":"Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir )","title":"Download and prepare a dataset"},{"location":"mmdet_custom_config/#parse-the-dataset","text":"# Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map","title":"Parse the dataset"},{"location":"mmdet_custom_config/#creating-datasets-with-agumentations-and-transforms","text":"# Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Creating datasets with agumentations and transforms"},{"location":"mmdet_custom_config/#creating-a-model","text":"This shows how to customize a retinanet model by using the cfg_options object: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True, cfg_options=cfg_options) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how to easily customize your model. # Just the config options you would like to update selection = 0 extra_args = {} # Example: Changing both loss weights: loss_bbox, loss_cls if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 , } # Example: cahnging anchor boxes ratios: elif selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.anchor_generator.ratios' : [ 1.0 ] } cfg_options {'model.bbox_head.loss_bbox.loss_weight': 2, 'model.bbox_head.loss_cls.loss_weight': 0.8} Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model and pass the `cfg_options` dictionary model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options ) print ( model . bbox_head . loss_bbox . loss_weight ) print ( model . bbox_head . loss_cls . loss_weight ) print ( model . bbox_head . anchor_generator . ratios ) print ( model . cfg . model . bbox_head . loss_cls . loss_weight ) print ( model . cfg . model . bbox_head . loss_bbox . loss_weight ) print ( model . cfg . model . bbox_head . anchor_generator . ratios ) 2 0.8 tensor([0.50000, 1.00000, 2.00000]) 0.8 2 [0.5, 1.0, 2.0]","title":"Creating a model"},{"location":"mmdet_custom_config/#print-config-settings","text":"# You have access to model's weights_path model . weights_path # You also have access to the whole config object model . cfg . __dict__ # Double checking model new attributes for `loss_cls` model . cfg . model . bbox_head . loss_cls # Double checkingmodel new attributes for `anchor_generator` model . cfg . model . bbox_head . anchor_generator Path('checkpoints/retinanet/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth') {'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 0.8, 'type': 'FocalLoss', 'use_sigmoid': True} {'octave_base_scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'}","title":"Print config settings"},{"location":"mmdet_custom_config/#data-loader","text":"The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False )","title":"Data Loader"},{"location":"mmdet_custom_config/#metrics","text":"The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"mmdet_custom_config/#training","text":"IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning .","title":"Training"},{"location":"mmdet_custom_config/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 20 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.229304 0.983039 0.092624 00:06 epoch train_loss valid_loss COCOMetric time 0 0.899269 0.741807 0.156066 00:05 1 0.810302 0.558525 0.365440 00:05 2 0.728987 0.518454 0.478187 00:05 3 0.663082 0.367077 0.609001 00:06 4 0.604693 0.365837 0.717380 00:05 5 0.555109 0.308284 0.837230 00:05 6 0.507140 0.260290 0.871975 00:05 7 0.464969 0.248942 0.881214 00:05 8 0.429737 0.238713 0.864140 00:05 9 0.398711 0.215525 0.916083 00:05 10 0.372950 0.206479 0.911718 00:05 11 0.356441 0.191768 0.909661 00:05 12 0.343991 0.215923 0.911696 00:05 13 0.331874 0.218360 0.887766 00:05 14 0.313435 0.194519 0.908697 00:05 15 0.299692 0.212282 0.900371 00:05 16 0.286332 0.189399 0.916208 00:05 17 0.282384 0.195011 0.911806 00:05 18 0.275653 0.200540 0.904800 00:05 19 0.273256 0.200690 0.904800 00:05","title":"Training using fastai"},{"location":"mmdet_custom_config/#training-using-pytorch-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Pytorch Lightning"},{"location":"mmdet_custom_config/#using-the-model-inference-and-showing-results","text":"The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Using the model - inference and showing results"},{"location":"mmdet_custom_config/#prediction","text":"NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ])","title":"Prediction"},{"location":"mmdet_custom_config/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"models/","text":"Models Source IceVision offers a large number of models by supporting the following Object Detection Libraries: Torchvision MMDetection Ross Wightman's EfficientDet You will enjoy using our unified API while having access to a large repertoire of SOTA models. Switching models is as easy as changing one word. There is no need to be familiar with all the quirks that new models and implementations introduce. Creating a model In order to create a model, we need to: Choose one of the libraries supported by IceVision Choose one of the models supported by the library Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Selecting a model only takes two simple lines of code. Check out the following examples illustrating some of the models libraries we support: MMDetection model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) Torchvision model_type = models.torchvision.retinanet backbone = model_type.backbones.resnet50_fpn # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) EfficientDet model_type = models.ross.efficientdet backbone = model_type.backbones.tf_lite0 # The efficientdet model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size) YOLOv5 model_type = models.ultralytics.yolov5 backbone = model_type.backbones.small # The yolov5 model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size) As pretrained models are used by default, we typically leave this out.","title":"Models"},{"location":"models/#models","text":"Source IceVision offers a large number of models by supporting the following Object Detection Libraries: Torchvision MMDetection Ross Wightman's EfficientDet You will enjoy using our unified API while having access to a large repertoire of SOTA models. Switching models is as easy as changing one word. There is no need to be familiar with all the quirks that new models and implementations introduce.","title":"Models"},{"location":"models/#creating-a-model","text":"In order to create a model, we need to: Choose one of the libraries supported by IceVision Choose one of the models supported by the library Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Selecting a model only takes two simple lines of code. Check out the following examples illustrating some of the models libraries we support:","title":"Creating a model"},{"location":"models/#mmdetection","text":"model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map))","title":"MMDetection"},{"location":"models/#torchvision","text":"model_type = models.torchvision.retinanet backbone = model_type.backbones.resnet50_fpn # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map))","title":"Torchvision"},{"location":"models/#efficientdet","text":"model_type = models.ross.efficientdet backbone = model_type.backbones.tf_lite0 # The efficientdet model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size)","title":"EfficientDet"},{"location":"models/#yolov5","text":"model_type = models.ultralytics.yolov5 backbone = model_type.backbones.small # The yolov5 model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size) As pretrained models are used by default, we typically leave this out.","title":"YOLOv5"},{"location":"negative_samples/","text":"How to use negative examples In some scenarios it might be useful to explicitly show the model images that should be considered background, these are called \"negative examples\" and are images that do not contain any annotations. In this tutorial we're going to be training two raccoons detectors and observe how they perform on images of dogs and cats. One of the models will be trained only with images of raccoons, while the other will also have access to images of dogs and cats (the negative examples). As you might already have imagined, the model that was trained with raccoon images predicts all animals to be raccoons! Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * Raccoon dataset The dataset is stored on github, so a simple git clone will do. ! git clone https : // github . com / datitran / raccoon_dataset fatal: destination path 'raccoon_dataset' already exists and is not an empty directory. The raccoon dataset uses the VOC annotation format, icevision natively supports this format: raccoon_data_dir = Path ( 'raccoon_dataset' ) raccoon_parser = parsers . VOCBBoxParser ( annotations_dir = raccoon_data_dir / 'annotations' , images_dir = raccoon_data_dir / 'images' ) Let's go ahead and parse our data with the default 80% train, 20% valid, split. raccoon_train_records , raccoon_valid_records = raccoon_parser . parse () show_records ( random . choices ( raccoon_train_records , k = 3 ), ncols = 3 , class_map = class_map ) 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s] Pets dataset With icedata we can easily download the pets dataset: pets_data_dir = icedata . pets . load_data () / 'images' Here we have a twist, instead of using the standard parser ( icedata.pets.parser ) which would parse all annotations, we will instead create a custom parser that only parsers the images. Remember the steps for generating a custom parser (check this tutorial for more information). pets_template_record = ObjectDetectionRecord () Parser . generate_template ( pets_template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_img_size(<ImgSize>) record.set_filepath(<Union[str, Path]>) record.detection.add_bboxes(<Sequence[BBox]>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) And now we use that to fill the required methods. We don't have to use the .detection methods since we don't want bboxes for these images. class PetsImageParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . image_filepaths = get_image_files ( data_dir ) def __iter__ ( self ) -> Any : yield from self . image_filepaths def __len__ ( self ) -> int : return len ( self . image_filepaths ) def record_id ( self , o ) -> Hashable : return o . stem def parse_fields ( self , o , record , is_new ): if is_new : record . set_img_size ( get_img_size ( o )) record . set_filepath ( o ) Now we're ready to instantiate the parser and parse the data: pets_parser = PetsImageParser ( pets_template_record , pets_data_dir ) pets_train_records , pets_valid_records = pets_parser . parse () show_records ( random . choices ( pets_train_records , k = 3 ), ncols = 3 ) 0%| | 0/7390 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7390 [00:00<?, ?it/s] Transforms Let's define a simple list of transforms, they are the same for both datasets. presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) Datasets and DataLoaders We create the raccoon dataset and dataloader as normal: model_type = models . ross . efficientdet batch_size = 8 raccoon_train_ds = Dataset ( raccoon_train_records , train_tfms ) raccoon_valid_ds = Dataset ( raccoon_valid_records , valid_tfms ) raccoon_train_dl = model_type . train_dl ( raccoon_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) raccoon_valid_dl = model_type . valid_dl ( raccoon_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) For adding the pets data, we simply have to combine the list of records. Note that the pets dataset contains a lot more images than the raccoon dataset, so we'll get only 100 images for train and 30 for valid, feel free to change these numbers and explore the results! combined_train_ds = Dataset ( raccoon_train_records + pets_train_records [: 100 ], train_tfms ) combined_valid_ds = Dataset ( raccoon_valid_records + pets_valid_records [: 30 ], valid_tfms ) combined_train_dl = model_type . train_dl ( combined_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) combined_valid_dl = model_type . valid_dl ( combined_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) Let's take a look at the combined dataset: show_samples ( random . choices ( combined_train_ds , k = 6 ), class_map = class_map , ncols = 3 ) Metrics As usual, let's stick with our COCOMetric : metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Models We're now ready to train a separate model for each dataset and see how the results change! Raccoons only backbone = model_type . backbones . tf_lite0 raccoon_model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) raccoon_learn = model_type . fastai . learner ( dls = [ raccoon_train_dl , raccoon_valid_dl ], model = raccoon_model , metrics = metrics ) raccoon_learn . lr_find () SuggestedLRs(valley=0.005248074419796467) raccoon_learn . fine_tune ( 30 , 0.005 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.597212 1.354181 0.000307 00:05 1 1.480324 1.354777 0.001507 00:04 2 1.319088 1.254850 0.034810 00:04 3 1.087830 1.081156 0.053211 00:04 4 0.928458 0.898133 0.098449 00:04 /home/ppotrykus/anaconda3/envs/icevision-dev/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes epoch train_loss valid_loss COCOMetric time 0 0.579946 0.861432 0.069980 00:05 1 0.571890 0.824743 0.077679 00:05 2 0.546770 0.768505 0.090791 00:05 3 0.518292 0.676042 0.382004 00:05 4 0.507741 0.644902 0.433709 00:05 5 0.495793 0.692048 0.240042 00:05 6 0.471785 0.577972 0.508806 00:05 7 0.456929 0.624429 0.383962 00:05 8 0.436571 0.644672 0.429864 00:05 9 0.426388 0.581951 0.486592 00:05 10 0.398668 0.562105 0.508463 00:05 11 0.384981 0.508337 0.543620 00:05 12 0.366980 0.497647 0.554876 00:05 13 0.357205 0.494502 0.553882 00:05 14 0.343215 0.491536 0.568322 00:05 15 0.326071 0.490430 0.584215 00:05 16 0.322718 0.501275 0.553775 00:05 17 0.311050 0.486727 0.592019 00:05 18 0.299104 0.476103 0.588887 00:05 19 0.297170 0.500982 0.536913 00:05 20 0.286925 0.482961 0.560512 00:05 21 0.278372 0.486913 0.544892 00:05 22 0.272823 0.497099 0.557630 00:05 23 0.271815 0.485606 0.560342 00:05 24 0.261464 0.475296 0.577226 00:05 25 0.260394 0.479651 0.566950 00:05 26 0.256806 0.477946 0.575284 00:05 27 0.257800 0.480531 0.576717 00:05 28 0.257753 0.484309 0.560253 00:05 29 0.253717 0.482582 0.566893 00:05 If only raccoon photos are showed during training, everything is a raccoon! model_type . show_results ( raccoon_model , combined_valid_ds ) Raccoons + pets combined_model = model_type . model ( backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) combined_learn = model_type . fastai . learner ( dls = [ combined_train_dl , combined_valid_dl ], model = combined_model , metrics = metrics ) combined_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 2.676468 20.114677 0.001932 00:07 1 1.999201 2.396560 0.075933 00:06 2 1.456598 32.068226 0.075432 00:06 3 1.105171 27.473427 0.141161 00:06 4 0.951076 24.071144 0.057629 00:06 epoch train_loss valid_loss COCOMetric time 0 0.610488 9.588224 0.115317 00:08 1 0.580771 6.247436 0.336951 00:07 2 0.559531 4.243937 0.357085 00:08 3 0.520752 1.651748 0.401737 00:08 4 0.519708 2.837212 0.371190 00:08 5 0.521373 6.292931 0.328725 00:09 6 0.476777 2.089493 0.456670 00:08 7 0.499804 6.462561 0.316811 00:08 8 0.981566 21.373594 0.285337 00:09 9 0.986519 12.665498 0.379301 00:08 10 0.717527 8.835299 0.468607 00:08 11 0.584880 7.107671 0.488671 00:08 12 0.497443 6.248025 0.465282 00:08 13 0.495668 4.710539 0.476700 00:08 14 0.484720 1.786158 0.453496 00:08 15 0.419404 2.497736 0.438899 00:08 16 0.379092 2.484356 0.502668 00:08 17 0.362614 1.563345 0.490237 00:09 18 0.337136 1.657581 0.538952 00:09 19 0.316169 1.152076 0.509543 00:08 20 0.304117 1.089841 0.521393 00:07 21 0.299743 1.460610 0.500326 00:08 22 0.278153 1.020550 0.489060 00:08 23 0.265477 0.737581 0.546207 00:08 24 0.252644 0.702136 0.494025 00:09 25 0.248898 0.650576 0.548857 00:08 26 0.273597 0.597172 0.544202 00:08 27 0.263164 0.943378 0.550043 00:08 28 0.243946 0.758281 0.553848 00:08 29 0.245725 0.694360 0.548657 00:09 When negative samples are used during training, the model get's way better understading what is not a raccoon. model_type . show_results ( combined_model , combined_valid_ds ) Happy Learning! That's it folks! If you need any assistance, feel free to join our forum .","title":"How to use negative samples"},{"location":"negative_samples/#how-to-use-negative-examples","text":"In some scenarios it might be useful to explicitly show the model images that should be considered background, these are called \"negative examples\" and are images that do not contain any annotations. In this tutorial we're going to be training two raccoons detectors and observe how they perform on images of dogs and cats. One of the models will be trained only with images of raccoons, while the other will also have access to images of dogs and cats (the negative examples). As you might already have imagined, the model that was trained with raccoon images predicts all animals to be raccoons!","title":"How to use negative examples"},{"location":"negative_samples/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"negative_samples/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"negative_samples/#raccoon-dataset","text":"The dataset is stored on github, so a simple git clone will do. ! git clone https : // github . com / datitran / raccoon_dataset fatal: destination path 'raccoon_dataset' already exists and is not an empty directory. The raccoon dataset uses the VOC annotation format, icevision natively supports this format: raccoon_data_dir = Path ( 'raccoon_dataset' ) raccoon_parser = parsers . VOCBBoxParser ( annotations_dir = raccoon_data_dir / 'annotations' , images_dir = raccoon_data_dir / 'images' ) Let's go ahead and parse our data with the default 80% train, 20% valid, split. raccoon_train_records , raccoon_valid_records = raccoon_parser . parse () show_records ( random . choices ( raccoon_train_records , k = 3 ), ncols = 3 , class_map = class_map ) 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s]","title":"Raccoon dataset"},{"location":"negative_samples/#pets-dataset","text":"With icedata we can easily download the pets dataset: pets_data_dir = icedata . pets . load_data () / 'images' Here we have a twist, instead of using the standard parser ( icedata.pets.parser ) which would parse all annotations, we will instead create a custom parser that only parsers the images. Remember the steps for generating a custom parser (check this tutorial for more information). pets_template_record = ObjectDetectionRecord () Parser . generate_template ( pets_template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_img_size(<ImgSize>) record.set_filepath(<Union[str, Path]>) record.detection.add_bboxes(<Sequence[BBox]>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) And now we use that to fill the required methods. We don't have to use the .detection methods since we don't want bboxes for these images. class PetsImageParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . image_filepaths = get_image_files ( data_dir ) def __iter__ ( self ) -> Any : yield from self . image_filepaths def __len__ ( self ) -> int : return len ( self . image_filepaths ) def record_id ( self , o ) -> Hashable : return o . stem def parse_fields ( self , o , record , is_new ): if is_new : record . set_img_size ( get_img_size ( o )) record . set_filepath ( o ) Now we're ready to instantiate the parser and parse the data: pets_parser = PetsImageParser ( pets_template_record , pets_data_dir ) pets_train_records , pets_valid_records = pets_parser . parse () show_records ( random . choices ( pets_train_records , k = 3 ), ncols = 3 ) 0%| | 0/7390 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7390 [00:00<?, ?it/s]","title":"Pets dataset"},{"location":"negative_samples/#transforms","text":"Let's define a simple list of transforms, they are the same for both datasets. presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"negative_samples/#datasets-and-dataloaders","text":"We create the raccoon dataset and dataloader as normal: model_type = models . ross . efficientdet batch_size = 8 raccoon_train_ds = Dataset ( raccoon_train_records , train_tfms ) raccoon_valid_ds = Dataset ( raccoon_valid_records , valid_tfms ) raccoon_train_dl = model_type . train_dl ( raccoon_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) raccoon_valid_dl = model_type . valid_dl ( raccoon_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) For adding the pets data, we simply have to combine the list of records. Note that the pets dataset contains a lot more images than the raccoon dataset, so we'll get only 100 images for train and 30 for valid, feel free to change these numbers and explore the results! combined_train_ds = Dataset ( raccoon_train_records + pets_train_records [: 100 ], train_tfms ) combined_valid_ds = Dataset ( raccoon_valid_records + pets_valid_records [: 30 ], valid_tfms ) combined_train_dl = model_type . train_dl ( combined_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) combined_valid_dl = model_type . valid_dl ( combined_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) Let's take a look at the combined dataset: show_samples ( random . choices ( combined_train_ds , k = 6 ), class_map = class_map , ncols = 3 )","title":"Datasets and DataLoaders"},{"location":"negative_samples/#metrics","text":"As usual, let's stick with our COCOMetric : metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"negative_samples/#models","text":"We're now ready to train a separate model for each dataset and see how the results change!","title":"Models"},{"location":"negative_samples/#raccoons-only","text":"backbone = model_type . backbones . tf_lite0 raccoon_model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) raccoon_learn = model_type . fastai . learner ( dls = [ raccoon_train_dl , raccoon_valid_dl ], model = raccoon_model , metrics = metrics ) raccoon_learn . lr_find () SuggestedLRs(valley=0.005248074419796467) raccoon_learn . fine_tune ( 30 , 0.005 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.597212 1.354181 0.000307 00:05 1 1.480324 1.354777 0.001507 00:04 2 1.319088 1.254850 0.034810 00:04 3 1.087830 1.081156 0.053211 00:04 4 0.928458 0.898133 0.098449 00:04 /home/ppotrykus/anaconda3/envs/icevision-dev/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes epoch train_loss valid_loss COCOMetric time 0 0.579946 0.861432 0.069980 00:05 1 0.571890 0.824743 0.077679 00:05 2 0.546770 0.768505 0.090791 00:05 3 0.518292 0.676042 0.382004 00:05 4 0.507741 0.644902 0.433709 00:05 5 0.495793 0.692048 0.240042 00:05 6 0.471785 0.577972 0.508806 00:05 7 0.456929 0.624429 0.383962 00:05 8 0.436571 0.644672 0.429864 00:05 9 0.426388 0.581951 0.486592 00:05 10 0.398668 0.562105 0.508463 00:05 11 0.384981 0.508337 0.543620 00:05 12 0.366980 0.497647 0.554876 00:05 13 0.357205 0.494502 0.553882 00:05 14 0.343215 0.491536 0.568322 00:05 15 0.326071 0.490430 0.584215 00:05 16 0.322718 0.501275 0.553775 00:05 17 0.311050 0.486727 0.592019 00:05 18 0.299104 0.476103 0.588887 00:05 19 0.297170 0.500982 0.536913 00:05 20 0.286925 0.482961 0.560512 00:05 21 0.278372 0.486913 0.544892 00:05 22 0.272823 0.497099 0.557630 00:05 23 0.271815 0.485606 0.560342 00:05 24 0.261464 0.475296 0.577226 00:05 25 0.260394 0.479651 0.566950 00:05 26 0.256806 0.477946 0.575284 00:05 27 0.257800 0.480531 0.576717 00:05 28 0.257753 0.484309 0.560253 00:05 29 0.253717 0.482582 0.566893 00:05 If only raccoon photos are showed during training, everything is a raccoon! model_type . show_results ( raccoon_model , combined_valid_ds )","title":"Raccoons only"},{"location":"negative_samples/#raccoons-pets","text":"combined_model = model_type . model ( backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) combined_learn = model_type . fastai . learner ( dls = [ combined_train_dl , combined_valid_dl ], model = combined_model , metrics = metrics ) combined_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 2.676468 20.114677 0.001932 00:07 1 1.999201 2.396560 0.075933 00:06 2 1.456598 32.068226 0.075432 00:06 3 1.105171 27.473427 0.141161 00:06 4 0.951076 24.071144 0.057629 00:06 epoch train_loss valid_loss COCOMetric time 0 0.610488 9.588224 0.115317 00:08 1 0.580771 6.247436 0.336951 00:07 2 0.559531 4.243937 0.357085 00:08 3 0.520752 1.651748 0.401737 00:08 4 0.519708 2.837212 0.371190 00:08 5 0.521373 6.292931 0.328725 00:09 6 0.476777 2.089493 0.456670 00:08 7 0.499804 6.462561 0.316811 00:08 8 0.981566 21.373594 0.285337 00:09 9 0.986519 12.665498 0.379301 00:08 10 0.717527 8.835299 0.468607 00:08 11 0.584880 7.107671 0.488671 00:08 12 0.497443 6.248025 0.465282 00:08 13 0.495668 4.710539 0.476700 00:08 14 0.484720 1.786158 0.453496 00:08 15 0.419404 2.497736 0.438899 00:08 16 0.379092 2.484356 0.502668 00:08 17 0.362614 1.563345 0.490237 00:09 18 0.337136 1.657581 0.538952 00:09 19 0.316169 1.152076 0.509543 00:08 20 0.304117 1.089841 0.521393 00:07 21 0.299743 1.460610 0.500326 00:08 22 0.278153 1.020550 0.489060 00:08 23 0.265477 0.737581 0.546207 00:08 24 0.252644 0.702136 0.494025 00:09 25 0.248898 0.650576 0.548857 00:08 26 0.273597 0.597172 0.544202 00:08 27 0.263164 0.943378 0.550043 00:08 28 0.243946 0.758281 0.553848 00:08 29 0.245725 0.694360 0.548657 00:09 When negative samples are used during training, the model get's way better understading what is not a raccoon. model_type . show_results ( combined_model , combined_valid_ds )","title":"Raccoons + pets"},{"location":"negative_samples/#happy-learning","text":"That's it folks! If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"ochuman_keypoint_detection/","text":"OCHuman dataset From the OCHuman repo: This dataset focus on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study. Disclaimer : it is currently not possible to run this notebook in Colab right away, given you need to download the OCHuman dataset manually. We advise running the notebook locally, as soon as you get access to the dataset. Installing IceVision # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Defining OCHuman parser from icevision.all import * _ = icedata . ochuman . load_data () \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m MANUALLY download AND unzip the dataset from https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman. You will need the path to the `ochuman.json` annotations file and the `images` directory. \u001b[0m | \u001b[36micedata.datasets.ochuman.data\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m7\u001b[0m Parse data Note : you might need to change the ../../ path used from this point onwards, according to your filesystem (e.g. according to where you stored the dataset). parser = icedata . ochuman . parser ( \"../../OCHuman/ochuman.json\" , \"../../OCHuman/images/\" ) train_records , valid_records = parser . parse ( data_splitter = RandomSplitter ([ 0.8 , 0.2 ]), cache_filepath = \"../../OCHuman/ochuman.pkl\" ) len ( train_records ), len ( valid_records ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLoading cached records from ../../OCHuman/ochuman.pkl\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m113\u001b[0m (4064, 1017) Datasets + augmentations presize = 1024 size = 512 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) len ( train_ds ), len ( valid_ds ) (4064, 1017) Dataloaders model_type = models . torchvision . keypoint_rcnn train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model model = model_type . model ( num_keypoints = 19 ) Train a fastai learner from fastai.callback.tracker import SaveModelCallback learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , cbs = [ SaveModelCallback ()]) learn . lr_find () /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] SuggestedLRs(valley=4.365158383734524e-05) learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 4.737989 4.609861 08:19 Better model found at epoch 0 with valid_loss value: 4.609861373901367. epoch train_loss valid_loss time 0 4.346546 4.307302 09:02 1 4.303442 4.230606 08:46 2 4.201407 4.191602 08:37 3 4.194221 4.123021 08:27 4 4.168465 4.063463 08:32 5 4.112132 4.037125 08:24 6 4.047480 3.952349 08:20 7 3.980796 3.875872 08:29 8 3.898531 3.818884 08:25 9 3.856582 3.771754 08:27 10 3.770988 3.699221 08:24 11 3.736982 3.637545 08:18 12 3.645181 3.561272 08:12 13 3.570732 3.501793 08:20 14 3.529509 3.464969 08:20 15 3.480687 3.416519 08:20 16 3.416651 3.388196 08:26 17 3.375072 3.358102 08:21 18 3.355783 3.351155 08:21 19 3.344901 3.357507 08:17 learn . recorder . plot_loss () Better model found at epoch 0 with valid_loss value: 4.3073015213012695. Better model found at epoch 1 with valid_loss value: 4.230605602264404. Better model found at epoch 2 with valid_loss value: 4.1916022300720215. Better model found at epoch 3 with valid_loss value: 4.123020648956299. Better model found at epoch 4 with valid_loss value: 4.063462734222412. Better model found at epoch 5 with valid_loss value: 4.037125110626221. Better model found at epoch 6 with valid_loss value: 3.9523494243621826. Better model found at epoch 7 with valid_loss value: 3.8758718967437744. Better model found at epoch 8 with valid_loss value: 3.8188838958740234. Better model found at epoch 9 with valid_loss value: 3.771754026412964. Better model found at epoch 10 with valid_loss value: 3.699220657348633. Better model found at epoch 11 with valid_loss value: 3.637545347213745. Better model found at epoch 12 with valid_loss value: 3.561272144317627. Better model found at epoch 13 with valid_loss value: 3.5017926692962646. Better model found at epoch 14 with valid_loss value: 3.464968681335449. Better model found at epoch 15 with valid_loss value: 3.4165189266204834. Better model found at epoch 16 with valid_loss value: 3.388195753097534. Better model found at epoch 17 with valid_loss value: 3.3581018447875977. Better model found at epoch 18 with valid_loss value: 3.3511552810668945. Show model results model_type . show_results ( model , valid_ds ) Save model torch . save ( model . state_dict (), \"../../OCHuman/model.pth\" ) model = model_type . model ( num_keypoints = 19 ) state_dict = torch . load ( \"../../OCHuman/model.pth\" ) model . load_state_dict ( state_dict ) <All keys matched successfully> Running inference on validation set infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) show_preds ( preds = preds [ 68 : 70 ], show = True , display_label = False , figsize = ( 10 , 10 )) 0%| | 0/128 [00:00<?, ?it/s] plot_top_losses #model.train() sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_total\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/1017 [00:00<?, ?it/s] 0%| | 0/128 [00:00<?, ?it/s]","title":"Advanced Keypoint Detection"},{"location":"ochuman_keypoint_detection/#ochuman-dataset","text":"From the OCHuman repo: This dataset focus on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study. Disclaimer : it is currently not possible to run this notebook in Colab right away, given you need to download the OCHuman dataset manually. We advise running the notebook locally, as soon as you get access to the dataset.","title":"OCHuman dataset"},{"location":"ochuman_keypoint_detection/#installing-icevision","text":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision"},{"location":"ochuman_keypoint_detection/#defining-ochuman-parser","text":"from icevision.all import * _ = icedata . ochuman . load_data () \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m MANUALLY download AND unzip the dataset from https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman. You will need the path to the `ochuman.json` annotations file and the `images` directory. \u001b[0m | \u001b[36micedata.datasets.ochuman.data\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m7\u001b[0m","title":"Defining OCHuman parser"},{"location":"ochuman_keypoint_detection/#parse-data","text":"Note : you might need to change the ../../ path used from this point onwards, according to your filesystem (e.g. according to where you stored the dataset). parser = icedata . ochuman . parser ( \"../../OCHuman/ochuman.json\" , \"../../OCHuman/images/\" ) train_records , valid_records = parser . parse ( data_splitter = RandomSplitter ([ 0.8 , 0.2 ]), cache_filepath = \"../../OCHuman/ochuman.pkl\" ) len ( train_records ), len ( valid_records ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLoading cached records from ../../OCHuman/ochuman.pkl\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m113\u001b[0m (4064, 1017)","title":"Parse data"},{"location":"ochuman_keypoint_detection/#datasets-augmentations","text":"presize = 1024 size = 512 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) len ( train_ds ), len ( valid_ds ) (4064, 1017)","title":"Datasets + augmentations"},{"location":"ochuman_keypoint_detection/#dataloaders","text":"model_type = models . torchvision . keypoint_rcnn train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"Dataloaders"},{"location":"ochuman_keypoint_detection/#model","text":"model = model_type . model ( num_keypoints = 19 )","title":"Model"},{"location":"ochuman_keypoint_detection/#train-a-fastai-learner","text":"from fastai.callback.tracker import SaveModelCallback learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , cbs = [ SaveModelCallback ()]) learn . lr_find () /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] SuggestedLRs(valley=4.365158383734524e-05) learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 4.737989 4.609861 08:19 Better model found at epoch 0 with valid_loss value: 4.609861373901367. epoch train_loss valid_loss time 0 4.346546 4.307302 09:02 1 4.303442 4.230606 08:46 2 4.201407 4.191602 08:37 3 4.194221 4.123021 08:27 4 4.168465 4.063463 08:32 5 4.112132 4.037125 08:24 6 4.047480 3.952349 08:20 7 3.980796 3.875872 08:29 8 3.898531 3.818884 08:25 9 3.856582 3.771754 08:27 10 3.770988 3.699221 08:24 11 3.736982 3.637545 08:18 12 3.645181 3.561272 08:12 13 3.570732 3.501793 08:20 14 3.529509 3.464969 08:20 15 3.480687 3.416519 08:20 16 3.416651 3.388196 08:26 17 3.375072 3.358102 08:21 18 3.355783 3.351155 08:21 19 3.344901 3.357507 08:17 learn . recorder . plot_loss () Better model found at epoch 0 with valid_loss value: 4.3073015213012695. Better model found at epoch 1 with valid_loss value: 4.230605602264404. Better model found at epoch 2 with valid_loss value: 4.1916022300720215. Better model found at epoch 3 with valid_loss value: 4.123020648956299. Better model found at epoch 4 with valid_loss value: 4.063462734222412. Better model found at epoch 5 with valid_loss value: 4.037125110626221. Better model found at epoch 6 with valid_loss value: 3.9523494243621826. Better model found at epoch 7 with valid_loss value: 3.8758718967437744. Better model found at epoch 8 with valid_loss value: 3.8188838958740234. Better model found at epoch 9 with valid_loss value: 3.771754026412964. Better model found at epoch 10 with valid_loss value: 3.699220657348633. Better model found at epoch 11 with valid_loss value: 3.637545347213745. Better model found at epoch 12 with valid_loss value: 3.561272144317627. Better model found at epoch 13 with valid_loss value: 3.5017926692962646. Better model found at epoch 14 with valid_loss value: 3.464968681335449. Better model found at epoch 15 with valid_loss value: 3.4165189266204834. Better model found at epoch 16 with valid_loss value: 3.388195753097534. Better model found at epoch 17 with valid_loss value: 3.3581018447875977. Better model found at epoch 18 with valid_loss value: 3.3511552810668945.","title":"Train a fastai learner"},{"location":"ochuman_keypoint_detection/#show-model-results","text":"model_type . show_results ( model , valid_ds )","title":"Show model results"},{"location":"ochuman_keypoint_detection/#save-model","text":"torch . save ( model . state_dict (), \"../../OCHuman/model.pth\" ) model = model_type . model ( num_keypoints = 19 ) state_dict = torch . load ( \"../../OCHuman/model.pth\" ) model . load_state_dict ( state_dict ) <All keys matched successfully>","title":"Save model"},{"location":"ochuman_keypoint_detection/#running-inference-on-validation-set","text":"infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) show_preds ( preds = preds [ 68 : 70 ], show = True , display_label = False , figsize = ( 10 , 10 )) 0%| | 0/128 [00:00<?, ?it/s]","title":"Running inference on validation set"},{"location":"ochuman_keypoint_detection/#plot_top_losses","text":"#model.train() sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_total\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/1017 [00:00<?, ?it/s] 0%| | 0/128 [00:00<?, ?it/s]","title":"plot_top_losses"},{"location":"parser/","text":"[source] Parser icevision . parsers . Parser ( template_record , class_map = None , idmap = None ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required fields for all parsers are image_id and image_width_height . Arguments idmap Optional[icevision.core.id_map.IDMap] : Maps from filenames to unique ids, pass an IDMap() if you need this information. Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source] parse Parser . parse ( data_splitter = None , autofix = True , show_pbar = True , cache_filepath = None ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. show_pbar bool : Whether or not to show a progress bar while parsing the data. cache_filepath Union[str, pathlib.Path] : Path to save records in pickle format. Defaults to None, e.g. if the user does not specify a path, no saving nor loading happens. Returns A list of records for each split defined by data_splitter .","title":"Parser"},{"location":"parser/#parser","text":"icevision . parsers . Parser ( template_record , class_map = None , idmap = None ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required fields for all parsers are image_id and image_width_height . Arguments idmap Optional[icevision.core.id_map.IDMap] : Maps from filenames to unique ids, pass an IDMap() if you need this information. Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source]","title":"Parser"},{"location":"parser/#parse","text":"Parser . parse ( data_splitter = None , autofix = True , show_pbar = True , cache_filepath = None ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. show_pbar bool : Whether or not to show a progress bar while parsing the data. cache_filepath Union[str, pathlib.Path] : Path to save records in pickle format. Defaults to None, e.g. if the user does not specify a path, no saving nor loading happens. Returns A list of records for each split defined by data_splitter .","title":"parse"},{"location":"plot_top_losses/","text":"The purpose of this notebook is to showcase the newly added plot_top_losses functionality, which allows users to inspect models' results by plotting images sorted by various combinations of losses. This API makes it easy to immediately spot pictures the model struggles the most with, giving the practitioner the opportunity to take swift action to correct this behaviour (remove wrong samples, correct mis-labellings, etc). plot_top_losses is available for all IceVision models, as the below notebook shows. Install IceVision # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Object Detection Load fridge dataset from icevision.all import * # Loading Data url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 1 , num_workers = 4 , shuffle = False ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[33m\u001b[1m\u001b[1mWARNING \u001b[0m\u001b[33m\u001b[1m\u001b[0m - \u001b[33m\u001b[1mThis function will be deprecated, instantiate the concrete classes instead: `VOCBBoxParser`, `VOCMaskParser`\u001b[0m | \u001b[36micevision.parsers.voc_parser\u001b[0m:\u001b[36mvoc\u001b[0m:\u001b[36m17\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] Train faster_rcnn model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map )) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.995082 1.145674 0.000057 00:09 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] epoch train_loss valid_loss COCOMetric time 0 1.193951 1.648449 0.032289 00:13 1 0.985826 0.735072 0.000160 00:10 2 0.924974 0.794665 0.018933 00:10 3 0.893960 0.844737 0.052620 00:10 4 0.893976 0.812259 0.183922 00:11 5 0.880687 0.681532 0.173614 00:09 6 0.858650 0.665555 0.247655 00:10 7 0.818621 0.510320 0.337507 00:10 8 0.776634 0.518227 0.360398 00:10 9 0.736172 0.486833 0.372902 00:09 Run top_plot_losses on faster_rcnn model results Values allowed to pass to sort_by are (for faster_rcnn ): \"loss_classifier\" \"loss_box_reg\" \"loss_objectness\" \"loss_rpn_box_reg\" \"loss_total\" (sum of the previous 4 losses) {\"method\": \"weighted\", \"weights\": {\"loss_box_reg\": 0.25, \"loss_classifier\": 0.25, \"loss_objectness\": 0.25, \"loss_rpn_box_reg\": 0.25,}} (calculates weighted sum of the 4 losses - Note : I have set weights to 0.25 for example purposes) Below we show several ways of invoking the same API on the trained model, sorting samples by different losses combinations. samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_total\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_classifier\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # in this case `loss_weighted` will be equal to `loss_box_reg` by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 1 , \"loss_classifier\" : 0 , \"loss_objectness\" : 0 , \"loss_rpn_box_reg\" : 0 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 0.25 , \"loss_classifier\" : 0.25 , \"loss_objectness\" : 0.25 , \"loss_rpn_box_reg\" : 0.25 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # `losses_stats` contains useful statistics for each computed loss in the dataset losses_stats # we can easily extract losses per image and display them in a pandas DataFrame for further analysis import pandas as pd from icevision.models.interpretation import get_samples_losses loss_per_image = get_samples_losses ( samples_plus_losses ) pd . DataFrame ( loss_per_image ) {'loss_classifier': {'min': 0.06532751768827438, 'max': 0.3570514917373657, 'mean': 0.22153257807860008, '1ile': 0.06532751768827438, '25ile': 0.17143525183200836, '50ile': 0.23105227947235107, '75ile': 0.2811724841594696, '99ile': 0.3570514917373657}, 'loss_box_reg': {'min': 0.04955608770251274, 'max': 0.41760146617889404, 'mean': 0.24227395410147998, '1ile': 0.04955608770251274, '25ile': 0.16739314794540405, '50ile': 0.23787736147642136, '75ile': 0.3278003931045532, '99ile': 0.41760146617889404}, 'loss_objectness': {'min': 0.0017869938164949417, 'max': 0.10180316120386124, 'mean': 0.016076406804271616, '1ile': 0.0017869938164949417, '25ile': 0.007386505138128996, '50ile': 0.010415146127343178, '75ile': 0.018654515966773033, '99ile': 0.10180316120386124}, 'loss_rpn_box_reg': {'min': 0.001359554473310709, 'max': 0.02417410910129547, 'mean': 0.008536153078938905, '1ile': 0.001359554473310709, '25ile': 0.0056922342628240585, '50ile': 0.007792716380208731, '75ile': 0.010672150179743767, '99ile': 0.02417410910129547}, 'loss_total': {'min': 0.11875081108883023, 'max': 0.7295006141066551, 'mean': 0.4884190920632906, '1ile': 0.11875081108883023, '25ile': 0.3626828184351325, '50ile': 0.5204635132104158, '75ile': 0.6233997480012476, '99ile': 0.7295006141066551}} .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg loss_total loss_weighted filepath 0 0.296700 0.417601 0.007554 0.007645 0.729501 0.182375 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/86.jpg 1 0.345282 0.344900 0.021045 0.006102 0.717329 0.179332 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/90.jpg 2 0.357051 0.327800 0.020415 0.008122 0.713388 0.178347 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/80.jpg 3 0.301462 0.354785 0.015013 0.015807 0.687068 0.171767 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/65.jpg 4 0.232049 0.370708 0.019768 0.010672 0.633197 0.158299 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/41.jpg 5 0.296007 0.310065 0.014165 0.008247 0.628484 0.157121 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/81.jpg 6 0.211539 0.393251 0.011715 0.006894 0.623400 0.155850 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/87.jpg 7 0.271977 0.331191 0.012193 0.007269 0.622630 0.155657 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/48.jpg 8 0.281172 0.282685 0.026252 0.014193 0.604303 0.151076 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/37.jpg 9 0.260208 0.324138 0.008174 0.011745 0.604265 0.151066 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/74.jpg 10 0.278057 0.261131 0.018655 0.007381 0.565223 0.141306 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/28.jpg 11 0.235263 0.310668 0.002415 0.007940 0.556287 0.139072 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/46.jpg 12 0.299006 0.214321 0.009115 0.011385 0.533828 0.133457 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/36.jpg 13 0.230056 0.258252 0.008532 0.010260 0.507099 0.126775 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/40.jpg 14 0.220261 0.210022 0.007387 0.005692 0.443363 0.110841 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/115.jpg 15 0.171435 0.145327 0.101803 0.024174 0.442740 0.110685 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/109.jpg 16 0.233022 0.204496 0.003368 0.001360 0.442246 0.110562 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/47.jpg 17 0.202497 0.217503 0.008341 0.010180 0.438521 0.109630 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/31.jpg 18 0.206598 0.164179 0.051936 0.015769 0.438482 0.109621 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/25.jpg 19 0.145833 0.187863 0.018361 0.010626 0.362683 0.090671 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/59.jpg 20 0.118557 0.171712 0.002909 0.001784 0.294962 0.073740 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/127.jpg 21 0.103582 0.167393 0.014592 0.006817 0.292384 0.073096 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/12.jpg 22 0.180912 0.102142 0.002801 0.002640 0.288496 0.072124 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/19.jpg 23 0.118964 0.102546 0.007565 0.004681 0.233756 0.058439 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/110.jpg 24 0.097024 0.074887 0.002125 0.002476 0.176512 0.044128 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/100.jpg 25 0.065328 0.049556 0.001787 0.002080 0.118751 0.029688 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/106.jpg Run top_plot_losses on a efficientdet pretrained (but not finetuned) model extra_args = {} model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = 384 model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"class_loss\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['effdet_total_loss', 'class_loss', 'box_loss']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Instance Segmentation plot_top_losses in action with a mask_rcnn model on the pennfudan dataset # Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x () # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) model = model_type . model ( backbone = backbone , num_classes = icedata . pennfudan . NUM_CLASSES ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 3e-4 , freeze_epochs = 2 ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 1.950177 0.444139 00:32 1 0.936212 0.439992 00:28 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` warnings.warn( epoch train_loss valid_loss time 0 0.374806 0.344675 00:27 1 0.347379 0.350029 00:27 2 0.336716 0.364372 00:25 3 0.329699 0.333309 00:28 4 0.315138 0.344930 00:27 5 0.308043 0.340804 00:27 6 0.289543 0.312908 00:27 7 0.278561 0.327445 00:26 8 0.266237 0.310647 00:25 9 0.262186 0.310190 00:26 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_mask\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_rpn_cls', 'loss_rpn_bbox', 'loss_cls', 'loss_bbox', 'loss_mask']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/34 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s] Keypoint Detection plot_top_losses in action with a keypoint_rcnn model on the biwi dataset model_type = models . torchvision . keypoint_rcnn data_dir = icedata . biwi . load_data () parser = icedata . biwi . parser ( data_dir ) train_records , valid_records = parser . parse () presize = 240 size = 120 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = model_type . train_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = False ) backbone = model_type . backbones . resnet18_fpn ( pretrained = True ) model = model_type . model ( backbone = backbone , num_keypoints = 1 ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 5 , 1e-4 , freeze_epochs = 2 ) 0%| | 0/593774 [00:00<?, ?B/s] 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 9.133101 7.838448 00:57 1 8.182481 6.926270 00:28 epoch train_loss valid_loss time 0 6.294706 5.840428 00:46 1 5.892668 5.038722 00:25 2 5.503245 4.533072 00:27 3 5.199810 4.395931 00:28 4 4.986382 4.209587 00:22 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_keypoint\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/40 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s]","title":"Model Debugging with Plot Top Losses"},{"location":"plot_top_losses/#install-icevision","text":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision"},{"location":"plot_top_losses/#object-detection","text":"","title":"Object Detection"},{"location":"plot_top_losses/#load-fridge-dataset","text":"from icevision.all import * # Loading Data url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 1 , num_workers = 4 , shuffle = False ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[33m\u001b[1m\u001b[1mWARNING \u001b[0m\u001b[33m\u001b[1m\u001b[0m - \u001b[33m\u001b[1mThis function will be deprecated, instantiate the concrete classes instead: `VOCBBoxParser`, `VOCMaskParser`\u001b[0m | \u001b[36micevision.parsers.voc_parser\u001b[0m:\u001b[36mvoc\u001b[0m:\u001b[36m17\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s]","title":"Load fridge dataset"},{"location":"plot_top_losses/#train-faster_rcnn-model","text":"model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map )) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.995082 1.145674 0.000057 00:09 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] epoch train_loss valid_loss COCOMetric time 0 1.193951 1.648449 0.032289 00:13 1 0.985826 0.735072 0.000160 00:10 2 0.924974 0.794665 0.018933 00:10 3 0.893960 0.844737 0.052620 00:10 4 0.893976 0.812259 0.183922 00:11 5 0.880687 0.681532 0.173614 00:09 6 0.858650 0.665555 0.247655 00:10 7 0.818621 0.510320 0.337507 00:10 8 0.776634 0.518227 0.360398 00:10 9 0.736172 0.486833 0.372902 00:09","title":"Train faster_rcnn model"},{"location":"plot_top_losses/#run-top_plot_losses-on-faster_rcnn-model-results","text":"Values allowed to pass to sort_by are (for faster_rcnn ): \"loss_classifier\" \"loss_box_reg\" \"loss_objectness\" \"loss_rpn_box_reg\" \"loss_total\" (sum of the previous 4 losses) {\"method\": \"weighted\", \"weights\": {\"loss_box_reg\": 0.25, \"loss_classifier\": 0.25, \"loss_objectness\": 0.25, \"loss_rpn_box_reg\": 0.25,}} (calculates weighted sum of the 4 losses - Note : I have set weights to 0.25 for example purposes) Below we show several ways of invoking the same API on the trained model, sorting samples by different losses combinations. samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_total\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_classifier\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # in this case `loss_weighted` will be equal to `loss_box_reg` by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 1 , \"loss_classifier\" : 0 , \"loss_objectness\" : 0 , \"loss_rpn_box_reg\" : 0 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 0.25 , \"loss_classifier\" : 0.25 , \"loss_objectness\" : 0.25 , \"loss_rpn_box_reg\" : 0.25 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # `losses_stats` contains useful statistics for each computed loss in the dataset losses_stats # we can easily extract losses per image and display them in a pandas DataFrame for further analysis import pandas as pd from icevision.models.interpretation import get_samples_losses loss_per_image = get_samples_losses ( samples_plus_losses ) pd . DataFrame ( loss_per_image ) {'loss_classifier': {'min': 0.06532751768827438, 'max': 0.3570514917373657, 'mean': 0.22153257807860008, '1ile': 0.06532751768827438, '25ile': 0.17143525183200836, '50ile': 0.23105227947235107, '75ile': 0.2811724841594696, '99ile': 0.3570514917373657}, 'loss_box_reg': {'min': 0.04955608770251274, 'max': 0.41760146617889404, 'mean': 0.24227395410147998, '1ile': 0.04955608770251274, '25ile': 0.16739314794540405, '50ile': 0.23787736147642136, '75ile': 0.3278003931045532, '99ile': 0.41760146617889404}, 'loss_objectness': {'min': 0.0017869938164949417, 'max': 0.10180316120386124, 'mean': 0.016076406804271616, '1ile': 0.0017869938164949417, '25ile': 0.007386505138128996, '50ile': 0.010415146127343178, '75ile': 0.018654515966773033, '99ile': 0.10180316120386124}, 'loss_rpn_box_reg': {'min': 0.001359554473310709, 'max': 0.02417410910129547, 'mean': 0.008536153078938905, '1ile': 0.001359554473310709, '25ile': 0.0056922342628240585, '50ile': 0.007792716380208731, '75ile': 0.010672150179743767, '99ile': 0.02417410910129547}, 'loss_total': {'min': 0.11875081108883023, 'max': 0.7295006141066551, 'mean': 0.4884190920632906, '1ile': 0.11875081108883023, '25ile': 0.3626828184351325, '50ile': 0.5204635132104158, '75ile': 0.6233997480012476, '99ile': 0.7295006141066551}} .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg loss_total loss_weighted filepath 0 0.296700 0.417601 0.007554 0.007645 0.729501 0.182375 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/86.jpg 1 0.345282 0.344900 0.021045 0.006102 0.717329 0.179332 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/90.jpg 2 0.357051 0.327800 0.020415 0.008122 0.713388 0.178347 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/80.jpg 3 0.301462 0.354785 0.015013 0.015807 0.687068 0.171767 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/65.jpg 4 0.232049 0.370708 0.019768 0.010672 0.633197 0.158299 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/41.jpg 5 0.296007 0.310065 0.014165 0.008247 0.628484 0.157121 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/81.jpg 6 0.211539 0.393251 0.011715 0.006894 0.623400 0.155850 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/87.jpg 7 0.271977 0.331191 0.012193 0.007269 0.622630 0.155657 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/48.jpg 8 0.281172 0.282685 0.026252 0.014193 0.604303 0.151076 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/37.jpg 9 0.260208 0.324138 0.008174 0.011745 0.604265 0.151066 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/74.jpg 10 0.278057 0.261131 0.018655 0.007381 0.565223 0.141306 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/28.jpg 11 0.235263 0.310668 0.002415 0.007940 0.556287 0.139072 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/46.jpg 12 0.299006 0.214321 0.009115 0.011385 0.533828 0.133457 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/36.jpg 13 0.230056 0.258252 0.008532 0.010260 0.507099 0.126775 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/40.jpg 14 0.220261 0.210022 0.007387 0.005692 0.443363 0.110841 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/115.jpg 15 0.171435 0.145327 0.101803 0.024174 0.442740 0.110685 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/109.jpg 16 0.233022 0.204496 0.003368 0.001360 0.442246 0.110562 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/47.jpg 17 0.202497 0.217503 0.008341 0.010180 0.438521 0.109630 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/31.jpg 18 0.206598 0.164179 0.051936 0.015769 0.438482 0.109621 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/25.jpg 19 0.145833 0.187863 0.018361 0.010626 0.362683 0.090671 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/59.jpg 20 0.118557 0.171712 0.002909 0.001784 0.294962 0.073740 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/127.jpg 21 0.103582 0.167393 0.014592 0.006817 0.292384 0.073096 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/12.jpg 22 0.180912 0.102142 0.002801 0.002640 0.288496 0.072124 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/19.jpg 23 0.118964 0.102546 0.007565 0.004681 0.233756 0.058439 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/110.jpg 24 0.097024 0.074887 0.002125 0.002476 0.176512 0.044128 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/100.jpg 25 0.065328 0.049556 0.001787 0.002080 0.118751 0.029688 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/106.jpg","title":"Run top_plot_losses on faster_rcnn model results"},{"location":"plot_top_losses/#run-top_plot_losses-on-a-efficientdet-pretrained-but-not-finetuned-model","text":"extra_args = {} model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = 384 model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"class_loss\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['effdet_total_loss', 'class_loss', 'box_loss']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes","title":"Run top_plot_losses on a efficientdet pretrained (but not finetuned) model"},{"location":"plot_top_losses/#instance-segmentation","text":"","title":"Instance Segmentation"},{"location":"plot_top_losses/#plot_top_losses-in-action-with-a-mask_rcnn-model-on-the-pennfudan-dataset","text":"# Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x () # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) model = model_type . model ( backbone = backbone , num_classes = icedata . pennfudan . NUM_CLASSES ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 3e-4 , freeze_epochs = 2 ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 1.950177 0.444139 00:32 1 0.936212 0.439992 00:28 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` warnings.warn( epoch train_loss valid_loss time 0 0.374806 0.344675 00:27 1 0.347379 0.350029 00:27 2 0.336716 0.364372 00:25 3 0.329699 0.333309 00:28 4 0.315138 0.344930 00:27 5 0.308043 0.340804 00:27 6 0.289543 0.312908 00:27 7 0.278561 0.327445 00:26 8 0.266237 0.310647 00:25 9 0.262186 0.310190 00:26 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_mask\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_rpn_cls', 'loss_rpn_bbox', 'loss_cls', 'loss_bbox', 'loss_mask']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/34 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s]","title":"plot_top_losses in action with a mask_rcnn model on the pennfudan dataset"},{"location":"plot_top_losses/#keypoint-detection","text":"","title":"Keypoint Detection"},{"location":"plot_top_losses/#plot_top_losses-in-action-with-a-keypoint_rcnn-model-on-the-biwi-dataset","text":"model_type = models . torchvision . keypoint_rcnn data_dir = icedata . biwi . load_data () parser = icedata . biwi . parser ( data_dir ) train_records , valid_records = parser . parse () presize = 240 size = 120 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = model_type . train_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = False ) backbone = model_type . backbones . resnet18_fpn ( pretrained = True ) model = model_type . model ( backbone = backbone , num_keypoints = 1 ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 5 , 1e-4 , freeze_epochs = 2 ) 0%| | 0/593774 [00:00<?, ?B/s] 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 9.133101 7.838448 00:57 1 8.182481 6.926270 00:28 epoch train_loss valid_loss time 0 6.294706 5.840428 00:46 1 5.892668 5.038722 00:25 2 5.503245 4.533072 00:27 3 5.199810 4.395931 00:28 4 4.986382 4.209587 00:22 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_keypoint\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/40 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s]","title":"plot_top_losses in action with a keypoint_rcnn model on the biwi dataset"},{"location":"progressive_resizing/","text":"Training using progressive resizing Quote from Fastbook: jargon: progressive resizing: Gradually using larger and larger images as you train. Progressive resizing is a very effective technique to train model from scratch or using transfer learning. IceVision now offers a good support for that technique. For more information about the progressive resizing technique, please check out the reference, here below: Fastai Fastbook Chapter: https://github.com/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb Check out the section: Progressive Resizing Paper highlighting the importance of progressive resizing: Rethinking Training from Scratch for Object Detection EfficientNetV2: Smaller Models and Faster Training Introduction This tutorial walk you through the different steps of training the fridge dataset using the progressive resizing technique. The main differences with IceVision standard training are the use of: The get_dataloaders() method # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) replacing dataloaders (corresponding to different image sizes) in either a Fastai Learner object or a Pytorch-Lightning Trainer object as follow: For Fastai: # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls # Standard training learn . lr_find () learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) Pytorch-Lightning: # Replace current dataloaders by the new ones (corresponding to the new size) trainer . train_dataloader = dls [ 0 ] trainer . valid_dataloader = dls [ 1 ] # Standard training trainer . fit ( light_model , dls [ 0 ], dls [ 1 ]) Installing IceVision and IceData # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * from icevision.models.utils import get_dataloaders from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /Users/fra/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map Creating a model # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) Train and Validation Dataset Transforms Initial size: First size (size = 384) # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) # dls[0].dataset[0] ds [ 0 ][ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=384, height=384) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 384x384x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:131.17572064203858, ymin:239.0191364865004, xmax:325.2123962126736, ymax:312.68812907493447)>, <BBox (xmin:82.22909604282495, ymin:82.72545127450111, xmax:181.53168623735334, ymax:288.81880204425784)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1] DataLoader model_type . show_batch ( first ( dls [ 0 ]), ncols = 4 ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = dls , model = model , metrics = metrics , cbs = SaveModelCallback ( monitor = 'COCOMetric' )) learn . lr_find () SuggestedLRs(lr_min=0.00010000000474974513, lr_steep=0.00015848931798245758) First Pass: First Training with the size = 384 learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.303019 1.198495 0.018335 00:07 Better model found at epoch 0 with COCOMetric value: 0.018334512022630832. epoch train_loss valid_loss COCOMetric time 0 1.146912 1.077046 0.076843 00:06 1 1.078911 0.835020 0.172249 00:06 2 0.936218 0.585196 0.317453 00:06 3 0.803767 0.500256 0.409220 00:06 4 0.701060 0.394725 0.598500 00:06 5 0.623153 0.357143 0.649460 00:06 6 0.564359 0.360178 0.691023 00:06 7 0.518424 0.343954 0.707619 00:06 8 0.478071 0.333479 0.735202 00:06 9 0.447511 0.331000 0.739396 00:06 Better model found at epoch 0 with COCOMetric value: 0.07684290821192277. Better model found at epoch 1 with COCOMetric value: 0.17224854342050785. Better model found at epoch 2 with COCOMetric value: 0.31745342072333244. Better model found at epoch 3 with COCOMetric value: 0.4092202239902201. Better model found at epoch 4 with COCOMetric value: 0.5985001096045487. Better model found at epoch 5 with COCOMetric value: 0.6494598763585641. Better model found at epoch 6 with COCOMetric value: 0.6910227038257398. Better model found at epoch 7 with COCOMetric value: 0.7076194302452169. Better model found at epoch 8 with COCOMetric value: 0.735202499994352. Better model found at epoch 9 with COCOMetric value: 0.7393958039433105. Restart resizing from here Subsequent Pass: Subsequent Training with the size = 512 # Second Pass (size = 512) # presize = 640 # size = 512 # Third Pass (size = 640) presize = 768 size = 640 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) dls [ 0 ] . dataset [ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=640, height=640) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 640x640x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:98.14351388888991, ymin:376.93381905417573, xmax:576.7811863685447, ymax:620.2620189083259)>, <BBox (xmin:5.201505318565632, ymin:12.437821266274758, xmax:329.20034254651534, ymax:498.3316159707134)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1] Convert Pytorch DataLoaders to Fastai DataLoaders from icevision.engines.fastai import * fastai_dls = convert_dataloaders_to_fastai ( dls = dls ) # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls print ( fastai_dls [ 0 ]) print ( learn . dls [ 0 ]) learn . lr_find () <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> SuggestedLRs(lr_min=2.2908675418875645e-07, lr_steep=9.12010818865383e-07) learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) # learn.fit_one_cycle(10, 2e-4) epoch train_loss valid_loss COCOMetric time 0 0.200535 0.183777 0.857028 00:13 Better model found at epoch 0 with COCOMetric value: 0.8570280948418122. epoch train_loss valid_loss COCOMetric time 0 0.192144 0.188399 0.846013 00:10 1 0.184424 0.167971 0.865162 00:10 2 0.192361 0.194635 0.853915 00:10 3 0.191424 0.195987 0.837771 00:10 4 0.192338 0.186021 0.828367 00:10 5 0.189835 0.152657 0.869115 00:10 6 0.184088 0.163762 0.859548 00:10 7 0.179976 0.155070 0.868776 00:10 8 0.175261 0.156499 0.867725 00:10 9 0.171284 0.156758 0.863302 00:10 model_type . show_results ( model , ds [ 1 ], detection_threshold = .5 ) Better model found at epoch 0 with COCOMetric value: 0.8460132624669883. Better model found at epoch 1 with COCOMetric value: 0.8651620055945506. Better model found at epoch 5 with COCOMetric value: 0.8691152429678355. Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( ds [ 1 ], batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s] Training using Pytorch Lightning You have to follow the same procedure as for the Fastai example. It is quite similar to the Fastai one except we don't need to convert dataloaders like in Fastai. PL dataloaders are just pytorch dataloaders. # Create a model class LightModel(model_type.lightning.ModelAdapter): def configure_optimizers(self): return SGD(self.parameters(), lr=1e-4) light_model = LightModel(model, metrics=metrics) # Create a trainer trainer = pl.Trainer(max_epochs=10, gpus=1) # First Pass (size = 384) # Transforms presize = 512 size = 384 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) First training trainer.fit(light_model, dls[0], dls[1]) # Second Pass (size = 512) presize = 640 size = 512 # Third Pass (size = 640) # presize = 768 # size = 640 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) # Replace current dataloaders by the new ones (corresponding to the new size) trainer.train_dataloader = dls[0] trainer.valid_dataloader = dls[1] # Subsequent training trainer.fit(light_model, dls[0], dls[1]) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_retinanet_prog_resizing_1.pth' ) Mounted at /content/gdrive Happy Learning! If you need any assistance, feel free to join our forum .","title":"Progressive resizing"},{"location":"progressive_resizing/#training-using-progressive-resizing","text":"Quote from Fastbook: jargon: progressive resizing: Gradually using larger and larger images as you train. Progressive resizing is a very effective technique to train model from scratch or using transfer learning. IceVision now offers a good support for that technique. For more information about the progressive resizing technique, please check out the reference, here below: Fastai Fastbook Chapter: https://github.com/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb Check out the section: Progressive Resizing","title":"Training using progressive resizing"},{"location":"progressive_resizing/#paper-highlighting-the-importance-of-progressive-resizing","text":"Rethinking Training from Scratch for Object Detection EfficientNetV2: Smaller Models and Faster Training","title":"Paper highlighting the importance of progressive resizing:"},{"location":"progressive_resizing/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset using the progressive resizing technique. The main differences with IceVision standard training are the use of: The get_dataloaders() method # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) replacing dataloaders (corresponding to different image sizes) in either a Fastai Learner object or a Pytorch-Lightning Trainer object as follow: For Fastai: # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls # Standard training learn . lr_find () learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) Pytorch-Lightning: # Replace current dataloaders by the new ones (corresponding to the new size) trainer . train_dataloader = dls [ 0 ] trainer . valid_dataloader = dls [ 1 ] # Standard training trainer . fit ( light_model , dls [ 0 ], dls [ 1 ])","title":"Introduction"},{"location":"progressive_resizing/#installing-icevision-and-icedata","text":"# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"progressive_resizing/#imports","text":"from icevision.all import * from icevision.models.utils import get_dataloaders from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /Users/fra/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"progressive_resizing/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map","title":"Datasets : Fridge Objects dataset"},{"location":"progressive_resizing/#creating-a-model","text":"# Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Creating a model"},{"location":"progressive_resizing/#train-and-validation-dataset-transforms","text":"","title":"Train and Validation Dataset Transforms"},{"location":"progressive_resizing/#initial-size-first-size-size-384","text":"# Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) # dls[0].dataset[0] ds [ 0 ][ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=384, height=384) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 384x384x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:131.17572064203858, ymin:239.0191364865004, xmax:325.2123962126736, ymax:312.68812907493447)>, <BBox (xmin:82.22909604282495, ymin:82.72545127450111, xmax:181.53168623735334, ymax:288.81880204425784)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1]","title":"Initial size: First size (size = 384)"},{"location":"progressive_resizing/#dataloader","text":"model_type . show_batch ( first ( dls [ 0 ]), ncols = 4 )","title":"DataLoader"},{"location":"progressive_resizing/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"progressive_resizing/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"progressive_resizing/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = dls , model = model , metrics = metrics , cbs = SaveModelCallback ( monitor = 'COCOMetric' )) learn . lr_find () SuggestedLRs(lr_min=0.00010000000474974513, lr_steep=0.00015848931798245758)","title":"Training using fastai"},{"location":"progressive_resizing/#first-pass-first-training-with-the-size-384","text":"learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.303019 1.198495 0.018335 00:07 Better model found at epoch 0 with COCOMetric value: 0.018334512022630832. epoch train_loss valid_loss COCOMetric time 0 1.146912 1.077046 0.076843 00:06 1 1.078911 0.835020 0.172249 00:06 2 0.936218 0.585196 0.317453 00:06 3 0.803767 0.500256 0.409220 00:06 4 0.701060 0.394725 0.598500 00:06 5 0.623153 0.357143 0.649460 00:06 6 0.564359 0.360178 0.691023 00:06 7 0.518424 0.343954 0.707619 00:06 8 0.478071 0.333479 0.735202 00:06 9 0.447511 0.331000 0.739396 00:06 Better model found at epoch 0 with COCOMetric value: 0.07684290821192277. Better model found at epoch 1 with COCOMetric value: 0.17224854342050785. Better model found at epoch 2 with COCOMetric value: 0.31745342072333244. Better model found at epoch 3 with COCOMetric value: 0.4092202239902201. Better model found at epoch 4 with COCOMetric value: 0.5985001096045487. Better model found at epoch 5 with COCOMetric value: 0.6494598763585641. Better model found at epoch 6 with COCOMetric value: 0.6910227038257398. Better model found at epoch 7 with COCOMetric value: 0.7076194302452169. Better model found at epoch 8 with COCOMetric value: 0.735202499994352. Better model found at epoch 9 with COCOMetric value: 0.7393958039433105.","title":"First Pass: First Training with the size = 384"},{"location":"progressive_resizing/#restart-resizing-from-here","text":"","title":"Restart resizing from here"},{"location":"progressive_resizing/#subsequent-pass-subsequent-training-with-the-size-512","text":"# Second Pass (size = 512) # presize = 640 # size = 512 # Third Pass (size = 640) presize = 768 size = 640 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) dls [ 0 ] . dataset [ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=640, height=640) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 640x640x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:98.14351388888991, ymin:376.93381905417573, xmax:576.7811863685447, ymax:620.2620189083259)>, <BBox (xmin:5.201505318565632, ymin:12.437821266274758, xmax:329.20034254651534, ymax:498.3316159707134)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1]","title":"Subsequent Pass: Subsequent Training with the size = 512"},{"location":"progressive_resizing/#convert-pytorch-dataloaders-to-fastai-dataloaders","text":"from icevision.engines.fastai import * fastai_dls = convert_dataloaders_to_fastai ( dls = dls ) # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls print ( fastai_dls [ 0 ]) print ( learn . dls [ 0 ]) learn . lr_find () <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> SuggestedLRs(lr_min=2.2908675418875645e-07, lr_steep=9.12010818865383e-07) learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) # learn.fit_one_cycle(10, 2e-4) epoch train_loss valid_loss COCOMetric time 0 0.200535 0.183777 0.857028 00:13 Better model found at epoch 0 with COCOMetric value: 0.8570280948418122. epoch train_loss valid_loss COCOMetric time 0 0.192144 0.188399 0.846013 00:10 1 0.184424 0.167971 0.865162 00:10 2 0.192361 0.194635 0.853915 00:10 3 0.191424 0.195987 0.837771 00:10 4 0.192338 0.186021 0.828367 00:10 5 0.189835 0.152657 0.869115 00:10 6 0.184088 0.163762 0.859548 00:10 7 0.179976 0.155070 0.868776 00:10 8 0.175261 0.156499 0.867725 00:10 9 0.171284 0.156758 0.863302 00:10 model_type . show_results ( model , ds [ 1 ], detection_threshold = .5 ) Better model found at epoch 0 with COCOMetric value: 0.8460132624669883. Better model found at epoch 1 with COCOMetric value: 0.8651620055945506. Better model found at epoch 5 with COCOMetric value: 0.8691152429678355.","title":"Convert Pytorch DataLoaders to Fastai DataLoaders"},{"location":"progressive_resizing/#inference","text":"","title":"Inference"},{"location":"progressive_resizing/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( ds [ 1 ], batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s]","title":"Predicting a batch of images"},{"location":"progressive_resizing/#training-using-pytorch-lightning","text":"You have to follow the same procedure as for the Fastai example. It is quite similar to the Fastai one except we don't need to convert dataloaders like in Fastai. PL dataloaders are just pytorch dataloaders. # Create a model class LightModel(model_type.lightning.ModelAdapter): def configure_optimizers(self): return SGD(self.parameters(), lr=1e-4) light_model = LightModel(model, metrics=metrics) # Create a trainer trainer = pl.Trainer(max_epochs=10, gpus=1) # First Pass (size = 384) # Transforms presize = 512 size = 384 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) First training trainer.fit(light_model, dls[0], dls[1]) # Second Pass (size = 512) presize = 640 size = 512 # Third Pass (size = 640) # presize = 768 # size = 640 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) # Replace current dataloaders by the new ones (corresponding to the new size) trainer.train_dataloader = dls[0] trainer.valid_dataloader = dls[1] # Subsequent training trainer.fit(light_model, dls[0], dls[1])","title":"Training using Pytorch Lightning"},{"location":"progressive_resizing/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_retinanet_prog_resizing_1.pth' ) Mounted at /content/gdrive","title":"Saving Model on Google Drive"},{"location":"progressive_resizing/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"readme_mkdocs/","text":"IceVision Documentation The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs . Building the documentation Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Generating Docs"},{"location":"readme_mkdocs/#icevision-documentation","text":"The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs .","title":"IceVision Documentation"},{"location":"readme_mkdocs/#building-the-documentation","text":"Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Building the documentation"},{"location":"transition/","text":"How to transition Parser The major modification was in the Parser API, the concept of a ParserMixin was completely removed. Now, instead of starting by defining the parser, we start by defining the record: Before: class MyParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): After: template_record = BaseRecord ( ( FilepathRecordComponent (), InstancesLabelsRecordComponent (), BBoxesRecordComponent (), ) ) We then continue to generate the parser template: Before: MyParser . generate_template () After: Parser . generate_template ( template_record ) And here's how the parser class looks: Before: class ChessParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): def __init__ ( self , data_dir ): self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) super () . __init__ ( class_map = class_map ) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def filepath ( self , o ) -> Union [ str , Path ]: return self . data_dir / 'images' / o . filename def image_width_height ( self , o ) -> Tuple [ int , int ]: return o . width , o . height def labels ( self , o ) -> List [ int ]: return [ o . label ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )] After: class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record ): record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detect . set_class_map ( self . class_map ) record . detect . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detect . add_labels ([ o . label ]) Record The attributes on the record are now separated by task, to access bboxes you now have to do record.detection.bboxes instead of record.bboxes . Common attributes (not specific to one task) can still be accessed directly: record.filepath . You can use print to check the attributes on the record and how to access them: BaseRecord common : - Image size ImgSize ( width = 416 , height = 416 ) - Image ID : 3 - Filepath : / home / lgvaz /. icevision / data / chess_sample / chess_sample - master / images / e79deba8fe520409790b601ad61da4ee_jpg . rf .016 bc04dee292f80d1f975931f32bc21 . jpg - Image : None detection : - BBoxes : [ < BBox ( xmin : 208 , ymin : 88 , xmax : 226 , ymax : 128 ) > ] - Labels : [ 5 ] All fields under common can be accessed directly, the others have to be specifified by it's corresponding task.","title":"How to transition"},{"location":"transition/#how-to-transition","text":"","title":"How to transition"},{"location":"transition/#parser","text":"The major modification was in the Parser API, the concept of a ParserMixin was completely removed. Now, instead of starting by defining the parser, we start by defining the record: Before: class MyParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): After: template_record = BaseRecord ( ( FilepathRecordComponent (), InstancesLabelsRecordComponent (), BBoxesRecordComponent (), ) ) We then continue to generate the parser template: Before: MyParser . generate_template () After: Parser . generate_template ( template_record ) And here's how the parser class looks: Before: class ChessParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): def __init__ ( self , data_dir ): self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) super () . __init__ ( class_map = class_map ) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def filepath ( self , o ) -> Union [ str , Path ]: return self . data_dir / 'images' / o . filename def image_width_height ( self , o ) -> Tuple [ int , int ]: return o . width , o . height def labels ( self , o ) -> List [ int ]: return [ o . label ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )] After: class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record ): record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detect . set_class_map ( self . class_map ) record . detect . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detect . add_labels ([ o . label ])","title":"Parser"},{"location":"transition/#record","text":"The attributes on the record are now separated by task, to access bboxes you now have to do record.detection.bboxes instead of record.bboxes . Common attributes (not specific to one task) can still be accessed directly: record.filepath . You can use print to check the attributes on the record and how to access them: BaseRecord common : - Image size ImgSize ( width = 416 , height = 416 ) - Image ID : 3 - Filepath : / home / lgvaz /. icevision / data / chess_sample / chess_sample - master / images / e79deba8fe520409790b601ad61da4ee_jpg . rf .016 bc04dee292f80d1f975931f32bc21 . jpg - Image : None detection : - BBoxes : [ < BBox ( xmin : 208 , ymin : 88 , xmax : 226 , ymax : 128 ) > ] - Labels : [ 5 ] All fields under common can be accessed directly, the others have to be specifified by it's corresponding task.","title":"Record"},{"location":"voc_predefined_splits/","text":"How to parse a voc dataset using predefined splits Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * Load Pascal VOC 2012 dataset path = icedata . voc . load_data () Set images, annotations and imagesets directories annotations_dir = path / \"Annotations\" images_dir = path / \"JPEGImages\" imagesets_dir = path / \"ImageSets/Main\" Define class_map class_map = icedata . voc . class_map () Split data using imagesets ImageSets directory contains text files containing subsets of the dataset. We will split our dataset using the train and validation sets for aeroplanes. ImageSets directory contains multiple text files containing subsets of images from JPEGImages. We can use these files to select subsets of our data ie aeroplanes. The values we need to pass to FixedSplitter are the values returned by record_id in our parser rather than the filenames. train = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_train.txt\" )] val = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_val.txt\" )] presplits = [ train , val ] data_splitter = FixedSplitter ( presplits ) Parser: use icevision predefined VOC parser parser = parsers . VOCBBoxParser ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) Train and validation records train_records , valid_records = parser . parse ( data_splitter , autofix = False ) show_records ( train_records [: 2 ], ncols = 2 ) 0%| | 0/17125 [00:00<?, ?it/s]","title":"Fixed Splitter"},{"location":"voc_predefined_splits/#how-to-parse-a-voc-dataset-using-predefined-splits","text":"","title":"How to parse a voc dataset using predefined splits"},{"location":"voc_predefined_splits/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"voc_predefined_splits/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"voc_predefined_splits/#load-pascal-voc-2012-dataset","text":"path = icedata . voc . load_data ()","title":"Load Pascal VOC 2012 dataset"},{"location":"voc_predefined_splits/#set-images-annotations-and-imagesets-directories","text":"annotations_dir = path / \"Annotations\" images_dir = path / \"JPEGImages\" imagesets_dir = path / \"ImageSets/Main\"","title":"Set images, annotations and imagesets directories"},{"location":"voc_predefined_splits/#define-class_map","text":"class_map = icedata . voc . class_map ()","title":"Define class_map"},{"location":"voc_predefined_splits/#split-data-using-imagesets","text":"ImageSets directory contains text files containing subsets of the dataset. We will split our dataset using the train and validation sets for aeroplanes. ImageSets directory contains multiple text files containing subsets of images from JPEGImages. We can use these files to select subsets of our data ie aeroplanes. The values we need to pass to FixedSplitter are the values returned by record_id in our parser rather than the filenames. train = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_train.txt\" )] val = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_val.txt\" )] presplits = [ train , val ] data_splitter = FixedSplitter ( presplits )","title":"Split data using imagesets"},{"location":"voc_predefined_splits/#parser-use-icevision-predefined-voc-parser","text":"parser = parsers . VOCBBoxParser ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map )","title":"Parser: use icevision predefined VOC parser"},{"location":"voc_predefined_splits/#train-and-validation-records","text":"train_records , valid_records = parser . parse ( data_splitter , autofix = False ) show_records ( train_records [: 2 ], ncols = 2 ) 0%| | 0/17125 [00:00<?, ?it/s]","title":"Train and validation records"},{"location":"wandb_efficientdet/","text":"IceVision meets W&B IceVision + W&B = Agnostic Object Detection Framework with Outstanding Experiments Tracking IceVision fully supports W&B by providing a one-liner API that enables users to track their trained models and display both the predicted and ground truth bounding boxes. W&B makes visualizing and tracking different models performance a highly enjoyable task. Indeed, we are able to monitor the performance of several EfficientDet backbones by changing few lines of code and obtaining very intuitive and easy-to-interpret figures that highlights both the similarities and differences between the different backbones. For more information check the Report . Note, however, that the report refers to an older version of IceVision. This tutorial is updated for IceVision 0.7. This tutorial emphasizes the additional work required to integrated W&B. If you are new to IceVision, then we suggest that you look at the getting started with object detection tutorial . In this tutorial, we are using the fastai library training loop, the efficientdet object detection model, and a sample dataset with images of objects that you might find in a fridge. Following the usual practice with IceVision, you can use W&B with other training loops, model libraries, models and backbones. The W&B specific lines below would not need to be changed. Install IceVision and IceData If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'} Imports from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... Load the Fridge Objects dataset The fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> Train and Validation Datasets # Transforms image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Create the model In IceVision, we need to select the model type and backbone. For this tutorial, we are selecting efficientdet and the tf_lite0 backbone. Some models require additional information, such as the image_size . # Library and model selection model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 ( pretrained = True ) # The efficientdet model requires an img_size parameter extra_args = { 'img_size' : image_size } model = model_type . model ( backbone = backbone , num_classes = len ( parser . class_map ), ** extra_args ) Downloading: \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_lite0-f5f303a9.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientdet_lite0-f5f303a9.pth Create the dataloaders The dataloaders differ somewhat across the model_types, so creating them comes after selecting the model type. # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Training Intialize W&B At this point, we initialize W&B. This works in the jupyter notebook, but it is more typical run W&B from within a programme. This is partly because it enables you to track the progress of your training jobs from a custom dashboard from your browser, tablet, or phone. The full interface also makes it easy to compare multiple training runs, which can be very powerful when combined with IceVision. You can easily see which model is best suited to your problem. Initializing is a single line from the W&B library. wandb . init ( project = \"icevision-fridge\" , name = \"efficientdet_tf_lite0\" , reinit = True ) Create the learner This tutorial is using fastai , but IceVision lets you us other frameworks such as pytorch-lightning . In order to use W&B within fastai, you need to specify the WandbCallback , which results in logging the metrics as well as other key parameters, as well as the SaveModelCallback , which enables W&B to log the models. Logging the model is very powerful, as it ensures that you have a copy of the best version of the model as you train. If you are using W&B on-line, however, it causes your model to be transferred to the W&B database as well as saved in a local wandb directory. learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )], cbs = [ WandbCallback (), SaveModelCallback ()]) Train In this case, we use the fit_one_cycle training method from fastai, which uses a specific policy for adjusting the learning rate. This model is likely to take around 2-10 seconds per epoch, depending on your hardware. Training for 30 epochs on this small dataset typically reaches a level around 0.8 (COCOMetric), which is sufficient for our demonstration purposes and saves some time. learn . fit_one_cycle ( 30 , 1e-2 ) Could not gather input dimensions WandbCallback was not able to prepare a DataLoader for logging prediction samples -> 'Dataset' object has no attribute 'items' epoch train_loss valid_loss COCOMetric time 0 1.940700 1.267195 0.000069 00:12 1 1.685492 1.271025 0.000023 00:07 2 1.540221 1.264216 0.017094 00:07 3 1.424315 1.147959 0.155413 00:07 4 1.303290 1.082212 0.218895 00:07 5 1.184958 0.875454 0.310934 00:07 6 1.083239 0.831463 0.260008 00:07 7 0.993147 0.897370 0.278169 00:07 8 0.912498 0.779531 0.356216 00:07 9 0.844710 0.740413 0.360409 00:07 10 0.782819 0.723692 0.377731 00:07 11 0.729454 0.745544 0.323658 00:07 12 0.687574 0.653606 0.413525 00:07 13 0.651915 0.655265 0.417471 00:07 14 0.620716 0.651639 0.429272 00:07 15 0.589860 0.543764 0.468006 00:07 16 0.559970 0.516403 0.574289 00:07 17 0.531339 0.510701 0.534853 00:07 18 0.504182 0.490539 0.590217 00:07 19 0.482276 0.391926 0.669296 00:07 20 0.460442 0.391556 0.693068 00:07 21 0.444368 0.344799 0.738415 00:07 22 0.425590 0.352732 0.725862 00:07 23 0.409553 0.319861 0.769296 00:07 24 0.395036 0.308085 0.769391 00:07 25 0.383020 0.302642 0.775355 00:07 26 0.372328 0.293886 0.785769 00:07 27 0.362688 0.292389 0.796928 00:07 28 0.355098 0.289134 0.799669 00:07 29 0.347778 0.283973 0.806622 00:07 /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Better model found at epoch 0 with valid_loss value: 1.2671946287155151. Better model found at epoch 2 with valid_loss value: 1.2642161846160889. Better model found at epoch 3 with valid_loss value: 1.1479589939117432. Better model found at epoch 4 with valid_loss value: 1.0822120904922485. Better model found at epoch 5 with valid_loss value: 0.8754537105560303. Better model found at epoch 6 with valid_loss value: 0.8314631581306458. Better model found at epoch 8 with valid_loss value: 0.7795311808586121. Better model found at epoch 9 with valid_loss value: 0.7404130697250366. Better model found at epoch 10 with valid_loss value: 0.7236919403076172. Better model found at epoch 12 with valid_loss value: 0.6536057591438293. Better model found at epoch 14 with valid_loss value: 0.6516388654708862. Better model found at epoch 15 with valid_loss value: 0.5437638759613037. Better model found at epoch 16 with valid_loss value: 0.5164031982421875. Better model found at epoch 17 with valid_loss value: 0.5107009410858154. Better model found at epoch 18 with valid_loss value: 0.4905391037464142. Better model found at epoch 19 with valid_loss value: 0.3919256627559662. Better model found at epoch 20 with valid_loss value: 0.3915559649467468. Better model found at epoch 21 with valid_loss value: 0.34479889273643494. Better model found at epoch 23 with valid_loss value: 0.3198612332344055. Better model found at epoch 24 with valid_loss value: 0.3080853819847107. Better model found at epoch 25 with valid_loss value: 0.302642285823822. Better model found at epoch 26 with valid_loss value: 0.29388612508773804. Better model found at epoch 27 with valid_loss value: 0.2923893630504608. Better model found at epoch 28 with valid_loss value: 0.2891344130039215. Better model found at epoch 29 with valid_loss value: 0.28397276997566223. Show results We can now look athe results of the training in the notebook. model_type . show_results ( model , valid_ds ) /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Get predictions Let's get the list of predictions from our model. We do this by creating an infer_dl - a dataloader used for inference and then getting predictions from the data loader. Please note the keep_images=True . By default, the predictions include scores, labels, and bounding boxes. In our case, we want to keep the images so that we log them to W&B. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) 0%| | 0/4 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Log results to W&B Now comes the most important bit of this tutorial - actually logging the predictions to W&B. This takes one line specific to icevision and a second line to send the information to W&B. # Create wandb_images for each prediction wandb_images = wandb_img_preds ( preds , add_ground_truth = True ) # Log the wandb_images to wandb wandb . log ({ \"Predicted images\" : wandb_images }) After logging and finishing the training, it is good to mark the run as completed. This can take a few seconds, as we wait for the W&B processes to transfer data and finalize logging. # optional: mark the run as completed wandb . join () Happy Learning! If you need any assistance, feel free to join our forum .","title":"Model Tracking Using Wandb"},{"location":"wandb_efficientdet/#icevision-meets-wb","text":"IceVision + W&B = Agnostic Object Detection Framework with Outstanding Experiments Tracking IceVision fully supports W&B by providing a one-liner API that enables users to track their trained models and display both the predicted and ground truth bounding boxes. W&B makes visualizing and tracking different models performance a highly enjoyable task. Indeed, we are able to monitor the performance of several EfficientDet backbones by changing few lines of code and obtaining very intuitive and easy-to-interpret figures that highlights both the similarities and differences between the different backbones. For more information check the Report . Note, however, that the report refers to an older version of IceVision. This tutorial is updated for IceVision 0.7. This tutorial emphasizes the additional work required to integrated W&B. If you are new to IceVision, then we suggest that you look at the getting started with object detection tutorial . In this tutorial, we are using the fastai library training loop, the efficientdet object detection model, and a sample dataset with images of objects that you might find in a fridge. Following the usual practice with IceVision, you can use W&B with other training loops, model libraries, models and backbones. The W&B specific lines below would not need to be changed.","title":"IceVision meets W&amp;B"},{"location":"wandb_efficientdet/#install-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'}","title":"Install IceVision and IceData"},{"location":"wandb_efficientdet/#imports","text":"from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...","title":"Imports"},{"location":"wandb_efficientdet/#load-the-fridge-objects-dataset","text":"The fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>","title":"Load the Fridge Objects dataset"},{"location":"wandb_efficientdet/#train-and-validation-datasets","text":"# Transforms image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Datasets"},{"location":"wandb_efficientdet/#create-the-model","text":"In IceVision, we need to select the model type and backbone. For this tutorial, we are selecting efficientdet and the tf_lite0 backbone. Some models require additional information, such as the image_size . # Library and model selection model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 ( pretrained = True ) # The efficientdet model requires an img_size parameter extra_args = { 'img_size' : image_size } model = model_type . model ( backbone = backbone , num_classes = len ( parser . class_map ), ** extra_args ) Downloading: \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_lite0-f5f303a9.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientdet_lite0-f5f303a9.pth","title":"Create the model"},{"location":"wandb_efficientdet/#create-the-dataloaders","text":"The dataloaders differ somewhat across the model_types, so creating them comes after selecting the model type. # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked))","title":"Create the dataloaders"},{"location":"wandb_efficientdet/#training","text":"","title":"Training"},{"location":"wandb_efficientdet/#intialize-wb","text":"At this point, we initialize W&B. This works in the jupyter notebook, but it is more typical run W&B from within a programme. This is partly because it enables you to track the progress of your training jobs from a custom dashboard from your browser, tablet, or phone. The full interface also makes it easy to compare multiple training runs, which can be very powerful when combined with IceVision. You can easily see which model is best suited to your problem. Initializing is a single line from the W&B library. wandb . init ( project = \"icevision-fridge\" , name = \"efficientdet_tf_lite0\" , reinit = True )","title":"Intialize W&amp;B"},{"location":"wandb_efficientdet/#create-the-learner","text":"This tutorial is using fastai , but IceVision lets you us other frameworks such as pytorch-lightning . In order to use W&B within fastai, you need to specify the WandbCallback , which results in logging the metrics as well as other key parameters, as well as the SaveModelCallback , which enables W&B to log the models. Logging the model is very powerful, as it ensures that you have a copy of the best version of the model as you train. If you are using W&B on-line, however, it causes your model to be transferred to the W&B database as well as saved in a local wandb directory. learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )], cbs = [ WandbCallback (), SaveModelCallback ()])","title":"Create the learner"},{"location":"wandb_efficientdet/#train","text":"In this case, we use the fit_one_cycle training method from fastai, which uses a specific policy for adjusting the learning rate. This model is likely to take around 2-10 seconds per epoch, depending on your hardware. Training for 30 epochs on this small dataset typically reaches a level around 0.8 (COCOMetric), which is sufficient for our demonstration purposes and saves some time. learn . fit_one_cycle ( 30 , 1e-2 ) Could not gather input dimensions WandbCallback was not able to prepare a DataLoader for logging prediction samples -> 'Dataset' object has no attribute 'items' epoch train_loss valid_loss COCOMetric time 0 1.940700 1.267195 0.000069 00:12 1 1.685492 1.271025 0.000023 00:07 2 1.540221 1.264216 0.017094 00:07 3 1.424315 1.147959 0.155413 00:07 4 1.303290 1.082212 0.218895 00:07 5 1.184958 0.875454 0.310934 00:07 6 1.083239 0.831463 0.260008 00:07 7 0.993147 0.897370 0.278169 00:07 8 0.912498 0.779531 0.356216 00:07 9 0.844710 0.740413 0.360409 00:07 10 0.782819 0.723692 0.377731 00:07 11 0.729454 0.745544 0.323658 00:07 12 0.687574 0.653606 0.413525 00:07 13 0.651915 0.655265 0.417471 00:07 14 0.620716 0.651639 0.429272 00:07 15 0.589860 0.543764 0.468006 00:07 16 0.559970 0.516403 0.574289 00:07 17 0.531339 0.510701 0.534853 00:07 18 0.504182 0.490539 0.590217 00:07 19 0.482276 0.391926 0.669296 00:07 20 0.460442 0.391556 0.693068 00:07 21 0.444368 0.344799 0.738415 00:07 22 0.425590 0.352732 0.725862 00:07 23 0.409553 0.319861 0.769296 00:07 24 0.395036 0.308085 0.769391 00:07 25 0.383020 0.302642 0.775355 00:07 26 0.372328 0.293886 0.785769 00:07 27 0.362688 0.292389 0.796928 00:07 28 0.355098 0.289134 0.799669 00:07 29 0.347778 0.283973 0.806622 00:07 /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Better model found at epoch 0 with valid_loss value: 1.2671946287155151. Better model found at epoch 2 with valid_loss value: 1.2642161846160889. Better model found at epoch 3 with valid_loss value: 1.1479589939117432. Better model found at epoch 4 with valid_loss value: 1.0822120904922485. Better model found at epoch 5 with valid_loss value: 0.8754537105560303. Better model found at epoch 6 with valid_loss value: 0.8314631581306458. Better model found at epoch 8 with valid_loss value: 0.7795311808586121. Better model found at epoch 9 with valid_loss value: 0.7404130697250366. Better model found at epoch 10 with valid_loss value: 0.7236919403076172. Better model found at epoch 12 with valid_loss value: 0.6536057591438293. Better model found at epoch 14 with valid_loss value: 0.6516388654708862. Better model found at epoch 15 with valid_loss value: 0.5437638759613037. Better model found at epoch 16 with valid_loss value: 0.5164031982421875. Better model found at epoch 17 with valid_loss value: 0.5107009410858154. Better model found at epoch 18 with valid_loss value: 0.4905391037464142. Better model found at epoch 19 with valid_loss value: 0.3919256627559662. Better model found at epoch 20 with valid_loss value: 0.3915559649467468. Better model found at epoch 21 with valid_loss value: 0.34479889273643494. Better model found at epoch 23 with valid_loss value: 0.3198612332344055. Better model found at epoch 24 with valid_loss value: 0.3080853819847107. Better model found at epoch 25 with valid_loss value: 0.302642285823822. Better model found at epoch 26 with valid_loss value: 0.29388612508773804. Better model found at epoch 27 with valid_loss value: 0.2923893630504608. Better model found at epoch 28 with valid_loss value: 0.2891344130039215. Better model found at epoch 29 with valid_loss value: 0.28397276997566223.","title":"Train"},{"location":"wandb_efficientdet/#show-results","text":"We can now look athe results of the training in the notebook. model_type . show_results ( model , valid_ds ) /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes","title":"Show results"},{"location":"wandb_efficientdet/#get-predictions","text":"Let's get the list of predictions from our model. We do this by creating an infer_dl - a dataloader used for inference and then getting predictions from the data loader. Please note the keep_images=True . By default, the predictions include scores, labels, and bounding boxes. In our case, we want to keep the images so that we log them to W&B. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) 0%| | 0/4 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes","title":"Get predictions"},{"location":"wandb_efficientdet/#log-results-to-wb","text":"Now comes the most important bit of this tutorial - actually logging the predictions to W&B. This takes one line specific to icevision and a second line to send the information to W&B. # Create wandb_images for each prediction wandb_images = wandb_img_preds ( preds , add_ground_truth = True ) # Log the wandb_images to wandb wandb . log ({ \"Predicted images\" : wandb_images }) After logging and finishing the training, it is good to mark the run as completed. This can take a few seconds, as we wait for the W&B processes to transfer data and finalize logging. # optional: mark the run as completed wandb . join ()","title":"Log results to W&amp;B"},{"location":"wandb_efficientdet/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"}]}