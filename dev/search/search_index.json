{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Join our Forum IceVision is the first agnostic computer vision framework to offer a curated collection with hundreds of high-quality pre-trained models from torchvision, MMLabs, and soon Pytorch Image Models. It orchestrates the end-to-end deep learning workflow allowing to train networks with easy-to-use robust high-performance libraries such as Pytorch-Lightning and Fastai IceVision Unique Features: Data curation/cleaning with auto-fix Access to an exploratory data analysis dashboard Pluggable transforms for better model generalization Access to hundreds of neural net models Access to multiple training loop libraries Multi-task training to efficiently combine object detection, segmentation, and classification models Quick Example: How to train the Fridge Objects Dataset Happy Learning! If you need any assistance, feel free to: Join our Forum","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"Join our Forum IceVision is the first agnostic computer vision framework to offer a curated collection with hundreds of high-quality pre-trained models from torchvision, MMLabs, and soon Pytorch Image Models. It orchestrates the end-to-end deep learning workflow allowing to train networks with easy-to-use robust high-performance libraries such as Pytorch-Lightning and Fastai IceVision Unique Features: Data curation/cleaning with auto-fix Access to an exploratory data analysis dashboard Pluggable transforms for better model generalization Access to hundreds of neural net models Access to multiple training loop libraries Multi-task training to efficiently combine object detection, segmentation, and classification models","title":""},{"location":"#quick-example-how-to-train-the-fridge-objects-dataset","text":"","title":"Quick Example: How to train the Fridge Objects Dataset"},{"location":"#happy-learning","text":"If you need any assistance, feel free to: Join our Forum","title":"Happy Learning!"},{"location":"IceApp_coco/","text":"IceVision Deployment App: COCO Dataset This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"IceApp coco"},{"location":"IceApp_coco/#icevision-deployment-app-coco-dataset","text":"This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  COCO Dataset"},{"location":"IceApp_coco/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_coco/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_coco/#loading-trained-model","text":"class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_coco/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_coco/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_coco/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_coco/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_masks/","text":"IceVision Deployment App: PennFudan Dataset This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0] Defining the get_masks method: called by gr.Interface(fn=get_masks, ...) def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface # Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"IceApp masks"},{"location":"IceApp_masks/#icevision-deployment-app-pennfudan-dataset","text":"This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  PennFudan Dataset"},{"location":"IceApp_masks/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_masks/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_masks/#loading-trained-model","text":"class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_masks/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_masks/#defining-the-get_masks-method-called-by-grinterfacefnget_masks","text":"def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the get_masks method: called by gr.Interface(fn=get_masks, ...)"},{"location":"IceApp_masks/#gradio-user-interface","text":"# Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_masks/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_pets/","text":"IceVision Deployment App: PETS Dataset This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"IceApp pets"},{"location":"IceApp_pets/#icevision-deployment-app-pets-dataset","text":"This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App: PETS Dataset"},{"location":"IceApp_pets/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_pets/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_pets/#loading-trained-model","text":"class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_pets/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_pets/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_pets/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_pets/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"SAHI_inference/","text":"IceVision + SAHI: addressing low performance in small object detection This notebook showcases the newly added IceVision + SAHI integration. You can find more detailed info about this work in this blog post . Installing Icevision and dependencies + SAHI Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master Install SAHI ! pip install sahi - q # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... Loading the Fridge dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) train_records , valid_records = parser . parse () 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] Defining augmentations and datasets image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = ( image_size , image_size ), presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad (( image_size , image_size )), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Choosing model # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x #model_type = models.mmdet.faster_rcnn #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.retinanet #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.ssd #backbone = model_type.backbones.ssd512 elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite1 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . medium # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size print ( model_type , backbone , extra_args ) model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) Getting dataloaders, defining metrics and instantiate fastai learner train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 8 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 8 , shuffle = False ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Finding best learning rate learn . lr_find () /usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=0.0004786300996784121) Training the model learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.809848 3.245110 0.219802 00:22 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 2.858475 2.487748 0.435076 00:24 1 2.506602 1.631549 0.439791 00:19 2 2.196014 1.348319 0.443677 00:19 3 1.966267 1.245886 0.692616 00:19 4 1.783822 1.063900 0.778038 00:19 5 1.637959 0.931765 0.855932 00:19 6 1.499139 0.803554 0.919840 00:19 7 1.391289 0.777090 0.922477 00:19 8 1.303855 0.758904 0.918359 00:19 9 1.213133 0.675470 0.942594 00:19 10 1.139793 0.716553 0.924616 00:19 11 1.084275 0.671022 0.935786 00:19 12 1.037781 0.623612 0.948895 00:19 13 0.991282 0.619961 0.960125 00:19 14 0.950265 0.616058 0.961337 00:19 15 0.918986 0.610729 0.958616 00:19 16 0.886407 0.608031 0.955717 00:19 17 0.858739 0.600173 0.956490 00:19 18 0.835620 0.598794 0.954467 00:19 19 0.810680 0.598279 0.956181 00:19 Downloading sample image ! wget -- no - check - certificate - O small_fridge . jpg 'https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS' --2021-11-30 18:12:37-- https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS Resolving docs.google.com (docs.google.com)... 172.217.214.101, 172.217.214.113, 172.217.214.139, ... Connecting to docs.google.com (docs.google.com)|172.217.214.101|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download [following] Warning: wildcards not supported in HTTP. --2021-11-30 18:12:37-- https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download Resolving doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)... 142.250.159.132, 2607:f8b0:4001:c58::84 Connecting to doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)|142.250.159.132|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 48117 (47K) [image/jpeg] Saving to: \u2018small_fridge.jpg\u2019 small_fridge.jpg 100%[===================>] 46.99K --.-KB/s in 0.001s 2021-11-30 18:12:37 (91.5 MB/s) - \u2018small_fridge.jpg\u2019 saved [48117/48117] PIL . Image . open ( \"small_fridge.jpg\" ) . resize (( 500 , 300 )) Running inference without SAHI No bbox detected! img = PIL . Image . open ( \"small_fridge.jpg\" ) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = parser . class_map , detection_threshold = 0.4 ) pred_dict [ 'img' ] Running inference with SAHI Check out how almost all objects (too small for a one-shot prediction) are detected using the sliding window approach SAHI offers. from icevision.models.inference_sahi import IceSahiModel sahimodel = IceSahiModel ( model_type = model_type , model = model , class_map = parser . class_map , tfms = valid_tfms , confidence_threshold = 0.4 ) pred = sahimodel . get_sliced_prediction ( \"small_fridge.jpg\" , keep_sahi_format = False , return_img = True , slice_height = 128 , slice_width = 128 , overlap_height_ratio = 0.2 , overlap_width_ratio = 0.2 , ) pred [ \"img\" ] Number of slices: 91","title":"Small Object Detection with SAHI"},{"location":"SAHI_inference/#icevision-sahi-addressing-low-performance-in-small-object-detection","text":"This notebook showcases the newly added IceVision + SAHI integration. You can find more detailed info about this work in this blog post .","title":"IceVision + SAHI: addressing low performance in small object detection"},{"location":"SAHI_inference/#installing-icevision-and-dependencies-sahi","text":"Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master Install SAHI ! pip install sahi - q # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing Icevision and dependencies + SAHI"},{"location":"SAHI_inference/#imports","text":"from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...","title":"Imports"},{"location":"SAHI_inference/#loading-the-fridge-dataset","text":"url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) train_records , valid_records = parser . parse () 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s]","title":"Loading the Fridge dataset"},{"location":"SAHI_inference/#defining-augmentations-and-datasets","text":"image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = ( image_size , image_size ), presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad (( image_size , image_size )), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Defining augmentations and datasets"},{"location":"SAHI_inference/#choosing-model","text":"# Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x #model_type = models.mmdet.faster_rcnn #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.retinanet #backbone = model_type.backbones.resnet50_fpn_1x #model_type = models.mmdet.ssd #backbone = model_type.backbones.ssd512 elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite1 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . medium # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size print ( model_type , backbone , extra_args ) model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Choosing model"},{"location":"SAHI_inference/#getting-dataloaders-defining-metrics-and-instantiate-fastai-learner","text":"train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 8 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 8 , shuffle = False ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked))","title":"Getting dataloaders, defining metrics and instantiate fastai learner"},{"location":"SAHI_inference/#finding-best-learning-rate","text":"learn . lr_find () /usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=0.0004786300996784121)","title":"Finding best learning rate"},{"location":"SAHI_inference/#training-the-model","text":"learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.809848 3.245110 0.219802 00:22 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 2.858475 2.487748 0.435076 00:24 1 2.506602 1.631549 0.439791 00:19 2 2.196014 1.348319 0.443677 00:19 3 1.966267 1.245886 0.692616 00:19 4 1.783822 1.063900 0.778038 00:19 5 1.637959 0.931765 0.855932 00:19 6 1.499139 0.803554 0.919840 00:19 7 1.391289 0.777090 0.922477 00:19 8 1.303855 0.758904 0.918359 00:19 9 1.213133 0.675470 0.942594 00:19 10 1.139793 0.716553 0.924616 00:19 11 1.084275 0.671022 0.935786 00:19 12 1.037781 0.623612 0.948895 00:19 13 0.991282 0.619961 0.960125 00:19 14 0.950265 0.616058 0.961337 00:19 15 0.918986 0.610729 0.958616 00:19 16 0.886407 0.608031 0.955717 00:19 17 0.858739 0.600173 0.956490 00:19 18 0.835620 0.598794 0.954467 00:19 19 0.810680 0.598279 0.956181 00:19","title":"Training the model"},{"location":"SAHI_inference/#downloading-sample-image","text":"! wget -- no - check - certificate - O small_fridge . jpg 'https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS' --2021-11-30 18:12:37-- https://docs.google.com/uc?export=download&id=16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS Resolving docs.google.com (docs.google.com)... 172.217.214.101, 172.217.214.113, 172.217.214.139, ... Connecting to docs.google.com (docs.google.com)|172.217.214.101|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download [following] Warning: wildcards not supported in HTTP. --2021-11-30 18:12:37-- https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tt3td2mcu62ih3vqc4ummb85jq526859/1638295950000/14481291337477770344/*/16cq_RmKLuXLGXXiDwdyWcE-0HpyYU1kS?e=download Resolving doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)... 142.250.159.132, 2607:f8b0:4001:c58::84 Connecting to doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)|142.250.159.132|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 48117 (47K) [image/jpeg] Saving to: \u2018small_fridge.jpg\u2019 small_fridge.jpg 100%[===================>] 46.99K --.-KB/s in 0.001s 2021-11-30 18:12:37 (91.5 MB/s) - \u2018small_fridge.jpg\u2019 saved [48117/48117] PIL . Image . open ( \"small_fridge.jpg\" ) . resize (( 500 , 300 ))","title":"Downloading sample image"},{"location":"SAHI_inference/#running-inference-without-sahi","text":"No bbox detected! img = PIL . Image . open ( \"small_fridge.jpg\" ) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = parser . class_map , detection_threshold = 0.4 ) pred_dict [ 'img' ]","title":"Running inference without SAHI"},{"location":"SAHI_inference/#running-inference-with-sahi","text":"Check out how almost all objects (too small for a one-shot prediction) are detected using the sliding window approach SAHI offers. from icevision.models.inference_sahi import IceSahiModel sahimodel = IceSahiModel ( model_type = model_type , model = model , class_map = parser . class_map , tfms = valid_tfms , confidence_threshold = 0.4 ) pred = sahimodel . get_sliced_prediction ( \"small_fridge.jpg\" , keep_sahi_format = False , return_img = True , slice_height = 128 , slice_width = 128 , overlap_height_ratio = 0.2 , overlap_width_ratio = 0.2 , ) pred [ \"img\" ] Number of slices: 91","title":"Running inference with SAHI"},{"location":"YOLOv5/","text":"YOLOv5 from Ultralytics This notebook showcases the newly added YOLOv5 suite of models to the IceVision library. Installing IceVision, IceData and yolov5-icevision yolov5-icevision is a tiny IceVision wrapper around the official YOLOv5 repo from Ultralytics, to make the code pip-installable. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master Imports from icevision.all import * import icedata \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m69\u001b[0m Loading the Fridge dataset data_dir = icedata . fridge . load_data () class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) train_records , valid_records = parser . parse () presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) len ( train_ds ), len ( valid_ds ) 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m136\u001b[0m (102, 26) YOLOv5 dataloaders model_type = models . ultralytics . yolov5 train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 2 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 2 , shuffle = False ) Showing what's inside a batch dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 2 , shuffle = False ) batch = first ( dl ) model_type . show_batch ( batch , ncols = 4 ) YOLOv5 model You can choose between a small , medium and large backbone. backbone = model_type . backbones . small ( pretrained = True ) #backbone = model_type.backbones.medium(pretrained=True) #backbone = model_type.backbones.large(pretrained=True) #backbone = model_type.backbones.extra_large(pretrained=True) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size , device = torch . device ( \"cuda\" )) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt to /root/.icevision/yolo/yolov5s.pt... 0%| | 0.00/14.1M [00:00<?, ?B/s] Training a fastai Learner metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(lr_min=0.0033113110810518267, lr_steep=9.12010818865383e-07) learn . fit_one_cycle ( 50 , 3e-4 ) <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='5' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress> 10.00% [5/50 00:16<02:28] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='0' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress> 0.00% [0/7 00:00<00:00] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 5 0.661492 0.513050 0.257225 00:03 6 0.622621 0.455875 0.482885 00:03 7 0.595726 0.436317 0.271832 00:03 8 0.563349 0.419788 0.236814 00:03 9 0.539367 0.387893 0.348432 00:03 10 0.515677 0.375338 0.283068 00:03 11 0.496582 0.369159 0.413665 00:03 12 0.474011 0.326403 0.472587 00:03 13 0.456676 0.332162 0.553740 00:03 14 0.434918 0.298136 0.627820 00:03 15 0.414545 0.272040 0.652365 00:03 16 0.395862 0.329013 0.391859 00:03 17 0.383311 0.271511 0.635272 00:03 18 0.367031 0.303586 0.322467 00:03 19 0.354010 0.271726 0.497521 00:03 20 0.340034 0.226416 0.700743 00:03 21 0.326033 0.225464 0.619431 00:03 22 0.310925 0.203816 0.769183 00:03 23 0.295661 0.189258 0.769431 00:03 24 0.281358 0.195591 0.681807 00:03 25 0.269978 0.190864 0.731312 00:03 26 0.260026 0.189974 0.700495 00:03 27 0.252257 0.180860 0.738119 00:03 28 0.247666 0.165262 0.762871 00:03 29 0.237585 0.172969 0.737624 00:03 30 0.228174 0.156880 0.806559 00:03 31 0.219802 0.149674 0.856312 00:03 32 0.214853 0.147571 0.800248 00:03 33 0.211070 0.139506 0.875248 00:03 34 0.203329 0.139021 0.843936 00:03 35 0.195134 0.126844 0.856312 00:03 36 0.188641 0.123761 0.881312 00:03 37 0.183303 0.128407 0.875000 00:03 38 0.177788 0.113928 0.868936 00:03 39 0.171578 0.126054 0.900248 00:03 40 0.168272 0.113003 0.868936 00:03 41 0.166278 0.113589 0.906559 00:03 42 0.161366 0.109419 0.893936 00:03 43 0.161012 0.107286 0.925248 00:03 44 0.155416 0.107902 0.925248 00:03 45 0.150046 0.104697 0.918936 00:03 46 0.145804 0.104805 0.918936 00:03 47 0.141888 0.104281 0.918936 00:03 48 0.137711 0.103718 0.918936 00:03 49 0.135240 0.103554 0.918936 00:03 ### Showing training results model_type . show_results ( model , valid_ds , detection_threshold = 0.25 ) ![png](images/YOLOv5/YOLOv5_23_0.png) ## Inference pipeline From a dataset... preds = model_type . predict ( model , valid_ds , detection_threshold = 0.25 ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) ![png](images/YOLOv5/YOLOv5_25_0.png) ... and from a Dataloader infer_dl = model_type . infer_dl ( valid_ds , batch_size = 1 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , detection_threshold = 0.25 , keep_images = True ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) 0%| | 0/26 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_27_1.png) ## Inspecting model predictions with `plot_top_losses` What are the images the model is having a hard time time with? #by = \"loss_total\" #by = \"loss_yolo\" by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_yolo\" : 0.5 , }, } sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = by ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_yolo']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m219\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_29_3.png) ## Training a PyTorch Lightning model class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 3e-4 ) backbone = model_type . backbones . medium ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 30 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5m.pt to /root/.icevision/yolo/yolov5m.pt... 0%| | 0.00/41.1M [00:00<?, ?B/s] GPU available: True, used: True TPU available: None, using: 0 TPU cores | Name | Type | Params -------------------------------- 0 | model | Model | 21.1 M -------------------------------- 21.1 M Trainable params 0 Non-trainable params 21.1 M Total params 84.290 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] 1 ## Happy Learning! If you need any assistance, feel free to join our [forum](https://discord.gg/JDBeZYK).","title":"YOLOv5"},{"location":"YOLOv5/#yolov5-from-ultralytics","text":"This notebook showcases the newly added YOLOv5 suite of models to the IceVision library.","title":"YOLOv5 from Ultralytics"},{"location":"YOLOv5/#installing-icevision-icedata-and-yolov5-icevision","text":"yolov5-icevision is a tiny IceVision wrapper around the official YOLOv5 repo from Ultralytics, to make the code pip-installable. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master","title":"Installing IceVision, IceData and yolov5-icevision"},{"location":"YOLOv5/#imports","text":"from icevision.all import * import icedata \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m69\u001b[0m","title":"Imports"},{"location":"YOLOv5/#loading-the-fridge-dataset","text":"data_dir = icedata . fridge . load_data () class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) train_records , valid_records = parser . parse () presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) len ( train_ds ), len ( valid_ds ) 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m136\u001b[0m (102, 26)","title":"Loading the Fridge dataset"},{"location":"YOLOv5/#yolov5-dataloaders","text":"model_type = models . ultralytics . yolov5 train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 2 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 2 , shuffle = False ) Showing what's inside a batch dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 2 , shuffle = False ) batch = first ( dl ) model_type . show_batch ( batch , ncols = 4 )","title":"YOLOv5 dataloaders"},{"location":"YOLOv5/#yolov5-model","text":"You can choose between a small , medium and large backbone. backbone = model_type . backbones . small ( pretrained = True ) #backbone = model_type.backbones.medium(pretrained=True) #backbone = model_type.backbones.large(pretrained=True) #backbone = model_type.backbones.extra_large(pretrained=True) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size , device = torch . device ( \"cuda\" )) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt to /root/.icevision/yolo/yolov5s.pt... 0%| | 0.00/14.1M [00:00<?, ?B/s]","title":"YOLOv5 model"},{"location":"YOLOv5/#training-a-fastai-learner","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(lr_min=0.0033113110810518267, lr_steep=9.12010818865383e-07) learn . fit_one_cycle ( 50 , 3e-4 ) <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='5' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress> 10.00% [5/50 00:16<02:28] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 <div> <style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } </style> <progress value='0' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress> 0.00% [0/7 00:00<00:00] </div> epoch train_loss valid_loss COCOMetric time 0 0.819279 0.792441 0.003035 00:03 1 0.788863 0.732729 0.012280 00:03 2 0.787299 0.656644 0.030487 00:03 3 0.754054 0.590940 0.067887 00:03 4 0.706845 0.523452 0.099730 00:03 5 0.661492 0.513050 0.257225 00:03 6 0.622621 0.455875 0.482885 00:03 7 0.595726 0.436317 0.271832 00:03 8 0.563349 0.419788 0.236814 00:03 9 0.539367 0.387893 0.348432 00:03 10 0.515677 0.375338 0.283068 00:03 11 0.496582 0.369159 0.413665 00:03 12 0.474011 0.326403 0.472587 00:03 13 0.456676 0.332162 0.553740 00:03 14 0.434918 0.298136 0.627820 00:03 15 0.414545 0.272040 0.652365 00:03 16 0.395862 0.329013 0.391859 00:03 17 0.383311 0.271511 0.635272 00:03 18 0.367031 0.303586 0.322467 00:03 19 0.354010 0.271726 0.497521 00:03 20 0.340034 0.226416 0.700743 00:03 21 0.326033 0.225464 0.619431 00:03 22 0.310925 0.203816 0.769183 00:03 23 0.295661 0.189258 0.769431 00:03 24 0.281358 0.195591 0.681807 00:03 25 0.269978 0.190864 0.731312 00:03 26 0.260026 0.189974 0.700495 00:03 27 0.252257 0.180860 0.738119 00:03 28 0.247666 0.165262 0.762871 00:03 29 0.237585 0.172969 0.737624 00:03 30 0.228174 0.156880 0.806559 00:03 31 0.219802 0.149674 0.856312 00:03 32 0.214853 0.147571 0.800248 00:03 33 0.211070 0.139506 0.875248 00:03 34 0.203329 0.139021 0.843936 00:03 35 0.195134 0.126844 0.856312 00:03 36 0.188641 0.123761 0.881312 00:03 37 0.183303 0.128407 0.875000 00:03 38 0.177788 0.113928 0.868936 00:03 39 0.171578 0.126054 0.900248 00:03 40 0.168272 0.113003 0.868936 00:03 41 0.166278 0.113589 0.906559 00:03 42 0.161366 0.109419 0.893936 00:03 43 0.161012 0.107286 0.925248 00:03 44 0.155416 0.107902 0.925248 00:03 45 0.150046 0.104697 0.918936 00:03 46 0.145804 0.104805 0.918936 00:03 47 0.141888 0.104281 0.918936 00:03 48 0.137711 0.103718 0.918936 00:03 49 0.135240 0.103554 0.918936 00:03 ### Showing training results model_type . show_results ( model , valid_ds , detection_threshold = 0.25 ) ![png](images/YOLOv5/YOLOv5_23_0.png) ## Inference pipeline From a dataset... preds = model_type . predict ( model , valid_ds , detection_threshold = 0.25 ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) ![png](images/YOLOv5/YOLOv5_25_0.png) ... and from a Dataloader infer_dl = model_type . infer_dl ( valid_ds , batch_size = 1 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , detection_threshold = 0.25 , keep_images = True ) show_preds ( preds = preds [: 6 ], denormalize_fn = denormalize_imagenet , ncols = 3 , ) 0%| | 0/26 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_27_1.png) ## Inspecting model predictions with `plot_top_losses` What are the images the model is having a hard time time with? #by = \"loss_total\" #by = \"loss_yolo\" by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_yolo\" : 0.5 , }, } sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = by ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_yolo']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m219\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] ![png](images/YOLOv5/YOLOv5_29_3.png) ## Training a PyTorch Lightning model class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 3e-4 ) backbone = model_type . backbones . medium ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = parser . class_map . num_classes , img_size = size ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 30 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5m.pt to /root/.icevision/yolo/yolov5m.pt... 0%| | 0.00/41.1M [00:00<?, ?B/s] GPU available: True, used: True TPU available: None, using: 0 TPU cores | Name | Type | Params -------------------------------- 0 | model | Model | 21.1 M -------------------------------- 21.1 M Trainable params 0 Non-trainable params 21.1 M Total params 84.290 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] 1 ## Happy Learning! If you need any assistance, feel free to join our [forum](https://discord.gg/JDBeZYK).","title":"Training a fastai Learner"},{"location":"about/","text":"Hall of Fame This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"About"},{"location":"about/#hall-of-fame","text":"This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"Hall of Fame"},{"location":"albumentations/","text":"Transforms Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases. Usage In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Albumentations"},{"location":"albumentations/#transforms","text":"Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases.","title":"Transforms"},{"location":"albumentations/#usage","text":"In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Usage"},{"location":"albumentations_tfms/","text":"[source] aug_tfms icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit_x = ( - 0.0625 , 0.0625 ), shift_limit_y = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 15 , 15 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 10 , 10 ), g_shift_limit = ( - 10 , 10 ), b_shift_limit = ( - 10 , 10 ) ), lighting = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (width, height). presize Optional[Union[int, Tuple[int, int]]] : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.geometric.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lighting Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source] Adapter icevision . tfms . albumentations . Adapter ( tfms ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Albumentations"},{"location":"albumentations_tfms/#aug_tfms","text":"icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit_x = ( - 0.0625 , 0.0625 ), shift_limit_y = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 15 , 15 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 10 , 10 ), g_shift_limit = ( - 10 , 10 ), b_shift_limit = ( - 10 , 10 ) ), lighting = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (width, height). presize Optional[Union[int, Tuple[int, int]]] : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.geometric.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lighting Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source]","title":"aug_tfms"},{"location":"albumentations_tfms/#adapter","text":"icevision . tfms . albumentations . Adapter ( tfms ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Adapter"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog . 0.12.0 The following PRs have been merged since the last version. dnth - Minor fix for clarity in mmdet utils.py (#1061) - Swin Transformer notebook (#1059) - Closes bug in #1057 (#1058) - CentripetalNet Support (#1050) - Add YOLACT support from mmdet (#1046) - Add more Swin backbones for RetinaNet and VFNet (#1042) - Fix Swin backbone issue with single stage models (#1039) - Add SABL (Side-Aware Boundary Localization for More Precise Object Detection) (#1038) - Add Feature Selective Anchor-Free Module for Single-Shot Object Detection (FSAF) (#1037) - Add Swin Transformer backbone to all mmdet models (#1035) - Support custom config for two stage detectors (#1034) - Add Swin Transformer backbone for Mask RCNN (#1033) - Add Deformable DETR model from mmdet (#1032) - Add YOLOF mmdet model (#1030) FraPochetti - removing fiftyone as a hard dependency (#1055) - fix progressive resizing nb + addings nbs to docs (#1023) - fixing plot_top_losses and semantic seg (#1019) - fixing the OCHuman notebook (#1008) - fixing wrong colab badge in SAHI notebook (#986) - SAHI inference integration (#984) ai-fast-track - Update FiftyOne NB and add it to documentation (#1052) - Added YOLOX backbones (#1010) - SSD update (#993) - Added training to the custom parser notebook (#989) - updated CHANGELOG.md to the last PR (#974) potipot - fix efficientdet metrics failing test (#1022) - Fix wandb_efficientdet notebook (#1021) - Fix opencv colab issue (#1020) - fixing the negative_samples notebook (#1013) - Update installation docs (#995) strickvl - Fix docstring typo (#1028) - Fix broken link (#1027) - Fix bullet list formatting error (#985) matt-deboer - Properly restore transformed masks after unload (#981) - fix for #978 (#980) - fix for #978: store transform results in detection.masks (#979) fstroth - Remove redundant model creation (#1036) - (Fix) Fixed the notebook and the draw records function as well a the \u2026 (#1018) hectorLop - refactor: Fixed the bug to add neck modules properly (#1029) - Implementation of DETR using mmdetection (#1026) 2649 - (feat) Added utils to convert Records and Predictions to fiftyone (#1031) fcakyon - compatibility for latest sahi updates (#1015) gablanouette - Fix checkpoint loading when model_name contains models or additional components (#1005) Anjum48 - Issue 987 - Add detection_threshold arg for all Lightning adapters (#1004) Thank you to all contributers: @dnth, @FraPochetti, @ai-fast-track, @potipot, @strickvl, @matt-deboer, @fstroth, @hectorLop, @2649, @fcakyon, @gablanouette, @Anjum48 0.11.0 The following PRs have been merged since the last version. ai-fast-track - Updating mmcv installation to torch 1.10.0 (#972) - Upgrade to torch 1.10 and torchvision 0.11 (#970) - Pass both map_location, and logger to downstream methods (#968) - Bumped torch and torchision versions (#961) - Update CHANGELOG.md for Release 0.11.0 (#959) - Adding an installation script for cuda and cpu (#956) - fixed yaml issue in doc generation CI/CD (#952) - Upgrade mk-docs-build.yml in the CI/CD (#951) - Update mmcv to 1.3.14 and mmdet to 2.17.0 in CI/CD (#949) - Update notebooks installation (#940) - Fix Colab script (#938) - Fixed Colab installation script (#937) - Update installation to torch 1.9 and dependencies (#935) - Inference - automatically recreate model trained with COCO (#929) - Simplify save and load model checkpoints (#924) - Update installation to torch 1.9 + dependencies (#919) - Added MMDetection VFNet Support. (#906) - Make MMDetection config object accessible to users (#904) - Adding progressive resizing support (#902) - Fix mmdet weights path issue (#900) - add docker-compose instructions (#898) - Added script for icevision inference installation (#893) - Added kwargs and label_border_color to end2end_detect() (#891) - Fix icevision installation in Colab (#887) - added kwargs to the EfficientDet model() method (#883) fstroth - (WIP) Fix masks for instance segmentation (#967) - (Refactor) Removed the coco function. (#964) - (Feature) init coco and via parser with a dict instead of the filepath (#963) - (Feature) Added way to output metrics for pytorchlightning during training (#960) - Fix for CHANGLOG.md update script. (#958) - Script for automatically updating CHANGELOG.md (#957) - (Update) Updated code to run with albumentations version 1.0.3. (#927) - Radiographic images (#912) potipot - Fix show pred (#930) - Fix inference on rectangular efficientdet input (#910) FraPochetti - adding docker support (#895) - Colab Install Script: fixing link to icevision master (#888) jaeeolma - Empty mask fix (#933) bogdan-evtushenko - Add support for yolox from mmdetection. (#932) drscotthawley - casting both caption parts as str (#922) lgvaz - Unet3 (#907) nicjac - Fixed PIL size bug in ImageRecordComponent (#889) (#894) Thank you to all contributers: @ai-fast-track, @fstroth, @potipot, @FraPochetti, @jaeeolma, @bogdan-evtushenko, @drscotthawley, @lgvaz, @nicjac Unreleased - 0.10.0a1 Main dependencies updated torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8 Unreleased - 0.9.0a1 Added Low level parsing workflow with RecordCollection Semantic segmentation support with fastai Changed Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore. [0.8.1] Added end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers. Changed Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights. [0.8.0] Supports pytorch 1.8 Added iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models Changed Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4) [0.7.0] BREAKING: API Refactor Added Metrics for mmdetection models Changed Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl [0.6.0b1] Added mmdetection models Changed Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record Deleted [0.5.2] Added aggregate_records_objects function Changed Added label_field to VIA parser to allow for alternate region_attribute names [0.5.0] Added Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample Changed improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable [0.4.0] Added retinanet: model, dataloaders, predict, ... Changed Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models [0.3.0] Added pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing Changed show_preds visual improvement [0.2.2] Added Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time Changed Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles [0.2.1] Changed updated matplotlib and ipykernel minimum version for colab compatibility [0.2.0] IMPORTANT Switched from poetry to setuptools Added Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data Changed Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height Deleted Removed CombinedParser , all parsing can be done with the standard Parser [0.1.6] Added Efficientdet now support empty annotations Changed Returns float instead of dict on FastaiMetricAdapter [0.1.5] Changed Updates fastai2 to the final release version [0.1.4] Added soft import icedata in icevision.all show_pbar parameter to COCOMetric Changed Deleted [0.1.3] Changed Effdet as direct dependency [0.1.2] Added show_results function for each model Changed Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision Deleted Removed datasets module to instead use the new icedata package [0.0.9] Added batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds Changed Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids [0.0.0-pre-release] Added CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class Changed All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map Removed MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule Links","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog .","title":"Changelog"},{"location":"changelog/#0120","text":"The following PRs have been merged since the last version. dnth - Minor fix for clarity in mmdet utils.py (#1061) - Swin Transformer notebook (#1059) - Closes bug in #1057 (#1058) - CentripetalNet Support (#1050) - Add YOLACT support from mmdet (#1046) - Add more Swin backbones for RetinaNet and VFNet (#1042) - Fix Swin backbone issue with single stage models (#1039) - Add SABL (Side-Aware Boundary Localization for More Precise Object Detection) (#1038) - Add Feature Selective Anchor-Free Module for Single-Shot Object Detection (FSAF) (#1037) - Add Swin Transformer backbone to all mmdet models (#1035) - Support custom config for two stage detectors (#1034) - Add Swin Transformer backbone for Mask RCNN (#1033) - Add Deformable DETR model from mmdet (#1032) - Add YOLOF mmdet model (#1030) FraPochetti - removing fiftyone as a hard dependency (#1055) - fix progressive resizing nb + addings nbs to docs (#1023) - fixing plot_top_losses and semantic seg (#1019) - fixing the OCHuman notebook (#1008) - fixing wrong colab badge in SAHI notebook (#986) - SAHI inference integration (#984) ai-fast-track - Update FiftyOne NB and add it to documentation (#1052) - Added YOLOX backbones (#1010) - SSD update (#993) - Added training to the custom parser notebook (#989) - updated CHANGELOG.md to the last PR (#974) potipot - fix efficientdet metrics failing test (#1022) - Fix wandb_efficientdet notebook (#1021) - Fix opencv colab issue (#1020) - fixing the negative_samples notebook (#1013) - Update installation docs (#995) strickvl - Fix docstring typo (#1028) - Fix broken link (#1027) - Fix bullet list formatting error (#985) matt-deboer - Properly restore transformed masks after unload (#981) - fix for #978 (#980) - fix for #978: store transform results in detection.masks (#979) fstroth - Remove redundant model creation (#1036) - (Fix) Fixed the notebook and the draw records function as well a the \u2026 (#1018) hectorLop - refactor: Fixed the bug to add neck modules properly (#1029) - Implementation of DETR using mmdetection (#1026) 2649 - (feat) Added utils to convert Records and Predictions to fiftyone (#1031) fcakyon - compatibility for latest sahi updates (#1015) gablanouette - Fix checkpoint loading when model_name contains models or additional components (#1005) Anjum48 - Issue 987 - Add detection_threshold arg for all Lightning adapters (#1004) Thank you to all contributers: @dnth, @FraPochetti, @ai-fast-track, @potipot, @strickvl, @matt-deboer, @fstroth, @hectorLop, @2649, @fcakyon, @gablanouette, @Anjum48","title":"0.12.0"},{"location":"changelog/#0110","text":"The following PRs have been merged since the last version. ai-fast-track - Updating mmcv installation to torch 1.10.0 (#972) - Upgrade to torch 1.10 and torchvision 0.11 (#970) - Pass both map_location, and logger to downstream methods (#968) - Bumped torch and torchision versions (#961) - Update CHANGELOG.md for Release 0.11.0 (#959) - Adding an installation script for cuda and cpu (#956) - fixed yaml issue in doc generation CI/CD (#952) - Upgrade mk-docs-build.yml in the CI/CD (#951) - Update mmcv to 1.3.14 and mmdet to 2.17.0 in CI/CD (#949) - Update notebooks installation (#940) - Fix Colab script (#938) - Fixed Colab installation script (#937) - Update installation to torch 1.9 and dependencies (#935) - Inference - automatically recreate model trained with COCO (#929) - Simplify save and load model checkpoints (#924) - Update installation to torch 1.9 + dependencies (#919) - Added MMDetection VFNet Support. (#906) - Make MMDetection config object accessible to users (#904) - Adding progressive resizing support (#902) - Fix mmdet weights path issue (#900) - add docker-compose instructions (#898) - Added script for icevision inference installation (#893) - Added kwargs and label_border_color to end2end_detect() (#891) - Fix icevision installation in Colab (#887) - added kwargs to the EfficientDet model() method (#883) fstroth - (WIP) Fix masks for instance segmentation (#967) - (Refactor) Removed the coco function. (#964) - (Feature) init coco and via parser with a dict instead of the filepath (#963) - (Feature) Added way to output metrics for pytorchlightning during training (#960) - Fix for CHANGLOG.md update script. (#958) - Script for automatically updating CHANGELOG.md (#957) - (Update) Updated code to run with albumentations version 1.0.3. (#927) - Radiographic images (#912) potipot - Fix show pred (#930) - Fix inference on rectangular efficientdet input (#910) FraPochetti - adding docker support (#895) - Colab Install Script: fixing link to icevision master (#888) jaeeolma - Empty mask fix (#933) bogdan-evtushenko - Add support for yolox from mmdetection. (#932) drscotthawley - casting both caption parts as str (#922) lgvaz - Unet3 (#907) nicjac - Fixed PIL size bug in ImageRecordComponent (#889) (#894) Thank you to all contributers: @ai-fast-track, @fstroth, @potipot, @FraPochetti, @jaeeolma, @bogdan-evtushenko, @drscotthawley, @lgvaz, @nicjac","title":"0.11.0"},{"location":"changelog/#unreleased-0100a1","text":"","title":"Unreleased - 0.10.0a1"},{"location":"changelog/#main-dependencies-updated","text":"torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8","title":"Main dependencies updated"},{"location":"changelog/#unreleased-090a1","text":"","title":"Unreleased - 0.9.0a1"},{"location":"changelog/#added","text":"Low level parsing workflow with RecordCollection Semantic segmentation support with fastai","title":"Added"},{"location":"changelog/#changed","text":"Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore.","title":"Changed"},{"location":"changelog/#081","text":"","title":"[0.8.1]"},{"location":"changelog/#added_1","text":"end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers.","title":"Added"},{"location":"changelog/#changed_1","text":"Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights.","title":"Changed"},{"location":"changelog/#080","text":"Supports pytorch 1.8","title":"[0.8.0]"},{"location":"changelog/#added_2","text":"iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models","title":"Added"},{"location":"changelog/#changed_2","text":"Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4)","title":"Changed"},{"location":"changelog/#070","text":"BREAKING: API Refactor","title":"[0.7.0]"},{"location":"changelog/#added_3","text":"Metrics for mmdetection models","title":"Added"},{"location":"changelog/#changed_3","text":"Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl","title":"Changed"},{"location":"changelog/#060b1","text":"","title":"[0.6.0b1]"},{"location":"changelog/#added_4","text":"mmdetection models","title":"Added"},{"location":"changelog/#changed_4","text":"Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record","title":"Changed"},{"location":"changelog/#deleted","text":"","title":"Deleted"},{"location":"changelog/#052","text":"","title":"[0.5.2]"},{"location":"changelog/#added_5","text":"aggregate_records_objects function","title":"Added"},{"location":"changelog/#changed_5","text":"Added label_field to VIA parser to allow for alternate region_attribute names","title":"Changed"},{"location":"changelog/#050","text":"","title":"[0.5.0]"},{"location":"changelog/#added_6","text":"Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample","title":"Added"},{"location":"changelog/#changed_6","text":"improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable","title":"Changed"},{"location":"changelog/#040","text":"","title":"[0.4.0]"},{"location":"changelog/#added_7","text":"retinanet: model, dataloaders, predict, ...","title":"Added"},{"location":"changelog/#changed_7","text":"Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models","title":"Changed"},{"location":"changelog/#030","text":"","title":"[0.3.0]"},{"location":"changelog/#added_8","text":"pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing","title":"Added"},{"location":"changelog/#changed_8","text":"show_preds visual improvement","title":"Changed"},{"location":"changelog/#022","text":"","title":"[0.2.2]"},{"location":"changelog/#added_9","text":"Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time","title":"Added"},{"location":"changelog/#changed_9","text":"Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles","title":"Changed"},{"location":"changelog/#021","text":"","title":"[0.2.1]"},{"location":"changelog/#changed_10","text":"updated matplotlib and ipykernel minimum version for colab compatibility","title":"Changed"},{"location":"changelog/#020","text":"","title":"[0.2.0]"},{"location":"changelog/#important","text":"Switched from poetry to setuptools","title":"IMPORTANT"},{"location":"changelog/#added_10","text":"Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data","title":"Added"},{"location":"changelog/#changed_11","text":"Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height","title":"Changed"},{"location":"changelog/#deleted_1","text":"Removed CombinedParser , all parsing can be done with the standard Parser","title":"Deleted"},{"location":"changelog/#016","text":"","title":"[0.1.6]"},{"location":"changelog/#added_11","text":"Efficientdet now support empty annotations","title":"Added"},{"location":"changelog/#changed_12","text":"Returns float instead of dict on FastaiMetricAdapter","title":"Changed"},{"location":"changelog/#015","text":"","title":"[0.1.5]"},{"location":"changelog/#changed_13","text":"Updates fastai2 to the final release version","title":"Changed"},{"location":"changelog/#014","text":"","title":"[0.1.4]"},{"location":"changelog/#added_12","text":"soft import icedata in icevision.all show_pbar parameter to COCOMetric","title":"Added"},{"location":"changelog/#changed_14","text":"","title":"Changed"},{"location":"changelog/#deleted_2","text":"","title":"Deleted"},{"location":"changelog/#013","text":"","title":"[0.1.3]"},{"location":"changelog/#changed_15","text":"Effdet as direct dependency","title":"Changed"},{"location":"changelog/#012","text":"","title":"[0.1.2]"},{"location":"changelog/#added_13","text":"show_results function for each model","title":"Added"},{"location":"changelog/#changed_16","text":"Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision","title":"Changed"},{"location":"changelog/#deleted_3","text":"Removed datasets module to instead use the new icedata package","title":"Deleted"},{"location":"changelog/#009","text":"","title":"[0.0.9]"},{"location":"changelog/#added_14","text":"batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds","title":"Added"},{"location":"changelog/#changed_17","text":"Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids","title":"Changed"},{"location":"changelog/#000-pre-release","text":"","title":"[0.0.0-pre-release]"},{"location":"changelog/#added_15","text":"CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class","title":"Added"},{"location":"changelog/#changed_18","text":"All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map","title":"Changed"},{"location":"changelog/#removed","text":"MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule","title":"Removed"},{"location":"changelog/#links","text":"","title":"Links"},{"location":"changelog_backup/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog . 0.11.0 The following PRs have been merged since the last version. ai-fast-track - Updating mmcv installation to torch 1.10.0 (#972) - Upgrade to torch 1.10 and torchvision 0.11 (#970) - Pass both map_location, and logger to downstream methods (#968) - Bumped torch and torchision versions (#961) - Update CHANGELOG.md for Release 0.11.0 (#959) - Adding an installation script for cuda and cpu (#956) - fixed yaml issue in doc generation CI/CD (#952) - Upgrade mk-docs-build.yml in the CI/CD (#951) - Update mmcv to 1.3.14 and mmdet to 2.17.0 in CI/CD (#949) - Update notebooks installation (#940) - Fix Colab script (#938) - Fixed Colab installation script (#937) - Update installation to torch 1.9 and dependencies (#935) - Inference - automatically recreate model trained with COCO (#929) - Simplify save and load model checkpoints (#924) - Update installation to torch 1.9 + dependencies (#919) - Added MMDetection VFNet Support. (#906) - Make MMDetection config object accessible to users (#904) - Adding progressive resizing support (#902) - Fix mmdet weights path issue (#900) - add docker-compose instructions (#898) - Added script for icevision inference installation (#893) - Added kwargs and label_border_color to end2end_detect() (#891) - Fix icevision installation in Colab (#887) - added kwargs to the EfficientDet model() method (#883) fstroth - (WIP) Fix masks for instance segmentation (#967) - (Refactor) Removed the coco function. (#964) - (Feature) init coco and via parser with a dict instead of the filepath (#963) - (Feature) Added way to output metrics for pytorchlightning during training (#960) - Fix for CHANGLOG.md update script. (#958) - Script for automatically updating CHANGELOG.md (#957) - (Update) Updated code to run with albumentations version 1.0.3. (#927) - Radiographic images (#912) potipot - Fix show pred (#930) - Fix inference on rectangular efficientdet input (#910) FraPochetti - adding docker support (#895) - Colab Install Script: fixing link to icevision master (#888) jaeeolma - Empty mask fix (#933) bogdan-evtushenko - Add support for yolox from mmdetection. (#932) drscotthawley - casting both caption parts as str (#922) lgvaz - Unet3 (#907) nicjac - Fixed PIL size bug in ImageRecordComponent (#889) (#894) Thank you to all contributers: @ai-fast-track, @fstroth, @potipot, @FraPochetti, @jaeeolma, @bogdan-evtushenko, @drscotthawley, @lgvaz, @nicjac Unreleased - 0.10.0a1 Main dependencies updated torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8 Unreleased - 0.9.0a1 Added Low level parsing workflow with RecordCollection Semantic segmentation support with fastai Changed Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore. [0.8.1] Added end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers. Changed Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights. [0.8.0] Supports pytorch 1.8 Added iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models Changed Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4) [0.7.0] BREAKING: API Refactor Added Metrics for mmdetection models Changed Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl [0.6.0b1] Added mmdetection models Changed Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record Deleted [0.5.2] Added aggregate_records_objects function Changed Added label_field to VIA parser to allow for alternate region_attribute names [0.5.0] Added Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample Changed improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable [0.4.0] Added retinanet: model, dataloaders, predict, ... Changed Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models [0.3.0] Added pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing Changed show_preds visual improvement [0.2.2] Added Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time Changed Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles [0.2.1] Changed updated matplotlib and ipykernel minimum version for colab compatibility [0.2.0] IMPORTANT Switched from poetry to setuptools Added Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data Changed Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height Deleted Removed CombinedParser , all parsing can be done with the standard Parser [0.1.6] Added Efficientdet now support empty annotations Changed Returns float instead of dict on FastaiMetricAdapter [0.1.5] Changed Updates fastai2 to the final release version [0.1.4] Added soft import icedata in icevision.all show_pbar parameter to COCOMetric Changed Deleted [0.1.3] Changed Effdet as direct dependency [0.1.2] Added show_results function for each model Changed Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision Deleted Removed datasets module to instead use the new icedata package [0.0.9] Added batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds Changed Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids [0.0.0-pre-release] Added CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class Changed All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map Removed MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule Links","title":"Changelog"},{"location":"changelog_backup/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog .","title":"Changelog"},{"location":"changelog_backup/#0110","text":"The following PRs have been merged since the last version. ai-fast-track - Updating mmcv installation to torch 1.10.0 (#972) - Upgrade to torch 1.10 and torchvision 0.11 (#970) - Pass both map_location, and logger to downstream methods (#968) - Bumped torch and torchision versions (#961) - Update CHANGELOG.md for Release 0.11.0 (#959) - Adding an installation script for cuda and cpu (#956) - fixed yaml issue in doc generation CI/CD (#952) - Upgrade mk-docs-build.yml in the CI/CD (#951) - Update mmcv to 1.3.14 and mmdet to 2.17.0 in CI/CD (#949) - Update notebooks installation (#940) - Fix Colab script (#938) - Fixed Colab installation script (#937) - Update installation to torch 1.9 and dependencies (#935) - Inference - automatically recreate model trained with COCO (#929) - Simplify save and load model checkpoints (#924) - Update installation to torch 1.9 + dependencies (#919) - Added MMDetection VFNet Support. (#906) - Make MMDetection config object accessible to users (#904) - Adding progressive resizing support (#902) - Fix mmdet weights path issue (#900) - add docker-compose instructions (#898) - Added script for icevision inference installation (#893) - Added kwargs and label_border_color to end2end_detect() (#891) - Fix icevision installation in Colab (#887) - added kwargs to the EfficientDet model() method (#883) fstroth - (WIP) Fix masks for instance segmentation (#967) - (Refactor) Removed the coco function. (#964) - (Feature) init coco and via parser with a dict instead of the filepath (#963) - (Feature) Added way to output metrics for pytorchlightning during training (#960) - Fix for CHANGLOG.md update script. (#958) - Script for automatically updating CHANGELOG.md (#957) - (Update) Updated code to run with albumentations version 1.0.3. (#927) - Radiographic images (#912) potipot - Fix show pred (#930) - Fix inference on rectangular efficientdet input (#910) FraPochetti - adding docker support (#895) - Colab Install Script: fixing link to icevision master (#888) jaeeolma - Empty mask fix (#933) bogdan-evtushenko - Add support for yolox from mmdetection. (#932) drscotthawley - casting both caption parts as str (#922) lgvaz - Unet3 (#907) nicjac - Fixed PIL size bug in ImageRecordComponent (#889) (#894) Thank you to all contributers: @ai-fast-track, @fstroth, @potipot, @FraPochetti, @jaeeolma, @bogdan-evtushenko, @drscotthawley, @lgvaz, @nicjac","title":"0.11.0"},{"location":"changelog_backup/#unreleased-0100a1","text":"","title":"Unreleased - 0.10.0a1"},{"location":"changelog_backup/#main-dependencies-updated","text":"torch 1.9.0 tochvision 0.10 mmdet 2.16.0 mmcv 1.3.14 fastai 2.5.2 pytorch-lightning 1.4.8","title":"Main dependencies updated"},{"location":"changelog_backup/#unreleased-090a1","text":"","title":"Unreleased - 0.9.0a1"},{"location":"changelog_backup/#added","text":"Low level parsing workflow with RecordCollection Semantic segmentation support with fastai","title":"Added"},{"location":"changelog_backup/#changed","text":"Breaking: Refactored mask components workflow Breaking: Due to the new mask components refactor, autofix doesn't work for mask components anymore.","title":"Changed"},{"location":"changelog_backup/#081","text":"","title":"[0.8.1]"},{"location":"changelog_backup/#added_1","text":"end2end_detect() : Run Object Detection inference (only bboxes ) on a single image, and return predicted boxes corresponding to original image size - Breaking: BaseLabelsRecordComponent as_dict() now returns both labels and labels_ids . labels are now strings instead of integers.","title":"Added"},{"location":"changelog_backup/#changed_1","text":"Breaking: On tfms.A.aug_tfms parameter size and presize changed from order (height, width) to (width, height) Added RecordCollection Breaking: Changed how the resnet (not-fpn) backbone cut is done for torchvision models. Previous resnet torchvision trained models will have trouble loading weights.","title":"Changed"},{"location":"changelog_backup/#080","text":"Supports pytorch 1.8","title":"[0.8.0]"},{"location":"changelog_backup/#added_2","text":"iou_thresholds parameter to COCOMetric SimpleConfusionMatrix Metric Negative samples support for mmdetection object detection models","title":"Added"},{"location":"changelog_backup/#changed_2","text":"Breaking: Albumentations aug_tfms defaults. rotate_limit changed from 45 to 15 rgb_shift_limit changed from 20 to 10 VOC parser uses image sizes from annotation file instead of image bumps fastai to latest version (<2.4)","title":"Changed"},{"location":"changelog_backup/#070","text":"BREAKING: API Refactor","title":"[0.7.0]"},{"location":"changelog_backup/#added_3","text":"Metrics for mmdetection models","title":"Added"},{"location":"changelog_backup/#changed_3","text":"Breaking: Renamed tasks default,detect,classif to common,detection,classification Breaking: Renamed imageid to record_id Breaking: Added parameter is_new to Parser.parse_fields Removed all dependencies on cv2 for visualisation Use new composite API for visualisation - covers user defined task names & multiple tasks Added a ton of visualisation goodies to icevision.visualize.draw_data.draw_sample - user can now use custom fonts control mask thickness control mask blending prettify labels -- show confidence score & capitalise label plot specific and/or exclude specific labels pass in a dictionary mapping labels to specific colors control label height & width padding from bbox edge add border around label for legibility (color is a parameter) Breaking: : Rename labels->label_ids , labels_names->labels in LabelsRecordComponent - Renamed torchvision resnet backbones: - resnet_fpn.resnet18 -> resnet18_fpn - resnest_fpn.resnest18 -> resnest18_fpn Breaking: Added parameters sample and keep_image to convert_raw_prediction Breaking: Renamed VocXmlParser to VOCBBoxParser and VocMaskParser to VOCMaskParser Breaking: Renamed predict_dl to predict_from_dl","title":"Changed"},{"location":"changelog_backup/#060b1","text":"","title":"[0.6.0b1]"},{"location":"changelog_backup/#added_4","text":"mmdetection models","title":"Added"},{"location":"changelog_backup/#changed_4","text":"Breaking: All Parser subclasses need to call super.__init__ Breaking: LabelsMixin.labels now needs to return List[Hashable] instead of List[int] (labels names instead of label ids) Breaking: Model namespace changes e.g. faster_rcnn -> models.torchvision.faster_rcnn , efficientdet -> models.ross.efficientdet Breaking: Renamed ClassMap.get_name/get_id to ClassMap.get_by_name/get_by_id Breaking: Removes idmap argument from Parser.parse . Instead pass idmap to the constructor ( __init__ ). ClassMap is not created inside of the parser, it's not required to instantiate it before class_map labels get automatically filled while parsing background for class_map is now always 0 (unless no background) adds class_map to Record","title":"Changed"},{"location":"changelog_backup/#deleted","text":"","title":"Deleted"},{"location":"changelog_backup/#052","text":"","title":"[0.5.2]"},{"location":"changelog_backup/#added_5","text":"aggregate_records_objects function","title":"Added"},{"location":"changelog_backup/#changed_5","text":"Added label_field to VIA parser to allow for alternate region_attribute names","title":"Changed"},{"location":"changelog_backup/#050","text":"","title":"[0.5.0]"},{"location":"changelog_backup/#added_6","text":"Keypoints full support: data API, model and training VGG Image Annotator v2 JSON format parser for bboxes figsize parameter to show_record and show_sample","title":"Added"},{"location":"changelog_backup/#changed_6","text":"improved visualisation for small bboxes COCOMetric now returns all metrics from pycocotools makes torchvision models torchscriptable","title":"Changed"},{"location":"changelog_backup/#040","text":"","title":"[0.4.0]"},{"location":"changelog_backup/#added_7","text":"retinanet: model, dataloaders, predict, ...","title":"Added"},{"location":"changelog_backup/#changed_7","text":"Breaking: models/rcnn renamed to models/torchvision_models tests/models/rcnn renamed to tests/models/torchvision_models","title":"Changed"},{"location":"changelog_backup/#030","text":"","title":"[0.3.0]"},{"location":"changelog_backup/#added_8","text":"pytorch 1.7 support, all dependencies updated tutorial with hard negative samples ability to skip record while parsing","title":"Added"},{"location":"changelog_backup/#changed_8","text":"show_preds visual improvement","title":"Changed"},{"location":"changelog_backup/#022","text":"","title":"[0.2.2]"},{"location":"changelog_backup/#added_9","text":"Cache records after parsing with the new parameter cache_filepath added to Parser.parse (#504) Added pretrained: bool = True argument to both faster_rcnn and mask_rcnn model() methods. (#516) new class EncodedRLEs all masks get converted to EncodedRLEs at parsing time","title":"Added"},{"location":"changelog_backup/#changed_9","text":"Removed warning on autofixing masks RLE default counts is now COCO style renamed Mask.to_erle to Mask.to_erles","title":"Changed"},{"location":"changelog_backup/#021","text":"","title":"[0.2.1]"},{"location":"changelog_backup/#changed_10","text":"updated matplotlib and ipykernel minimum version for colab compatibility","title":"Changed"},{"location":"changelog_backup/#020","text":"","title":"[0.2.0]"},{"location":"changelog_backup/#important","text":"Switched from poetry to setuptools","title":"IMPORTANT"},{"location":"changelog_backup/#added_10","text":"Function wandb_img_preds to help logging bboxes to wandb wandb as a soft dependency Template code for parsers.SizeMixin if parsers.FilepathMixin is used Get image size without opening image with get_image_size Ability to skip record while parsing with AbortParseRecord Autofix for record: autofix_records function and autofix:bool parameter added to Parser.parse Record class and mixins, create_mixed_record function to help creating Records InvalidDataError for BBox Catches InvalidDataError while parsing data","title":"Added"},{"location":"changelog_backup/#changed_11","text":"Breaking: Unified parsers.SizeMixin functions image_width and image_height into a single function image_width_height Rename Parser SizeMixin fields from width height to image_width image_height","title":"Changed"},{"location":"changelog_backup/#deleted_1","text":"Removed CombinedParser , all parsing can be done with the standard Parser","title":"Deleted"},{"location":"changelog_backup/#016","text":"","title":"[0.1.6]"},{"location":"changelog_backup/#added_11","text":"Efficientdet now support empty annotations","title":"Added"},{"location":"changelog_backup/#changed_12","text":"Returns float instead of dict on FastaiMetricAdapter","title":"Changed"},{"location":"changelog_backup/#015","text":"","title":"[0.1.5]"},{"location":"changelog_backup/#changed_13","text":"Updates fastai2 to the final release version","title":"Changed"},{"location":"changelog_backup/#014","text":"","title":"[0.1.4]"},{"location":"changelog_backup/#added_12","text":"soft import icedata in icevision.all show_pbar parameter to COCOMetric","title":"Added"},{"location":"changelog_backup/#changed_14","text":"","title":"Changed"},{"location":"changelog_backup/#deleted_2","text":"","title":"Deleted"},{"location":"changelog_backup/#013","text":"","title":"[0.1.3]"},{"location":"changelog_backup/#changed_15","text":"Effdet as direct dependency","title":"Changed"},{"location":"changelog_backup/#012","text":"","title":"[0.1.2]"},{"location":"changelog_backup/#added_13","text":"show_results function for each model","title":"Added"},{"location":"changelog_backup/#changed_16","text":"Default data_splitter for Parser changed to RandomSplitter Renamed package from mantisshrimp to icevision","title":"Changed"},{"location":"changelog_backup/#deleted_3","text":"Removed datasets module to instead use the new icedata package","title":"Deleted"},{"location":"changelog_backup/#009","text":"","title":"[0.0.9]"},{"location":"changelog_backup/#added_14","text":"batch, samples = <model_name>.build_infer_batch(dataset) preds = <model_name>.predict(model, batch) infer_dl = <model_name>.infer_dataloader(dataset) samples, preds = predict_dl(model, infer_dl) Dataset.from_images Contructs a Dataset from a list of images (numpy arrays) tfms.A.aug_tfms for easy access to common augmentation transforms with albumentations tfms.A.resize_and_pad , useful as a validation transform **predict_kwargs to predict_dl signature from mantisshrimp.all import * to import internal modules and external imports show parameter to show_img download_gdrive and download_and_extract_gdrive New datasets pennfundan and birds","title":"Added"},{"location":"changelog_backup/#changed_17","text":"Renames AlbuTransform to AlbumentationTransforms All build_batch method now returns batch, samples , the batch is always a tuple of inputs to the model batch_tfms moved to tfms.batch AlbumentationTransforms moved to tfms.A.Adapter All parsers function were moved to their own namespace parsers instead of being on the global namespace so, for example, instead of Parser now we have to do parsers.Parser Removed Parser word from Mixins, e.g. ImageidParserMixin -> parsers.ImageidMixin Removed Parser word from parser default bundle, e.g. FasterRCNNParser -> parsers.FasterRCNN COCO and VOC parsers moved from datasets to parsers DataSplitter s moved from parsers/splits.py to utils/data_splitter.py Renames *_dataloader to *_dl , e.g. mask_rcnn.train_dataloader to mask_rcnn.train_dl Moves RecordType from parsers to core Refactors IDMap , adds methods get_name and get_id Moves IDMap from utils to data DataSplitter.split now receives idmap instead of ids","title":"Changed"},{"location":"changelog_backup/#000-pre-release","text":"","title":"[0.0.0-pre-release]"},{"location":"changelog_backup/#added_15","text":"CaptureStdout for capturing writes to stdout (print), e.g. from COCOMetric mantisshrimp.models.<model_name>.convert_raw_predictions to convert raw preds (tensors output from the model) to library standard dict COCOMetricType for selecting what metric type to use ( bbox , mask , keypoints ) COCOMetric fixed sort parameter for get_image_files ClassMap : A class that handles the mapping between ids and names, with the optional insertion of the background class","title":"Added"},{"location":"changelog_backup/#changed_18","text":"All dataloaders now return the batch and the records, e.g. return (images, targets), records Metric.accumulate signature changed to (records, preds) , reflects in FastaiMetricAdapter and LightningModelAdapter datasets.<name>.CLASSES substituted by a function datasets.<name>.class_map that returns a ClassMap datasets.voc.VocXmlParser , show methods: parameter classes: Sequence[str] substituted by class_map: ClassMap datasets.fridge.parser , datasets.pets.parser : additional required parameter class_map","title":"Changed"},{"location":"changelog_backup/#removed","text":"MantisFasterRCNN , MantisMaskRCNN MantisEfficientDet CategoryMap , Category MantisModule","title":"Removed"},{"location":"changelog_backup/#links","text":"","title":"Links"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contribution Guide We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps Step 1: Forking and Installing IceVision \u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, locally install the package: pip install -e \".[all,dev]\" Then, set up pre-commit hooks using: pre-commit install Step 2: Stay in Sync with the original (upstream) repo Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master Step 3: Creating a new branch git checkout -b feature-name git branch master * feature_name: Step 4: Make changes, and commit your file changes Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files cd to/icevision/folder black . git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary. Step 5: Submitting a Pull Request 1. Create a pull request git Upload your local branch to your remote GitHub repo (github.com/username/icevision) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request 2. Confirm PR was created: Ensure your PR is listed here 3. Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before> Reviewing Your PR Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well. Resolving Conflicts In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts. Feature Requests and questions For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Contributing Guide"},{"location":"contributing/#contribution-guide","text":"We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps","title":"Contribution Guide"},{"location":"contributing/#step-1-forking-and-installing-icevision","text":"\u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, locally install the package: pip install -e \".[all,dev]\" Then, set up pre-commit hooks using: pre-commit install","title":"Step 1: Forking and Installing IceVision"},{"location":"contributing/#step-2-stay-in-sync-with-the-original-upstream-repo","text":"Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master","title":"Step 2: Stay in Sync with the original (upstream) repo"},{"location":"contributing/#step-3-creating-a-new-branch","text":"git checkout -b feature-name git branch master * feature_name:","title":"Step 3: Creating a new branch"},{"location":"contributing/#step-4-make-changes-and-commit-your-file-changes","text":"Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files cd to/icevision/folder black . git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary.","title":"Step 4: Make changes, and commit your file changes"},{"location":"contributing/#step-5-submitting-a-pull-request","text":"","title":"Step 5: Submitting a Pull Request"},{"location":"contributing/#1-create-a-pull-request-git","text":"Upload your local branch to your remote GitHub repo (github.com/username/icevision) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request","title":"1. Create a pull request git"},{"location":"contributing/#2-confirm-pr-was-created","text":"Ensure your PR is listed here","title":"2. Confirm PR was created:"},{"location":"contributing/#3-updating-a-pr","text":"Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before>","title":"3.  Updating a PR:"},{"location":"contributing/#reviewing-your-pr","text":"Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well.","title":"Reviewing Your PR"},{"location":"contributing/#resolving-conflicts","text":"In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts.","title":"Resolving Conflicts"},{"location":"contributing/#feature-requests-and-questions","text":"For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Feature Requests and questions"},{"location":"custom_parser/","text":"Custom Parser - Simple Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions Install from pypi... # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 ... or from icevision master # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'} Imports As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd Download dataset We're going to be using a small sample of the chess dataset, the full dataset is offered by roboflow here data_url = \"https://github.com/airctic/chess_sample/archive/master.zip\" data_dir = icedata . load_data ( data_url , 'chess_sample' ) / 'chess_sample-master' Understand the data format In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. df = pd . read_csv ( data_dir / \"annotations.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> filename width height label xmin ymin xmax ymax 0 0.jpg 416 416 black-bishop 280 227 310 284 1 0.jpg 416 416 black-king 311 110 345 195 2 0.jpg 416 416 black-queen 237 85 262 159 3 0.jpg 416 416 black-rook 331 277 366 333 4 0.jpg 416 416 black-rook 235 3 255 51 At first glance, we can make the following assumptions: Multiple rows with the same filename, width, height A label for each row A bbox [xmin, ymin, xmax, ymax] for each row Once we know what our data provides we can create our custom Parser . Create the Parser The first step is to create a template record for our specific type of dataset, in this case we're doing standard object detection: template_record = ObjectDetectionRecord () Now use the method generate_template that will print out all the necessary steps we have to implement. Parser . generate_template ( template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_filepath(<Union[str, Path]>) record.set_img_size(<ImgSize>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) record.detection.add_bboxes(<Sequence[BBox]>) We can copy the template and use it as our starting point. Let's go over each of the methods we have to define: __init__ : What happens here is completely up to you, normally we have to pass some reference to our data, data_dir in our case. __iter__ : This tells our parser how to iterate over our data, each item returned here will be passed to parse_fields as o . In our case we call df.itertuples to iterate over all df rows. __len__ : How many items will be iterating over. imageid : Should return a Hashable ( int , str , etc). In our case we want all the dataset items that have the same filename to be unified in the same record. parse_fields : Here is where the attributes of the record are collected, the template will suggest what methods we need to call on the record and what parameters it expects. The parameter o it receives is the item returned by __iter__ . Important Be sure to pass the correct type on all record methods! class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : for o in self . df . itertuples (): yield o def __len__ ( self ) -> int : return len ( self . df ) def record_id ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record , is_new ): if is_new : record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detection . set_class_map ( self . class_map ) record . detection . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detection . add_labels ([ o . label ]) Let's randomly split the data and parser with Parser.parse : parser = ChessParser ( template_record , data_dir ) train_records , valid_records = parser . parse () 0%| | 0/109 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7 [00:00<?, ?it/s] Let's take a look at one record: show_record ( train_records [ 0 ], display_label = False , figsize = ( 14 , 10 )) train_records [ 0 ] BaseRecord # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Filepath: /root/.icevision/data/chess_sample/chess_sample-master/images/6.jpg - Img: None - Image size ImgSize(width=416, height=416) - Record ID: 6.jpg detection: - Class Map: <ClassMap: {'background': 0, 'black-bishop': 1, 'black-king': 2, 'black-queen': 3, 'black-rook': 4, 'black-pawn': 5, 'black-knight': 6, 'white-queen': 7, 'white-rook': 8, 'white-king': 9, 'white-bishop': 10, 'white-knight': 11, 'white-pawn': 12}> - Labels: [8, 9, 12, 12, 12, 12, 2, 4, 3, 5, 5, 5] - BBoxes: [<BBox (xmin:170, ymin:79, xmax:195, ymax:132)>, <BBox (xmin:131, ymin:129, xmax:162, ymax:213)>, <BBox (xmin:167, ymin:119, xmax:189, ymax:165)>, <BBox (xmin:136, ymin:52, xmax:158, ymax:97)>, <BBox (xmin:101, ymin:15, xmax:123, ymax:59)>, <BBox (xmin:94, ymin:256, xmax:118, ymax:304)>, <BBox (xmin:257, ymin:268, xmax:291, ymax:354)>, <BBox (xmin:335, ymin:297, xmax:368, ymax:355)>, <BBox (xmin:234, ymin:97, xmax:260, ymax:173)>, <BBox (xmin:273, ymin:84, xmax:291, ymax:128)>, <BBox (xmin:204, ymin:118, xmax:223, ymax:162)>, <BBox (xmin:248, ymin:204, xmax:270, ymax:251)>] Models We've selected a few of the many options below. You can easily pick which libraries, models, and backbones you like to use. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } elif selection == 3 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 4 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 5 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) (<module 'icevision.models.mmdet.models.vfnet' from '/usr/local/lib/python3.7/dist-packages/icevision/models/mmdet/models/vfnet/__init__.py'>, <icevision.models.mmdet.models.vfnet.backbones.resnet_fpn.MMDetVFNETBackboneConfig at 0x7f3d3eefcf50>, {}) Training metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=3.0199516913853586e-05) learn . fine_tune ( 40 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.581739 1.825835 0.196081 00:01 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 1.581900 1.786651 0.216952 00:01 1 1.608743 1.763155 0.224355 00:01 2 1.631124 1.756258 0.224446 00:01 3 1.605482 1.744663 0.221207 00:01 4 1.601004 1.733463 0.225954 00:01 5 1.602483 1.737170 0.227496 00:01 6 1.585189 1.726894 0.230818 00:01 7 1.569807 1.705676 0.242893 00:01 8 1.568512 1.702688 0.239849 00:01 9 1.554865 1.681125 0.247037 00:01 10 1.555655 1.684015 0.297935 00:01 11 1.543609 1.681997 0.301196 00:01 12 1.534735 1.690655 0.310736 00:01 13 1.524568 1.689521 0.314591 00:01 14 1.516276 1.659921 0.330271 00:01 15 1.511769 1.629157 0.348593 00:01 16 1.502966 1.596284 0.362406 00:01 17 1.497759 1.586747 0.371476 00:01 18 1.497266 1.607103 0.369360 00:01 19 1.492118 1.642410 0.333980 00:01 20 1.489123 1.666914 0.333943 00:01 21 1.487922 1.678961 0.331371 00:01 22 1.483647 1.671102 0.349929 00:01 23 1.480486 1.642387 0.365551 00:01 24 1.472842 1.604754 0.377337 00:01 25 1.466778 1.569934 0.398319 00:01 26 1.457183 1.547147 0.410581 00:01 27 1.452692 1.534132 0.419814 00:01 28 1.447047 1.526892 0.423409 00:01 29 1.443192 1.525016 0.422199 00:01 30 1.444314 1.528092 0.420855 00:01 31 1.442283 1.533094 0.419651 00:01 32 1.434897 1.537819 0.417674 00:01 33 1.431285 1.542786 0.418381 00:01 34 1.424665 1.546462 0.418103 00:01 35 1.418521 1.548954 0.416256 00:01 36 1.414101 1.550630 0.416183 00:01 37 1.407963 1.551399 0.416183 00:01 38 1.402073 1.551711 0.416183 00:01 39 1.393795 1.551780 0.416183 00:01 Training using Pytorch Lightning # class LightModel(model_type.lightning.ModelAdapter): # def configure_optimizers(self): # return Adam(self.parameters(), lr=1e-4) # light_model = LightModel(model, metrics=metrics) # trainer = pl.Trainer(max_epochs=20, gpus=1) # trainer.fit(light_model, train_dl, valid_dl) Showing the results model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Batch Inference (Prediction) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/1 [00:00<?, ?it/s] Conclusion This notebook shows ho to create a custom parser to process data stored in format different than the popular COCO or VOV formats. The parsed data feed any IceVision models that could be trained by either Fastai or Pytorch-Lightning training loops. Happy Learning! If you need any assistance, feel free to join our forum .","title":"Custom Parser"},{"location":"custom_parser/#custom-parser-simple","text":"","title":"Custom Parser - Simple"},{"location":"custom_parser/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions Install from pypi... # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 ... or from icevision master # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'}","title":"Installing IceVision and IceData"},{"location":"custom_parser/#imports","text":"As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd","title":"Imports"},{"location":"custom_parser/#download-dataset","text":"We're going to be using a small sample of the chess dataset, the full dataset is offered by roboflow here data_url = \"https://github.com/airctic/chess_sample/archive/master.zip\" data_dir = icedata . load_data ( data_url , 'chess_sample' ) / 'chess_sample-master'","title":"Download dataset"},{"location":"custom_parser/#understand-the-data-format","text":"In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. df = pd . read_csv ( data_dir / \"annotations.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> filename width height label xmin ymin xmax ymax 0 0.jpg 416 416 black-bishop 280 227 310 284 1 0.jpg 416 416 black-king 311 110 345 195 2 0.jpg 416 416 black-queen 237 85 262 159 3 0.jpg 416 416 black-rook 331 277 366 333 4 0.jpg 416 416 black-rook 235 3 255 51 At first glance, we can make the following assumptions: Multiple rows with the same filename, width, height A label for each row A bbox [xmin, ymin, xmax, ymax] for each row Once we know what our data provides we can create our custom Parser .","title":"Understand the data format"},{"location":"custom_parser/#create-the-parser","text":"The first step is to create a template record for our specific type of dataset, in this case we're doing standard object detection: template_record = ObjectDetectionRecord () Now use the method generate_template that will print out all the necessary steps we have to implement. Parser . generate_template ( template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_filepath(<Union[str, Path]>) record.set_img_size(<ImgSize>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) record.detection.add_bboxes(<Sequence[BBox]>) We can copy the template and use it as our starting point. Let's go over each of the methods we have to define: __init__ : What happens here is completely up to you, normally we have to pass some reference to our data, data_dir in our case. __iter__ : This tells our parser how to iterate over our data, each item returned here will be passed to parse_fields as o . In our case we call df.itertuples to iterate over all df rows. __len__ : How many items will be iterating over. imageid : Should return a Hashable ( int , str , etc). In our case we want all the dataset items that have the same filename to be unified in the same record. parse_fields : Here is where the attributes of the record are collected, the template will suggest what methods we need to call on the record and what parameters it expects. The parameter o it receives is the item returned by __iter__ . Important Be sure to pass the correct type on all record methods! class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : for o in self . df . itertuples (): yield o def __len__ ( self ) -> int : return len ( self . df ) def record_id ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record , is_new ): if is_new : record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detection . set_class_map ( self . class_map ) record . detection . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detection . add_labels ([ o . label ]) Let's randomly split the data and parser with Parser.parse : parser = ChessParser ( template_record , data_dir ) train_records , valid_records = parser . parse () 0%| | 0/109 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7 [00:00<?, ?it/s] Let's take a look at one record: show_record ( train_records [ 0 ], display_label = False , figsize = ( 14 , 10 )) train_records [ 0 ] BaseRecord # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Filepath: /root/.icevision/data/chess_sample/chess_sample-master/images/6.jpg - Img: None - Image size ImgSize(width=416, height=416) - Record ID: 6.jpg detection: - Class Map: <ClassMap: {'background': 0, 'black-bishop': 1, 'black-king': 2, 'black-queen': 3, 'black-rook': 4, 'black-pawn': 5, 'black-knight': 6, 'white-queen': 7, 'white-rook': 8, 'white-king': 9, 'white-bishop': 10, 'white-knight': 11, 'white-pawn': 12}> - Labels: [8, 9, 12, 12, 12, 12, 2, 4, 3, 5, 5, 5] - BBoxes: [<BBox (xmin:170, ymin:79, xmax:195, ymax:132)>, <BBox (xmin:131, ymin:129, xmax:162, ymax:213)>, <BBox (xmin:167, ymin:119, xmax:189, ymax:165)>, <BBox (xmin:136, ymin:52, xmax:158, ymax:97)>, <BBox (xmin:101, ymin:15, xmax:123, ymax:59)>, <BBox (xmin:94, ymin:256, xmax:118, ymax:304)>, <BBox (xmin:257, ymin:268, xmax:291, ymax:354)>, <BBox (xmin:335, ymin:297, xmax:368, ymax:355)>, <BBox (xmin:234, ymin:97, xmax:260, ymax:173)>, <BBox (xmin:273, ymin:84, xmax:291, ymax:128)>, <BBox (xmin:204, ymin:118, xmax:223, ymax:162)>, <BBox (xmin:248, ymin:204, xmax:270, ymax:251)>]","title":"Create the Parser"},{"location":"custom_parser/#models","text":"We've selected a few of the many options below. You can easily pick which libraries, models, and backbones you like to use. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } elif selection == 3 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 4 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 5 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) (<module 'icevision.models.mmdet.models.vfnet' from '/usr/local/lib/python3.7/dist-packages/icevision/models/mmdet/models/vfnet/__init__.py'>, <icevision.models.mmdet.models.vfnet.backbones.resnet_fpn.MMDetVFNETBackboneConfig at 0x7f3d3eefcf50>, {})","title":"Models"},{"location":"custom_parser/#training","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Training"},{"location":"custom_parser/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' SuggestedLRs(valley=3.0199516913853586e-05) learn . fine_tune ( 40 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.581739 1.825835 0.196081 00:01 /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /usr/local/lib/python3.7/dist-packages/mmdet/core/anchor/anchor_generator.py:361: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` '``single_level_grid_anchors`` would be deprecated soon. ' epoch train_loss valid_loss COCOMetric time 0 1.581900 1.786651 0.216952 00:01 1 1.608743 1.763155 0.224355 00:01 2 1.631124 1.756258 0.224446 00:01 3 1.605482 1.744663 0.221207 00:01 4 1.601004 1.733463 0.225954 00:01 5 1.602483 1.737170 0.227496 00:01 6 1.585189 1.726894 0.230818 00:01 7 1.569807 1.705676 0.242893 00:01 8 1.568512 1.702688 0.239849 00:01 9 1.554865 1.681125 0.247037 00:01 10 1.555655 1.684015 0.297935 00:01 11 1.543609 1.681997 0.301196 00:01 12 1.534735 1.690655 0.310736 00:01 13 1.524568 1.689521 0.314591 00:01 14 1.516276 1.659921 0.330271 00:01 15 1.511769 1.629157 0.348593 00:01 16 1.502966 1.596284 0.362406 00:01 17 1.497759 1.586747 0.371476 00:01 18 1.497266 1.607103 0.369360 00:01 19 1.492118 1.642410 0.333980 00:01 20 1.489123 1.666914 0.333943 00:01 21 1.487922 1.678961 0.331371 00:01 22 1.483647 1.671102 0.349929 00:01 23 1.480486 1.642387 0.365551 00:01 24 1.472842 1.604754 0.377337 00:01 25 1.466778 1.569934 0.398319 00:01 26 1.457183 1.547147 0.410581 00:01 27 1.452692 1.534132 0.419814 00:01 28 1.447047 1.526892 0.423409 00:01 29 1.443192 1.525016 0.422199 00:01 30 1.444314 1.528092 0.420855 00:01 31 1.442283 1.533094 0.419651 00:01 32 1.434897 1.537819 0.417674 00:01 33 1.431285 1.542786 0.418381 00:01 34 1.424665 1.546462 0.418103 00:01 35 1.418521 1.548954 0.416256 00:01 36 1.414101 1.550630 0.416183 00:01 37 1.407963 1.551399 0.416183 00:01 38 1.402073 1.551711 0.416183 00:01 39 1.393795 1.551780 0.416183 00:01","title":"Training using fastai"},{"location":"custom_parser/#training-using-pytorch-lightning","text":"# class LightModel(model_type.lightning.ModelAdapter): # def configure_optimizers(self): # return Adam(self.parameters(), lr=1e-4) # light_model = LightModel(model, metrics=metrics) # trainer = pl.Trainer(max_epochs=20, gpus=1) # trainer.fit(light_model, train_dl, valid_dl)","title":"Training using Pytorch Lightning"},{"location":"custom_parser/#showing-the-results","text":"model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Showing the results"},{"location":"custom_parser/#batch-inference-prediction","text":"infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/1 [00:00<?, ?it/s]","title":"Batch Inference (Prediction)"},{"location":"custom_parser/#conclusion","text":"This notebook shows ho to create a custom parser to process data stored in format different than the popular COCO or VOV formats. The parsed data feed any IceVision models that could be trained by either Fastai or Pytorch-Lightning training loops.","title":"Conclusion"},{"location":"custom_parser/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"data_splits/","text":"[source] DataSplitter icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source] RandomSplitter icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source] FixedSplitter icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source] SingleSplitSplitter icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"Data Splitters"},{"location":"data_splits/#datasplitter","text":"icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source]","title":"DataSplitter"},{"location":"data_splits/#randomsplitter","text":"icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source]","title":"RandomSplitter"},{"location":"data_splits/#fixedsplitter","text":"icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source]","title":"FixedSplitter"},{"location":"data_splits/#singlesplitsplitter","text":"icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"SingleSplitSplitter"},{"location":"dataset/","text":"[source] Dataset icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm Optional[icevision.tfms.transform.Transform] : Transforms to be applied to each item. [source] from_images Dataset . from_images ( images , tfm = None , class_map = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"Dataset"},{"location":"dataset/#dataset","text":"icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm Optional[icevision.tfms.transform.Transform] : Transforms to be applied to each item. [source]","title":"Dataset"},{"location":"dataset/#from_images","text":"Dataset . from_images ( images , tfm = None , class_map = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"from_images"},{"location":"deployment/","text":"Deployment We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Deployment"},{"location":"deployment/#deployment","text":"We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Deployment"},{"location":"efficientdet/","text":"[source] model icevision . models . ross . efficientdet . model . model ( backbone , num_classes , img_size , ** kwargs ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments backbone icevision.models.ross.efficientdet.utils.EfficientDetBackboneConfig : Specifies the backbone to use create the model. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 128. Returns A PyTorch model. [source] train_dl icevision . models . ross . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . ross . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . ross . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . ross . efficientdet . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . ross . efficientdet . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . ross . efficientdet . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"efficientdet/#model","text":"icevision . models . ross . efficientdet . model . model ( backbone , num_classes , img_size , ** kwargs ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments backbone icevision.models.ross.efficientdet.utils.EfficientDetBackboneConfig : Specifies the backbone to use create the model. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 128. Returns A PyTorch model. [source]","title":"model"},{"location":"efficientdet/#train_dl","text":"icevision . models . ross . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"efficientdet/#valid_dl","text":"icevision . models . ross . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"efficientdet/#infer_dl","text":"icevision . models . ross . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"efficientdet/#build_train_batch","text":"icevision . models . ross . efficientdet . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"efficientdet/#build_valid_batch","text":"icevision . models . ross . efficientdet . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"efficientdet/#build_infer_batch","text":"icevision . models . ross . efficientdet . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"efficientdet_fastai/","text":"[source] learner icevision . models . ross . efficientdet . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"efficientdet_fastai/#learner","text":"icevision . models . ross . efficientdet . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"efficientdet_lightning/","text":"[source] ModelAdapter icevision . models . ross . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"efficientdet_lightning/#modeladapter","text":"icevision . models . ross . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"faster_rcnn/","text":"[source] model icevision . models . torchvision . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained : Argument passed to fastercnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source] train_dl icevision . models . torchvision . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . torchvision . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . torchvision . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . torchvision . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . torchvision . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . torchvision . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"faster_rcnn/#model","text":"icevision . models . torchvision . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. pretrained : Argument passed to fastercnn_resnet50_fpn if backbone is None . By default it is set to True : this is generally used when training a new model (transfer learning). pretrained = False is used during inference (prediction) for cases where the users have their own pretrained weights. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source]","title":"model"},{"location":"faster_rcnn/#train_dl","text":"icevision . models . torchvision . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"faster_rcnn/#valid_dl","text":"icevision . models . torchvision . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"faster_rcnn/#infer_dl","text":"icevision . models . torchvision . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"faster_rcnn/#build_train_batch","text":"icevision . models . torchvision . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"faster_rcnn/#build_valid_batch","text":"icevision . models . torchvision . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"faster_rcnn/#build_infer_batch","text":"icevision . models . torchvision . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"faster_rcnn_fastai/","text":"[source] learner icevision . models . torchvision . faster_rcnn . fastai . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"faster_rcnn_fastai/#learner","text":"icevision . models . torchvision . faster_rcnn . fastai . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"faster_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . torchvision . faster_rcnn . lightning . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"faster_rcnn_lightning/#modeladapter","text":"icevision . models . torchvision . faster_rcnn . lightning . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"getting_started_instance_segmentation/","text":"Getting Started with Instance Segmentation using IceVision Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the pennfudan folder in icedata. Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions Install from pypi... # # IceVision - IceData - MMDetection - YOLO v5 Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Alternative option of importing icevision In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc). Model To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section Choose a model and backbone We use MMDet here. When you want to use the torch vision version the COCOMetric will not be correct at the moment due to a problem in the bounding box conversion. # Just change the value of selection to try another model selection = 1 if selection == 0 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x if selection == 1 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . mask_rcnn_swin_t_p4_w7_fpn_1x_coco if selection == 2 : model_type = models . mmdet . yolact backbone = model_type . backbones . r101_1x8_coco if selection == 3 : model_type = models . torchvision . mask_rcnn backbone = model_type . backbones . resnet18_fpn Datasets : Pennfudan Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s] Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) DataLoader # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 , shuffle = False ) valid_batch = first ( valid_dl ) infer_batch = first ( infer_dl ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Model Now that we determined the number of classes ( num_classes ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = icedata . pennfudan . NUM_CLASSES ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . mask )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(valley=9.120108734350652e-05) learn . fine_tune ( 5 , 3e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss COCOMetric time 0 1.518704 0.520843 0.610489 00:11 1 0.799316 0.388823 0.689349 00:10 epoch train_loss valid_loss COCOMetric time 0 0.373752 0.356427 0.690000 00:11 1 0.362768 0.354911 0.721681 00:11 2 0.345709 0.342564 0.722313 00:11 3 0.327585 0.345134 0.733165 00:11 4 0.324897 0.341429 0.732759 00:11 Training using Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 5e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Show Results model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial batch , records = first ( valid_dl ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ], ncols = 3 ) 0%| | 0/9 [00:00<?, ?it/s] Happy Learning! If you need any assistance, feel free to join our forum .","title":"Instance Segmentation"},{"location":"getting_started_instance_segmentation/#getting-started-with-instance-segmentation-using-icevision","text":"","title":"Getting Started with Instance Segmentation using IceVision"},{"location":"getting_started_instance_segmentation/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the pennfudan folder in icedata.","title":"Introduction"},{"location":"getting_started_instance_segmentation/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions Install from pypi... # # IceVision - IceData - MMDetection - YOLO v5 Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"getting_started_instance_segmentation/#imports","text":"from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"getting_started_instance_segmentation/#alternative-option-of-importing-icevision","text":"In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc).","title":"Alternative option of importing icevision"},{"location":"getting_started_instance_segmentation/#model","text":"To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section","title":"Model"},{"location":"getting_started_instance_segmentation/#choose-a-model-and-backbone","text":"We use MMDet here. When you want to use the torch vision version the COCOMetric will not be correct at the moment due to a problem in the bounding box conversion. # Just change the value of selection to try another model selection = 1 if selection == 0 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x if selection == 1 : model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . mask_rcnn_swin_t_p4_w7_fpn_1x_coco if selection == 2 : model_type = models . mmdet . yolact backbone = model_type . backbones . r101_1x8_coco if selection == 3 : model_type = models . torchvision . mask_rcnn backbone = model_type . backbones . resnet18_fpn","title":"Choose a model and backbone"},{"location":"getting_started_instance_segmentation/#datasets-pennfudan","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s]","title":"Datasets : Pennfudan"},{"location":"getting_started_instance_segmentation/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Displaying the same image with different transforms"},{"location":"getting_started_instance_segmentation/#dataloader","text":"# DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 , shuffle = False ) valid_batch = first ( valid_dl ) infer_batch = first ( infer_dl ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"DataLoader"},{"location":"getting_started_instance_segmentation/#model_1","text":"Now that we determined the number of classes ( num_classes ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = icedata . pennfudan . NUM_CLASSES )","title":"Model"},{"location":"getting_started_instance_segmentation/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . mask )]","title":"Metrics"},{"location":"getting_started_instance_segmentation/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"getting_started_instance_segmentation/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () SuggestedLRs(valley=9.120108734350652e-05) learn . fine_tune ( 5 , 3e-4 , freeze_epochs = 2 ) epoch train_loss valid_loss COCOMetric time 0 1.518704 0.520843 0.610489 00:11 1 0.799316 0.388823 0.689349 00:10 epoch train_loss valid_loss COCOMetric time 0 0.373752 0.356427 0.690000 00:11 1 0.362768 0.354911 0.721681 00:11 2 0.345709 0.342564 0.722313 00:11 3 0.327585 0.345134 0.733165 00:11 4 0.324897 0.341429 0.732759 00:11","title":"Training using fastai"},{"location":"getting_started_instance_segmentation/#training-using-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 5e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"getting_started_instance_segmentation/#show-results","text":"model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Show Results"},{"location":"getting_started_instance_segmentation/#inference","text":"","title":"Inference"},{"location":"getting_started_instance_segmentation/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial batch , records = first ( valid_dl ) infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ], ncols = 3 ) 0%| | 0/9 [00:00<?, ?it/s]","title":"Predicting a batch of images"},{"location":"getting_started_instance_segmentation/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"getting_started_keypoint_detection/","text":"Getting Started with Keypoint Detection using IceVision Introduction This tutorial walk you through the different steps of training the biwi dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the biwi dataset as well as its corresponding parser check out the biwi folder in icedata. Installing IceVision and IceData Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * Alternative option of importing icevision In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc). Model To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section Choose a model and backbone TorchVision model_type = models . torchvision . keypoint_rcnn backbone = model_type . backbones . resnet34_fpn () Datasets : Biwi Biwi dataset is tiny dataset that contains 200 images of 1 class Nose . We will use icedata to download the dataset and get it with pre-defined transforms for training and validation. Note: If you want a more challenging dataset take a look at OCHuman . # Loading Data data_dir = icedata . biwi . load_data () train_ds , valid_ds = icedata . biwi . dataset ( data_dir ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) DataLoader # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Model Now that we determined the number of classes ( num_classes ) and number of keypoints ( num_keypoints ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone , num_keypoints = 1 , num_classes = icedata . biwi . NUM_CLASSES ) Metrics Metrics for keypoint are a work in progress # metrics = [COCOMetric(metric_type=COCOMetricType.keypoint)] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . lr_find () SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 10 , 3e-5 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 8.645806 7.013292 00:28 epoch train_loss valid_loss time 0 6.684147 6.487582 00:26 1 6.277197 5.731036 00:26 2 5.836541 5.114465 00:26 3 5.408250 4.583161 00:20 4 4.996461 4.225419 00:23 5 4.657006 3.987719 00:21 6 4.335581 3.832931 00:20 7 4.114164 3.862228 00:20 8 3.902603 3.697594 00:20 9 3.768973 3.694008 00:21 Training using Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 3e-5 ) light_model = LightModel ( model ) trainer = pl . Trainer ( max_epochs = 2 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Show Results model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Keypoint Detection"},{"location":"getting_started_keypoint_detection/#getting-started-with-keypoint-detection-using-icevision","text":"","title":"Getting Started with Keypoint Detection using IceVision"},{"location":"getting_started_keypoint_detection/#introduction","text":"This tutorial walk you through the different steps of training the biwi dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the biwi dataset as well as its corresponding parser check out the biwi folder in icedata.","title":"Introduction"},{"location":"getting_started_keypoint_detection/#installing-icevision-and-icedata","text":"Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"getting_started_keypoint_detection/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"getting_started_keypoint_detection/#alternative-option-of-importing-icevision","text":"In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc).","title":"Alternative option of importing icevision"},{"location":"getting_started_keypoint_detection/#model","text":"To create a model, we need to: Choose one of the models supported by IceVision Choose one of the backbones corresponding to a chosen model Determine the number of the object classes : This will be done after parsing a dataset. Check out the Parsing Section","title":"Model"},{"location":"getting_started_keypoint_detection/#choose-a-model-and-backbone","text":"TorchVision model_type = models . torchvision . keypoint_rcnn backbone = model_type . backbones . resnet34_fpn ()","title":"Choose a model and backbone"},{"location":"getting_started_keypoint_detection/#datasets-biwi","text":"Biwi dataset is tiny dataset that contains 200 images of 1 class Nose . We will use icedata to download the dataset and get it with pre-defined transforms for training and validation. Note: If you want a more challenging dataset take a look at OCHuman . # Loading Data data_dir = icedata . biwi . load_data () train_ds , valid_ds = icedata . biwi . dataset ( data_dir )","title":"Datasets : Biwi"},{"location":"getting_started_keypoint_detection/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Displaying the same image with different transforms"},{"location":"getting_started_keypoint_detection/#dataloader","text":"# DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"DataLoader"},{"location":"getting_started_keypoint_detection/#model_1","text":"Now that we determined the number of classes ( num_classes ) and number of keypoints ( num_keypoints ), we can create our model object. # TODO: Better flow for train_ds model = model_type . model ( backbone = backbone , num_keypoints = 1 , num_classes = icedata . biwi . NUM_CLASSES )","title":"Model"},{"location":"getting_started_keypoint_detection/#metrics","text":"Metrics for keypoint are a work in progress # metrics = [COCOMetric(metric_type=COCOMetricType.keypoint)]","title":"Metrics"},{"location":"getting_started_keypoint_detection/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"getting_started_keypoint_detection/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . lr_find () SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 10 , 3e-5 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 8.645806 7.013292 00:28 epoch train_loss valid_loss time 0 6.684147 6.487582 00:26 1 6.277197 5.731036 00:26 2 5.836541 5.114465 00:26 3 5.408250 4.583161 00:20 4 4.996461 4.225419 00:23 5 4.657006 3.987719 00:21 6 4.335581 3.832931 00:20 7 4.114164 3.862228 00:20 8 3.902603 3.697594 00:20 9 3.768973 3.694008 00:21","title":"Training using fastai"},{"location":"getting_started_keypoint_detection/#training-using-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 3e-5 ) light_model = LightModel ( model ) trainer = pl . Trainer ( max_epochs = 2 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"getting_started_keypoint_detection/#show-results","text":"model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Show Results"},{"location":"getting_started_keypoint_detection/#inference","text":"","title":"Inference"},{"location":"getting_started_keypoint_detection/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batches at the time: This option is more memory efficient. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ])","title":"Predicting a batch of images"},{"location":"getting_started_keypoint_detection/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"getting_started_object_detection/","text":"Getting Started with Object Detection using IceVision Introduction IceVision is a Framework for object detection and deep learning that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets. It enables you to work with multiple training engines, including fastai , and pytorch-lightning . It enables you to work with some of the best deep learning libraries including mmdetection , Ross Wightman's efficientdet implementation and model library, torchvision , and ultralytics Yolo . It enables you to select from many possible models and backbones from these libraries. IceVision lets you switch between them with ease. This means that you can pick the engine, library, model, and data format that work for you now and easily change them in the future. You can experiment with with them to see which ones meet your requirements. In this tutorial, you will learn how to 1. Install IceVision. This will include the IceData package that provides easy access to several sample datasets, as well as the engines and libraries that IceVision works with. 2. Download and prepare a dataset to work with. 3. Select an object detection library, model, and backbone. 4. Instantiate the model, and then train it with both the fastai and pytorch lightning engines. 5. And finally, use the model to identify objects in images. The notebook is set up so that you can easily select different libraries, models, and backbones to try. Install IceVision and IceData The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Alternative option of importing icevision In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc). Download and prepare a dataset Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) Parse the dataset The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible autofix capability that identifies common errors in annotation files, reports, and often corrects them automatically. The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed. The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations. # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map Creating datasets with augmentations and transforms Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Understanding the transforms The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) Select a library, model, and backbone In order to create a model, we need to: * Choose one of the libraries supported by IceVision * Choose one of the models supported by the library * Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Creating a model Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone could be specified by: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how easy it is to try new libraries, models, and backbones. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . resnet101_fpn_2x # extra_args['cfg_options'] = { # 'model.roi_head.bbox_head.loss_bbox.loss_weight': 2, # 'model.roi_head.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 3 : model_type = models . mmdet . ssd backbone = model_type . backbones . ssd300 if selection == 4 : model_type = models . mmdet . yolox backbone = model_type . backbones . yolox_s_8x8 if selection == 5 : model_type = models . mmdet . yolof backbone = model_type . backbones . yolof_r50_c5_8x8_1x_coco if selection == 6 : model_type = models . mmdet . detr backbone = model_type . backbones . r50_8x2_150e_coco if selection == 7 : model_type = models . mmdet . deformable_detr backbone = model_type . backbones . twostage_refine_r50_16x2_50e_coco if selection == 8 : model_type = models . mmdet . fsaf backbone = model_type . backbones . x101_64x4d_fpn_1x_coco if selection == 9 : model_type = models . mmdet . sabl backbone = model_type . backbones . r101_fpn_gn_2x_ms_640_800_coco if selection == 10 : model_type = models . mmdet . centripetalnet backbone = model_type . backbones . hourglass104_mstest_16x6_210e_coco elif selection == 11 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 12 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 13 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args backbone . __dict__ Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) Data Loader The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Metrics The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning . Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) learn . fine_tune ( 20 , 0.00158 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 4.282796 3.556162 0.000000 00:03 epoch train_loss valid_loss COCOMetric time 0 2.906284 1.651251 0.514744 00:03 1 2.120166 1.340260 0.636351 00:03 2 1.760227 1.048941 0.744560 00:03 3 1.543842 1.071704 0.850686 00:03 4 1.387548 0.964828 0.843832 00:03 5 1.284623 0.858511 0.878575 00:03 6 1.216737 0.871548 0.878225 00:03 7 1.137540 0.832400 0.880371 00:03 8 1.083292 0.753999 0.909935 00:03 9 1.021289 0.762208 0.885468 00:03 10 0.957823 0.682070 0.904922 00:03 11 0.914076 0.667000 0.900920 00:03 12 0.868626 0.731849 0.892283 00:03 13 0.825539 0.636049 0.908815 00:03 14 0.784374 0.613358 0.928341 00:03 15 0.750682 0.584259 0.929360 00:03 16 0.721707 0.569473 0.929833 00:03 17 0.701580 0.573457 0.930522 00:03 18 0.676955 0.569403 0.931394 00:03 19 0.659083 0.566195 0.929665 00:03 Training using Pytorch Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Testing using Pytorch Lightning For testing, it is recommended to use a separate test dataset that the model did not see during training but for demonstration purposes we'll re-use the validation dataset. trainer = pl . Trainer () trainer . test ( light_model , valid_dl ) Using the model - inference and showing results The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Prediction Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s] Happy Learning! If you need any assistance, feel free to join our forum .","title":"Object Detection"},{"location":"getting_started_object_detection/#getting-started-with-object-detection-using-icevision","text":"","title":"Getting Started with Object Detection using IceVision"},{"location":"getting_started_object_detection/#introduction","text":"IceVision is a Framework for object detection and deep learning that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets. It enables you to work with multiple training engines, including fastai , and pytorch-lightning . It enables you to work with some of the best deep learning libraries including mmdetection , Ross Wightman's efficientdet implementation and model library, torchvision , and ultralytics Yolo . It enables you to select from many possible models and backbones from these libraries. IceVision lets you switch between them with ease. This means that you can pick the engine, library, model, and data format that work for you now and easily change them in the future. You can experiment with with them to see which ones meet your requirements. In this tutorial, you will learn how to 1. Install IceVision. This will include the IceData package that provides easy access to several sample datasets, as well as the engines and libraries that IceVision works with. 2. Download and prepare a dataset to work with. 3. Select an object detection library, model, and backbone. 4. Instantiate the model, and then train it with both the fastai and pytorch lightning engines. 5. And finally, use the model to identify objects in images. The notebook is set up so that you can easily select different libraries, models, and backbones to try.","title":"Introduction"},{"location":"getting_started_object_detection/#install-icevision-and-icedata","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision and IceData"},{"location":"getting_started_object_detection/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/dnth/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"getting_started_object_detection/#alternative-option-of-importing-icevision","text":"In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc).","title":"Alternative option of importing icevision"},{"location":"getting_started_object_detection/#download-and-prepare-a-dataset","text":"Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir )","title":"Download and prepare a dataset"},{"location":"getting_started_object_detection/#parse-the-dataset","text":"The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible autofix capability that identifies common errors in annotation files, reports, and often corrects them automatically. The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed. The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations. # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map","title":"Parse the dataset"},{"location":"getting_started_object_detection/#creating-datasets-with-augmentations-and-transforms","text":"Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Creating datasets with augmentations and transforms"},{"location":"getting_started_object_detection/#understanding-the-transforms","text":"The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Understanding the transforms"},{"location":"getting_started_object_detection/#select-a-library-model-and-backbone","text":"In order to create a model, we need to: * Choose one of the libraries supported by IceVision * Choose one of the models supported by the library * Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library.","title":"Select a library, model, and backbone"},{"location":"getting_started_object_detection/#creating-a-model","text":"Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone could be specified by: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how easy it is to try new libraries, models, and backbones. # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . resnet50_fpn_mstrain_2x if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x # extra_args['cfg_options'] = { # 'model.bbox_head.loss_bbox.loss_weight': 2, # 'model.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 2 : model_type = models . mmdet . faster_rcnn backbone = model_type . backbones . resnet101_fpn_2x # extra_args['cfg_options'] = { # 'model.roi_head.bbox_head.loss_bbox.loss_weight': 2, # 'model.roi_head.bbox_head.loss_cls.loss_weight': 0.8, # } if selection == 3 : model_type = models . mmdet . ssd backbone = model_type . backbones . ssd300 if selection == 4 : model_type = models . mmdet . yolox backbone = model_type . backbones . yolox_s_8x8 if selection == 5 : model_type = models . mmdet . yolof backbone = model_type . backbones . yolof_r50_c5_8x8_1x_coco if selection == 6 : model_type = models . mmdet . detr backbone = model_type . backbones . r50_8x2_150e_coco if selection == 7 : model_type = models . mmdet . deformable_detr backbone = model_type . backbones . twostage_refine_r50_16x2_50e_coco if selection == 8 : model_type = models . mmdet . fsaf backbone = model_type . backbones . x101_64x4d_fpn_1x_coco if selection == 9 : model_type = models . mmdet . sabl backbone = model_type . backbones . r101_fpn_gn_2x_ms_640_800_coco if selection == 10 : model_type = models . mmdet . centripetalnet backbone = model_type . backbones . hourglass104_mstest_16x6_210e_coco elif selection == 11 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 12 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 13 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args backbone . __dict__ Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Creating a model"},{"location":"getting_started_object_detection/#data-loader","text":"The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"Data Loader"},{"location":"getting_started_object_detection/#metrics","text":"The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"getting_started_object_detection/#training","text":"IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning .","title":"Training"},{"location":"getting_started_object_detection/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) learn . fine_tune ( 20 , 0.00158 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 4.282796 3.556162 0.000000 00:03 epoch train_loss valid_loss COCOMetric time 0 2.906284 1.651251 0.514744 00:03 1 2.120166 1.340260 0.636351 00:03 2 1.760227 1.048941 0.744560 00:03 3 1.543842 1.071704 0.850686 00:03 4 1.387548 0.964828 0.843832 00:03 5 1.284623 0.858511 0.878575 00:03 6 1.216737 0.871548 0.878225 00:03 7 1.137540 0.832400 0.880371 00:03 8 1.083292 0.753999 0.909935 00:03 9 1.021289 0.762208 0.885468 00:03 10 0.957823 0.682070 0.904922 00:03 11 0.914076 0.667000 0.900920 00:03 12 0.868626 0.731849 0.892283 00:03 13 0.825539 0.636049 0.908815 00:03 14 0.784374 0.613358 0.928341 00:03 15 0.750682 0.584259 0.929360 00:03 16 0.721707 0.569473 0.929833 00:03 17 0.701580 0.573457 0.930522 00:03 18 0.676955 0.569403 0.931394 00:03 19 0.659083 0.566195 0.929665 00:03","title":"Training using fastai"},{"location":"getting_started_object_detection/#training-using-pytorch-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Pytorch Lightning"},{"location":"getting_started_object_detection/#testing-using-pytorch-lightning","text":"For testing, it is recommended to use a separate test dataset that the model did not see during training but for demonstration purposes we'll re-use the validation dataset. trainer = pl . Trainer () trainer . test ( light_model , valid_dl )","title":"Testing using Pytorch Lightning"},{"location":"getting_started_object_detection/#using-the-model-inference-and-showing-results","text":"The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Using the model - inference and showing results"},{"location":"getting_started_object_detection/#prediction","text":"Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s]","title":"Prediction"},{"location":"getting_started_object_detection/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"getting_started_semantic_segmentation/","text":"Getting Started with Semantic Segmentation using IceVision Introduction to IceVision IceVision is a Framework for object detection, instance segmentation and semantic segmentation that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets. It enables you to work with multiple training engines, including fastai , and pytorch-lightning . It enables you to work with some of the best deep learning libraries including mmdetection , Ross Wightman's efficientdet implementation and model library, torchvision , ultralytics Yolo , and mmsegmentation . It enables you to select from many possible models and backbones from these libraries. IceVision lets you switch between them with ease. This means that you can pick the engine, library, model, and data format that work for you now and easily change them in the future. You can experiment with with them to see which ones meet your requirements. Getting Started with Semantic Segmentation This notebook will walk you through the training of models for semantic segmentation - a task that consists in classifying each pixel of an image into one of multiple classes. In this tutorial, you will learn how to 1. Install IceVision. This will include the IceData package that provides easy access to several sample datasets, as well as the engines and libraries that IceVision works with. 2. Download and prepare a dataset to work with. 3. Select an object detection library, model, and backbone. 4. Instantiate the model, and then train it with both the fastai engine. 5. And finally, use the model to identify objects in images. The notebook is set up so that you can easily select different libraries, models, and backbones to try. Install IceVision and IceData The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, the MMSegmentation library and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet - mmsegmentation Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet - mmsegmentation Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * Alternative option of importing icevision In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc). Download and prepare a dataset Now we can start by downloading the camvid tiny dataset, which contains . This tiny dataset contains 100 images whose pixels are classified in 33 classes, including: - animal, - car, - bridge, - building. IceVision provides methods to load a dataset, parse annotation files, and more. Download the camvid tiny dataset and load it using icedata # Download data data_url = 'https://s3.amazonaws.com/fast-ai-sample/camvid_tiny.tgz' data_dir = icedata . load_data ( data_url , 'camvid_tiny' ) / 'camvid_tiny' Retrieve class codes from dataset file and create a class map (a structure that maps a class identifier, in this case an integer, to the actual class) codes = np . loadtxt ( data_dir / 'codes.txt' , dtype = str ) class_map = ClassMap ( list ( codes )) Get images files images_dir = data_dir / 'images' labels_dir = data_dir / 'labels' image_files = get_image_files ( images_dir ) Parse the dataset A unit of data in IceVision is called a record, which contains all the information required to handle a given image (e.g. path to the image, segmentation masks, class map, etc..). Here, we build a collection of records by iterating through the image files. records = RecordCollection ( SemanticSegmentationRecord ) for image_file in pbar ( image_files ): record = records . get_by_record_id ( image_file . stem ) if record . is_new : record . set_filepath ( image_file ) record . set_img_size ( get_img_size ( image_file )) record . segmentation . set_class_map ( class_map ) mask_file = SemanticMaskFile ( labels_dir / f ' { image_file . stem } _P.png' ) record . segmentation . set_mask ( mask_file ) records = records . autofix () train_records , valid_records = records . make_splits ( RandomSplitter ([ 0.8 , 0.2 ])) Take a peak at records Using show_records , we can preview the content of the records we created sample_records = random . choices ( records , k = 3 ) show_records ( sample_records , ncols = 3 ) Creating datasets with augmentations and transforms Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. presize , size = 512 , 384 presize , size = ImgSize ( presize , int ( presize * .75 )), ImgSize ( size , int ( size * .75 )) aug_tfms = tfms . A . aug_tfms ( presize = presize , size = size , pad = None , crop_fn = partial ( tfms . A . RandomCrop , p = 0.5 ), shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 2 ), ) train_tfms = tfms . A . Adapter ([ * aug_tfms , tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ tfms . A . resize ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Understanding the transforms The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. ds_samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( ds_samples , ncols = 3 ) Select a library, model, and backbone In order to create a model, we need to: Choose one of the libraries supported by IceVision Choose one of the models supported by the library Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Creating a model Selections only take two simple lines of code. For example, to try the mmsegmentation library using the deeplabv3 model and the resnet50_d8 backbone could be specified by: model_type = models . mmseg . deeplab3 backbone = model_type . backbones . backbones . resnet50_d8 As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how easy it is to try new libraries, models, and backbones. selection = 0 if selection == 0 : model_type = models . fastai . unet backbone = model_type . backbones . resnet34 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes , img_size = size ) if selection == 1 : model_type = models . mmseg . deeplabv3 backbone = model_type . backbones . resnet50_d8 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes ) if selection == 2 : model_type = models . mmseg . deeplabv3 backbone = model_type . backbones . resnet50_d8 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes ) if selection == 3 : model_type = models . mmseg . segformer backbone = model_type . backbones . mit_b0 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes ) Data Loader The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Metrics The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. As this is a segmentation problem, we are going to use two metrics: multi-class Diece coefficient, and segmentation accuracy. Note that we are ignoring \"void\" when computing accuracy metrics = [ MulticlassDiceCoefficient (), SegmentationAccuracy ( ignore_class = class_map . get_by_name ( \"Void\" ))] Training IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning . Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Because we use fastai, we get access to its features such as the learning rate finder learn . lr_find () learn . fine_tune ( 10 , 1e-4 ) Using the model - inference and showing results The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , num_samples = 2 ) Prediction Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_sample ( preds [ 0 ] . pred ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Semantic Segmentation"},{"location":"getting_started_semantic_segmentation/#getting-started-with-semantic-segmentation-using-icevision","text":"","title":"Getting Started with Semantic Segmentation using IceVision"},{"location":"getting_started_semantic_segmentation/#introduction-to-icevision","text":"IceVision is a Framework for object detection, instance segmentation and semantic segmentation that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets. It enables you to work with multiple training engines, including fastai , and pytorch-lightning . It enables you to work with some of the best deep learning libraries including mmdetection , Ross Wightman's efficientdet implementation and model library, torchvision , ultralytics Yolo , and mmsegmentation . It enables you to select from many possible models and backbones from these libraries. IceVision lets you switch between them with ease. This means that you can pick the engine, library, model, and data format that work for you now and easily change them in the future. You can experiment with with them to see which ones meet your requirements.","title":"Introduction to IceVision"},{"location":"getting_started_semantic_segmentation/#getting-started-with-semantic-segmentation","text":"This notebook will walk you through the training of models for semantic segmentation - a task that consists in classifying each pixel of an image into one of multiple classes. In this tutorial, you will learn how to 1. Install IceVision. This will include the IceData package that provides easy access to several sample datasets, as well as the engines and libraries that IceVision works with. 2. Download and prepare a dataset to work with. 3. Select an object detection library, model, and backbone. 4. Instantiate the model, and then train it with both the fastai engine. 5. And finally, use the model to identify objects in images. The notebook is set up so that you can easily select different libraries, models, and backbones to try.","title":"Getting Started with Semantic Segmentation"},{"location":"getting_started_semantic_segmentation/#install-icevision-and-icedata","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, the MMSegmentation library and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet - mmsegmentation Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet - mmsegmentation Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision and IceData"},{"location":"getting_started_semantic_segmentation/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import *","title":"Imports"},{"location":"getting_started_semantic_segmentation/#alternative-option-of-importing-icevision","text":"In some environments wildcard imports are not allowed (eg: Kubeflow Pipelines & distributed environment that rely on pickling). In that case you can call import icevision.all as iv instead of from icevision.all import * In that case you should reference icevision objects with the iv. prefix, so for instance one would do iv.Dataset instead of the regular approach of just doing Dataset , same for any other icevision object (eg: iv.tfms.A.Adapter instead of tfms.A.Adapter etc).","title":"Alternative option of importing icevision"},{"location":"getting_started_semantic_segmentation/#download-and-prepare-a-dataset","text":"Now we can start by downloading the camvid tiny dataset, which contains . This tiny dataset contains 100 images whose pixels are classified in 33 classes, including: - animal, - car, - bridge, - building. IceVision provides methods to load a dataset, parse annotation files, and more. Download the camvid tiny dataset and load it using icedata # Download data data_url = 'https://s3.amazonaws.com/fast-ai-sample/camvid_tiny.tgz' data_dir = icedata . load_data ( data_url , 'camvid_tiny' ) / 'camvid_tiny' Retrieve class codes from dataset file and create a class map (a structure that maps a class identifier, in this case an integer, to the actual class) codes = np . loadtxt ( data_dir / 'codes.txt' , dtype = str ) class_map = ClassMap ( list ( codes )) Get images files images_dir = data_dir / 'images' labels_dir = data_dir / 'labels' image_files = get_image_files ( images_dir )","title":"Download and prepare a dataset"},{"location":"getting_started_semantic_segmentation/#parse-the-dataset","text":"A unit of data in IceVision is called a record, which contains all the information required to handle a given image (e.g. path to the image, segmentation masks, class map, etc..). Here, we build a collection of records by iterating through the image files. records = RecordCollection ( SemanticSegmentationRecord ) for image_file in pbar ( image_files ): record = records . get_by_record_id ( image_file . stem ) if record . is_new : record . set_filepath ( image_file ) record . set_img_size ( get_img_size ( image_file )) record . segmentation . set_class_map ( class_map ) mask_file = SemanticMaskFile ( labels_dir / f ' { image_file . stem } _P.png' ) record . segmentation . set_mask ( mask_file ) records = records . autofix () train_records , valid_records = records . make_splits ( RandomSplitter ([ 0.8 , 0.2 ]))","title":"Parse the dataset"},{"location":"getting_started_semantic_segmentation/#take-a-peak-at-records","text":"Using show_records , we can preview the content of the records we created sample_records = random . choices ( records , k = 3 ) show_records ( sample_records , ncols = 3 )","title":"Take a peak at records"},{"location":"getting_started_semantic_segmentation/#creating-datasets-with-augmentations-and-transforms","text":"Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. presize , size = 512 , 384 presize , size = ImgSize ( presize , int ( presize * .75 )), ImgSize ( size , int ( size * .75 )) aug_tfms = tfms . A . aug_tfms ( presize = presize , size = size , pad = None , crop_fn = partial ( tfms . A . RandomCrop , p = 0.5 ), shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 2 ), ) train_tfms = tfms . A . Adapter ([ * aug_tfms , tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ tfms . A . resize ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Creating datasets with augmentations and transforms"},{"location":"getting_started_semantic_segmentation/#understanding-the-transforms","text":"The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. ds_samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( ds_samples , ncols = 3 )","title":"Understanding the transforms"},{"location":"getting_started_semantic_segmentation/#select-a-library-model-and-backbone","text":"In order to create a model, we need to: Choose one of the libraries supported by IceVision Choose one of the models supported by the library Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library.","title":"Select a library, model, and backbone"},{"location":"getting_started_semantic_segmentation/#creating-a-model","text":"Selections only take two simple lines of code. For example, to try the mmsegmentation library using the deeplabv3 model and the resnet50_d8 backbone could be specified by: model_type = models . mmseg . deeplab3 backbone = model_type . backbones . backbones . resnet50_d8 As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how easy it is to try new libraries, models, and backbones. selection = 0 if selection == 0 : model_type = models . fastai . unet backbone = model_type . backbones . resnet34 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes , img_size = size ) if selection == 1 : model_type = models . mmseg . deeplabv3 backbone = model_type . backbones . resnet50_d8 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes ) if selection == 2 : model_type = models . mmseg . deeplabv3 backbone = model_type . backbones . resnet50_d8 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes ) if selection == 3 : model_type = models . mmseg . segformer backbone = model_type . backbones . mit_b0 ( pretrained = True ) model = model_type . model ( backbone = backbone , num_classes = class_map . num_classes )","title":"Creating a model"},{"location":"getting_started_semantic_segmentation/#data-loader","text":"The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"Data Loader"},{"location":"getting_started_semantic_segmentation/#metrics","text":"The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. As this is a segmentation problem, we are going to use two metrics: multi-class Diece coefficient, and segmentation accuracy. Note that we are ignoring \"void\" when computing accuracy metrics = [ MulticlassDiceCoefficient (), SegmentationAccuracy ( ignore_class = class_map . get_by_name ( \"Void\" ))]","title":"Metrics"},{"location":"getting_started_semantic_segmentation/#training","text":"IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning .","title":"Training"},{"location":"getting_started_semantic_segmentation/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Because we use fastai, we get access to its features such as the learning rate finder learn . lr_find () learn . fine_tune ( 10 , 1e-4 )","title":"Training using fastai"},{"location":"getting_started_semantic_segmentation/#using-the-model-inference-and-showing-results","text":"The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , num_samples = 2 )","title":"Using the model - inference and showing results"},{"location":"getting_started_semantic_segmentation/#prediction","text":"Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_sample ( preds [ 0 ] . pred )","title":"Prediction"},{"location":"getting_started_semantic_segmentation/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"inference/","text":"Inference using IceVision Install IceVision The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master --2022-08-12 18:38:33-- https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2820 (2.8K) [text/plain] Saving to: \u2018icevision_install.sh.2\u2019 icevision_install.s 100%[===================>] 2.75K --.-KB/s in 0s 2022-08-12 18:38:33 (41.2 MB/s) - \u2018icevision_install.sh.2\u2019 saved [2820/2820] # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Installing icevision + dependencices for cuda11 - Installing torch and its dependencies Looking in links: https://download.pytorch.org/whl/torch_stable.html Requirement already satisfied: torch==1.10.0+cu111 in /opt/conda/lib/python3.9/site-packages (1.10.0+cu111) Requirement already satisfied: torchvision==0.11.1+cu111 in /opt/conda/lib/python3.9/site-packages (0.11.1+cu111) Requirement already satisfied: torchtext==0.11.0 in /opt/conda/lib/python3.9/site-packages (0.11.0) Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.10.0+cu111) (4.3.0) Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.1+cu111) (1.23.1) Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.1+cu111) (8.4.0) Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchtext==0.11.0) (2.27.1) Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from torchtext==0.11.0) (4.63.0) Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (2.0.4) Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (3.3) Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (1.26.8) Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (2022.6.15) \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing mmcv \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing mmdet \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing mmseg \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing icevision from master \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing icedata from master \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66)\u001b[0m\u001b[31m \u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python-headless==4.1.2.30\u001b[0m\u001b[31m \u001b[0micevision installation finished! {'status': 'ok', 'restart': True} Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m67\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmseg configs\u001b[0m | \u001b[36micevision.models.mmseg.download_configs\u001b[0m:\u001b[36mdownload_mmseg_configs\u001b[0m:\u001b[36m33\u001b[0m 0%| | 0/334331 [00:00<?, ?B/s] List of images for inference Please store your images in a folder, and populate the path_to_folder variable with the corresponding folder name. # Pick your images folder path_to_image_folder = \"./images\" img_files = get_image_files ( path_to_image_folder ) img_files img = PIL . Image . open ( img_files [ 0 ]) img (#5) [Path('images/boy_skating.png'),Path('images/clock_shop.jpeg'),Path('images/donuts.jpeg'),Path('images/toothbrushes.jpeg'),Path('images/umbrellas.jpg')] Loading a checkpoint and creating the corresponding model The checkpoint file can be either a local file or an URL # checkpoint_path = 'checkpoints/fridge-retinanet-save-checkpoint-full.pth' checkpoint_path = 'http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth' from icevision.models import * # The model is automatically recreated in the evaluation mode. To unset that mode, you only need to pass `eval_mode=Fales`) checkpoint_and_model = model_from_checkpoint ( checkpoint_path , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , img_size = 640 , is_coco = True ) load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth 2022-08-12 19:06:38,025 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'} 2022-08-12 19:06:38,029 - mmcv - INFO - load model from: torchvision://resnet50 2022-08-12 19:06:38,030 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50 2022-08-12 19:06:38,124 - mmcv - WARNING - The model and loaded state dict do not match exactly unexpected key in source state_dict: fc.weight, fc.bias # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type backbone class_map img_size 2022-08-12 19:06:38,142 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'} 2022-08-12 19:06:38,181 - mmcv - INFO - initialize RetinaHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'retina_cls', 'std': 0.01, 'bias_prob': 0.01}} 2022-08-12 19:06:38,223 - mmcv - INFO - backbone.conv1.weight - torch.Size([64, 3, 7, 7]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,223 - mmcv - INFO - backbone.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,224 - mmcv - INFO - backbone.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,224 - mmcv - INFO - backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,225 - mmcv - INFO - backbone.layer1.0.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,225 - mmcv - INFO - backbone.layer1.0.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,226 - mmcv - INFO - backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,226 - mmcv - INFO - backbone.layer1.0.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,228 - mmcv - INFO - backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,228 - mmcv - INFO - backbone.layer1.0.downsample.1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,228 - mmcv - INFO - backbone.layer1.0.downsample.1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,229 - mmcv - INFO - backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,229 - mmcv - INFO - backbone.layer1.1.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,229 - mmcv - INFO - backbone.layer1.1.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,230 - mmcv - INFO - backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,230 - mmcv - INFO - backbone.layer1.1.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,232 - mmcv - INFO - backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,232 - mmcv - INFO - backbone.layer1.2.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,234 - mmcv - INFO - backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,234 - mmcv - INFO - backbone.layer1.2.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,234 - mmcv - INFO - backbone.layer1.2.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,236 - mmcv - INFO - backbone.layer2.0.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,236 - mmcv - INFO - backbone.layer2.0.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,236 - mmcv - INFO - backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.downsample.1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,238 - mmcv - INFO - backbone.layer2.0.downsample.1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,238 - mmcv - INFO - backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,238 - mmcv - INFO - backbone.layer2.1.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,240 - mmcv - INFO - backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,240 - mmcv - INFO - backbone.layer2.1.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,240 - mmcv - INFO - backbone.layer2.1.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,241 - mmcv - INFO - backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,241 - mmcv - INFO - backbone.layer2.2.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,241 - mmcv - INFO - backbone.layer2.2.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,242 - mmcv - INFO - backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,242 - mmcv - INFO - backbone.layer2.2.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,242 - mmcv - INFO - backbone.layer2.2.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.2.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.2.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,244 - mmcv - INFO - backbone.layer2.3.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,244 - mmcv - INFO - backbone.layer2.3.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,244 - mmcv - INFO - backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,245 - mmcv - INFO - backbone.layer2.3.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,245 - mmcv - INFO - backbone.layer2.3.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,245 - mmcv - INFO - backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer2.3.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer2.3.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer3.0.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,247 - mmcv - INFO - backbone.layer3.0.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,247 - mmcv - INFO - backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,247 - mmcv - INFO - backbone.layer3.0.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,248 - mmcv - INFO - backbone.layer3.0.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,248 - mmcv - INFO - backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,248 - mmcv - INFO - backbone.layer3.0.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,249 - mmcv - INFO - backbone.layer3.0.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,249 - mmcv - INFO - backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,249 - mmcv - INFO - backbone.layer3.0.downsample.1.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,250 - mmcv - INFO - backbone.layer3.0.downsample.1.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,250 - mmcv - INFO - backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,250 - mmcv - INFO - backbone.layer3.1.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,251 - mmcv - INFO - backbone.layer3.1.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,251 - mmcv - INFO - backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,251 - mmcv - INFO - backbone.layer3.1.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,252 - mmcv - INFO - backbone.layer3.1.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,252 - mmcv - INFO - backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,253 - mmcv - INFO - backbone.layer3.1.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,253 - mmcv - INFO - backbone.layer3.1.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,254 - mmcv - INFO - backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,254 - mmcv - INFO - backbone.layer3.2.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,254 - mmcv - INFO - backbone.layer3.2.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,255 - mmcv - INFO - backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,255 - mmcv - INFO - backbone.layer3.2.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,257 - mmcv - INFO - backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,257 - mmcv - INFO - backbone.layer3.3.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,257 - mmcv - INFO - backbone.layer3.3.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,258 - mmcv - INFO - backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,258 - mmcv - INFO - backbone.layer3.3.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,259 - mmcv - INFO - backbone.layer3.3.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,259 - mmcv - INFO - backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,259 - mmcv - INFO - backbone.layer3.3.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,260 - mmcv - INFO - backbone.layer3.3.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,260 - mmcv - INFO - backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,260 - mmcv - INFO - backbone.layer3.4.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,261 - mmcv - INFO - backbone.layer3.4.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,261 - mmcv - INFO - backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,261 - mmcv - INFO - backbone.layer3.4.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,262 - mmcv - INFO - backbone.layer3.4.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,262 - mmcv - INFO - backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,263 - mmcv - INFO - backbone.layer3.4.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,263 - mmcv - INFO - backbone.layer3.4.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,263 - mmcv - INFO - backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,264 - mmcv - INFO - backbone.layer3.5.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,264 - mmcv - INFO - backbone.layer3.5.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,264 - mmcv - INFO - backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,265 - mmcv - INFO - backbone.layer3.5.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,265 - mmcv - INFO - backbone.layer3.5.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,265 - mmcv - INFO - backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,266 - mmcv - INFO - backbone.layer3.5.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,266 - mmcv - INFO - backbone.layer3.5.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,267 - mmcv - INFO - backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,267 - mmcv - INFO - backbone.layer4.0.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,267 - mmcv - INFO - backbone.layer4.0.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,268 - mmcv - INFO - backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,268 - mmcv - INFO - backbone.layer4.0.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,270 - mmcv - INFO - backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,270 - mmcv - INFO - backbone.layer4.0.downsample.1.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,270 - mmcv - INFO - backbone.layer4.0.downsample.1.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,271 - mmcv - INFO - backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,271 - mmcv - INFO - backbone.layer4.1.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,271 - mmcv - INFO - backbone.layer4.1.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,272 - mmcv - INFO - backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,272 - mmcv - INFO - backbone.layer4.1.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,272 - mmcv - INFO - backbone.layer4.1.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,273 - mmcv - INFO - backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,273 - mmcv - INFO - backbone.layer4.1.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,273 - mmcv - INFO - backbone.layer4.1.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,275 - mmcv - INFO - backbone.layer4.2.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,275 - mmcv - INFO - backbone.layer4.2.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,275 - mmcv - INFO - backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,276 - mmcv - INFO - backbone.layer4.2.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,276 - mmcv - INFO - backbone.layer4.2.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,276 - mmcv - INFO - neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,277 - mmcv - INFO - neck.lateral_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,277 - mmcv - INFO - neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,277 - mmcv - INFO - neck.lateral_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,278 - mmcv - INFO - neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,278 - mmcv - INFO - neck.lateral_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,279 - mmcv - INFO - neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,279 - mmcv - INFO - neck.fpn_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,279 - mmcv - INFO - neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,280 - mmcv - INFO - neck.fpn_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,280 - mmcv - INFO - neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,280 - mmcv - INFO - neck.fpn_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.4.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,282 - mmcv - INFO - bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,282 - mmcv - INFO - bbox_head.cls_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,284 - mmcv - INFO - bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,284 - mmcv - INFO - bbox_head.cls_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,284 - mmcv - INFO - bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,285 - mmcv - INFO - bbox_head.reg_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,285 - mmcv - INFO - bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,285 - mmcv - INFO - bbox_head.reg_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,286 - mmcv - INFO - bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,286 - mmcv - INFO - bbox_head.reg_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,286 - mmcv - INFO - bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,287 - mmcv - INFO - bbox_head.reg_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,287 - mmcv - INFO - bbox_head.retina_cls.weight - torch.Size([720, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 19:06:38,287 - mmcv - INFO - bbox_head.retina_cls.bias - torch.Size([720]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 19:06:38,288 - mmcv - INFO - bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,288 - mmcv - INFO - bbox_head.retina_reg.bias - torch.Size([36]): NormalInit: mean=0, std=0.01, bias=0 <module 'icevision.models.mmdet.models.retinanet' from '/notebooks/icevision/models/mmdet/models/retinanet/__init__.py'> <icevision.models.mmdet.models.retinanet.backbones.backbone_config.MMDetRetinanetBackboneConfig at 0x7fad702b70d0> <ClassMap: {'background': 0, 'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 12, 'parking meter': 13, 'bench': 14, 'bird': 15, 'cat': 16, 'dog': 17, 'horse': 18, 'sheep': 19, 'cow': 20, 'elephant': 21, 'bear': 22, 'zebra': 23, 'giraffe': 24, 'backpack': 25, 'umbrella': 26, 'handbag': 27, 'tie': 28, 'suitcase': 29, 'frisbee': 30, 'skis': 31, 'snowboard': 32, 'sports ball': 33, 'kite': 34, 'baseball bat': 35, 'baseball glove': 36, 'skateboard': 37, 'surfboard': 38, 'tennis racket': 39, 'bottle': 40, 'wine glass': 41, 'cup': 42, 'fork': 43, 'knife': 44, 'spoon': 45, 'bowl': 46, 'banana': 47, 'apple': 48, 'sandwich': 49, 'orange': 50, 'broccoli': 51, 'carrot': 52, 'hot dog': 53, 'pizza': 54, 'donut': 55, 'cake': 56, 'chair': 57, 'couch': 58, 'potted plant': 59, 'bed': 60, 'dining table': 61, 'toilet': 62, 'tv': 63, 'laptop': 64, 'mouse': 65, 'remote': 66, 'keyboard': 67, 'cell phone': 68, 'microwave': 69, 'oven': 70, 'toaster': 71, 'sink': 72, 'refrigerator': 73, 'book': 74, 'clock': 75, 'vase': 76, 'scissors': 77, 'teddy bear': 78, 'hair drier': 79, 'toothbrush': 80}> 640 Get Model Object model_from_checkpoint(checkpoint_path) returns a dictionary: checkpoint_and_model . The model object is stored in checkpoint_and_model[\"model\"] . # Get model object # The model is automatically set in the evaluation mode model = checkpoint_and_model [ \"model\" ] # Check device device = next ( model . parameters ()) . device device # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) device(type='cpu') Single Image Inference The end2end_detect() not only compute predictions for a single image but also automatically adjust predicted boxes to the original image size img = PIL . Image . open ( img_files [ 0 ]) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = class_map , detection_threshold = 0.5 ) pred_dict [ 'img' ] Batch Inference The following option shows to do generate inference for a set of images. The latter is processed in batches. # Create a dataset imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True , detection_threshold = 0.33 ) # You may need to un-comment this in certain environments to preview images: # %matplotlib inline show_preds ( preds = preds [ 0 : 5 ]) 0%| | 0/2 [00:00<?, ?it/s] How to export inferences as COCO annotations These will match the dimensions of the original images. This could be useful if you'd like to create preliminary pseudo-annotations for unlabeled data: from icevision.data.convert_records_to_coco_style import export_batch_inferences_as_coco_annotations info = { \"year\" : \"2022\" , \"version\" : \"1\" , \"description\" : \"Exported from IceVision\" , \"contributor\" : \"Awesome contributor\" , \"url\" : \"https://lazyannotator.fun\" , \"date_created\" : \"2022-08-05T20:13:09+00:00\" } licenses = [ { \"name\" : \"Creative Commons Attribution 4.0\" , \"id\" : 0 , \"url\" : \"https://creativecommons.org/licenses/by/4.0/legalcode\" , } ] export_batch_inferences_as_coco_annotations ( preds = preds , img_files = img_files , transforms = valid_tfms , class_map = class_map , output_filepath = \"../inferences_for_pseudo_labels.json\" , info = info , licenses = licenses , ) New COCO annotation file saved to ../inferences_for_pseudo_labels.json Preview predictions on original image: # Index of image you'd like to check i = 4 this_pred = preds [ i ] record = this_pred . pred # Draw that image pred_img = draw_record ( record = this_pred , class_map = class_map , display_label = True , display_score = True , display_bbox = True , font_path = get_default_font (), font_size = 12 , label_color = ( \"#FF59D6\" ), return_as_pil_img = True , ) pred_img How to save a model and its metadata in IceVision When saving a model weights, we could also store the model metadata that are retrieved by the model_from_checkpoint(checkpoint_path) method # How to save a model and its metadata checkpoint_path = 'coco-retinanet-checkpoint-full.pth' save_icevision_checkpoint ( model , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , classes = class_map . get_classes (), img_size = img_size , filename = checkpoint_path , meta = { 'icevision_version' : '0.9.1' }) Loading models already containing metadata If you have saved your model weights with the model metadata, you only need to call model_from_checkpoint(checkpoint_path) : No other arguments ( model_name, backbone_name, classes, img_size ) are needed. All the information is already embedded in the checkpoint file. checkpoint_path = 'https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth' checkpoint_and_model = model_from_checkpoint ( checkpoint_path ) load checkpoint from http path: https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth Downloading: \"https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth\" to /root/.cache/torch/hub/checkpoints/fridge-retinanet-checkpoint-full.pth 0%| | 0.00/139M [00:00<?, ?B/s] 2022-08-12 18:56:49,593 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'} 2022-08-12 18:56:49,594 - mmcv - INFO - load model from: torchvision://resnet50 2022-08-12 18:56:49,594 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50 2022-08-12 18:56:49,687 - mmcv - WARNING - The model and loaded state dict do not match exactly unexpected key in source state_dict: fc.weight, fc.bias # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size # Inference # Model model = checkpoint_and_model [ \"model\" ] # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) # Pick your images folder path_to_image_folder = \"../samples/fridge/odFridgeObjects/images\" img_files = get_image_files ( path_to_image_folder ) # Create a dataset with appropriate images imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [ 0 : 2 ]) 2022-08-12 18:56:49,704 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'} 2022-08-12 18:56:49,744 - mmcv - INFO - initialize RetinaHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'retina_cls', 'std': 0.01, 'bias_prob': 0.01}} 2022-08-12 18:56:49,772 - mmcv - INFO - backbone.conv1.weight - torch.Size([64, 3, 7, 7]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,774 - mmcv - INFO - backbone.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,776 - mmcv - INFO - backbone.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,777 - mmcv - INFO - backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,779 - mmcv - INFO - backbone.layer1.0.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,781 - mmcv - INFO - backbone.layer1.0.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,781 - mmcv - INFO - backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,781 - mmcv - INFO - backbone.layer1.0.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,783 - mmcv - INFO - backbone.layer1.0.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,784 - mmcv - INFO - backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,784 - mmcv - INFO - backbone.layer1.0.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,786 - mmcv - INFO - backbone.layer1.0.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,787 - mmcv - INFO - backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,787 - mmcv - INFO - backbone.layer1.0.downsample.1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.0.downsample.1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.1.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.1.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,791 - mmcv - INFO - backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,792 - mmcv - INFO - backbone.layer1.1.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,792 - mmcv - INFO - backbone.layer1.1.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,792 - mmcv - INFO - backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.1.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.1.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.2.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,795 - mmcv - INFO - backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,795 - mmcv - INFO - backbone.layer1.2.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,795 - mmcv - INFO - backbone.layer1.2.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,797 - mmcv - INFO - backbone.layer2.0.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,797 - mmcv - INFO - backbone.layer2.0.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,797 - mmcv - INFO - backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.downsample.1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.0.downsample.1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.1.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.1.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,801 - mmcv - INFO - backbone.layer2.1.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,801 - mmcv - INFO - backbone.layer2.1.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,801 - mmcv - INFO - backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,802 - mmcv - INFO - backbone.layer2.2.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,802 - mmcv - INFO - backbone.layer2.2.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,802 - mmcv - INFO - backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.2.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.3.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.3.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,805 - mmcv - INFO - backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,805 - mmcv - INFO - backbone.layer2.3.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,805 - mmcv - INFO - backbone.layer2.3.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,806 - mmcv - INFO - backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,806 - mmcv - INFO - backbone.layer2.3.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,806 - mmcv - INFO - backbone.layer2.3.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,808 - mmcv - INFO - backbone.layer3.0.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,808 - mmcv - INFO - backbone.layer3.0.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,808 - mmcv - INFO - backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.downsample.1.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,810 - mmcv - INFO - backbone.layer3.0.downsample.1.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,810 - mmcv - INFO - backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,810 - mmcv - INFO - backbone.layer3.1.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,811 - mmcv - INFO - backbone.layer3.1.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,811 - mmcv - INFO - backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,812 - mmcv - INFO - backbone.layer3.1.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,812 - mmcv - INFO - backbone.layer3.1.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,812 - mmcv - INFO - backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,813 - mmcv - INFO - backbone.layer3.1.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,813 - mmcv - INFO - backbone.layer3.1.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,813 - mmcv - INFO - backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,814 - mmcv - INFO - backbone.layer3.2.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,814 - mmcv - INFO - backbone.layer3.2.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,814 - mmcv - INFO - backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,815 - mmcv - INFO - backbone.layer3.2.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,815 - mmcv - INFO - backbone.layer3.2.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,816 - mmcv - INFO - backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,816 - mmcv - INFO - backbone.layer3.2.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,816 - mmcv - INFO - backbone.layer3.2.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,818 - mmcv - INFO - backbone.layer3.3.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,820 - mmcv - INFO - backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,820 - mmcv - INFO - backbone.layer3.4.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,820 - mmcv - INFO - backbone.layer3.4.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,822 - mmcv - INFO - backbone.layer3.4.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,822 - mmcv - INFO - backbone.layer3.4.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,824 - mmcv - INFO - backbone.layer3.5.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,824 - mmcv - INFO - backbone.layer3.5.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,824 - mmcv - INFO - backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,826 - mmcv - INFO - backbone.layer3.5.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,826 - mmcv - INFO - backbone.layer3.5.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,826 - mmcv - INFO - backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,828 - mmcv - INFO - backbone.layer4.0.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,828 - mmcv - INFO - backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,828 - mmcv - INFO - backbone.layer4.0.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,829 - mmcv - INFO - backbone.layer4.0.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,829 - mmcv - INFO - backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,829 - mmcv - INFO - backbone.layer4.0.downsample.1.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.0.downsample.1.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.1.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.1.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,831 - mmcv - INFO - backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,831 - mmcv - INFO - backbone.layer4.1.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,831 - mmcv - INFO - backbone.layer4.1.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,832 - mmcv - INFO - backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,832 - mmcv - INFO - backbone.layer4.1.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,833 - mmcv - INFO - backbone.layer4.1.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,833 - mmcv - INFO - backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,833 - mmcv - INFO - backbone.layer4.2.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,834 - mmcv - INFO - backbone.layer4.2.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,834 - mmcv - INFO - backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,834 - mmcv - INFO - backbone.layer4.2.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,836 - mmcv - INFO - neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,836 - mmcv - INFO - neck.lateral_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,836 - mmcv - INFO - neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,837 - mmcv - INFO - neck.lateral_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,837 - mmcv - INFO - neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,837 - mmcv - INFO - neck.lateral_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,838 - mmcv - INFO - neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,838 - mmcv - INFO - neck.fpn_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,838 - mmcv - INFO - neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,840 - mmcv - INFO - neck.fpn_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,840 - mmcv - INFO - neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,840 - mmcv - INFO - neck.fpn_convs.4.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,841 - mmcv - INFO - bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,841 - mmcv - INFO - bbox_head.cls_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,842 - mmcv - INFO - bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,842 - mmcv - INFO - bbox_head.cls_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,843 - mmcv - INFO - bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,843 - mmcv - INFO - bbox_head.cls_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,843 - mmcv - INFO - bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.cls_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.reg_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,845 - mmcv - INFO - bbox_head.reg_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,845 - mmcv - INFO - bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,845 - mmcv - INFO - bbox_head.reg_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,846 - mmcv - INFO - bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,846 - mmcv - INFO - bbox_head.reg_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,846 - mmcv - INFO - bbox_head.retina_cls.weight - torch.Size([36, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 18:56:49,847 - mmcv - INFO - bbox_head.retina_cls.bias - torch.Size([36]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 18:56:49,847 - mmcv - INFO - bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,847 - mmcv - INFO - bbox_head.retina_reg.bias - torch.Size([36]): NormalInit: mean=0, std=0.01, bias=0 (<module 'icevision.models.mmdet.models.retinanet' from '/notebooks/icevision/models/mmdet/models/retinanet/__init__.py'>, <icevision.models.mmdet.models.retinanet.backbones.backbone_config.MMDetRetinanetBackboneConfig at 0x7fad702b70d0>, <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>, 384) 0%| | 0/2 [00:00<?, ?it/s] Happy Learning! If you need any assistance, feel free to join our forum .","title":"Inference"},{"location":"inference/#inference-using-icevision","text":"","title":"Inference using IceVision"},{"location":"inference/#install-icevision","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master --2022-08-12 18:38:33-- https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2820 (2.8K) [text/plain] Saving to: \u2018icevision_install.sh.2\u2019 icevision_install.s 100%[===================>] 2.75K --.-KB/s in 0s 2022-08-12 18:38:33 (41.2 MB/s) - \u2018icevision_install.sh.2\u2019 saved [2820/2820] # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Installing icevision + dependencices for cuda11 - Installing torch and its dependencies Looking in links: https://download.pytorch.org/whl/torch_stable.html Requirement already satisfied: torch==1.10.0+cu111 in /opt/conda/lib/python3.9/site-packages (1.10.0+cu111) Requirement already satisfied: torchvision==0.11.1+cu111 in /opt/conda/lib/python3.9/site-packages (0.11.1+cu111) Requirement already satisfied: torchtext==0.11.0 in /opt/conda/lib/python3.9/site-packages (0.11.0) Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.10.0+cu111) (4.3.0) Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.1+cu111) (1.23.1) Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision==0.11.1+cu111) (8.4.0) Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchtext==0.11.0) (2.27.1) Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from torchtext==0.11.0) (4.63.0) Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (2.0.4) Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (3.3) Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (1.26.8) Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (2022.6.15) \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing mmcv \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing mmdet \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing mmseg \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing icevision from master \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m- Installing icedata from master \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m \u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66)\u001b[0m\u001b[31m \u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python-headless==4.1.2.30\u001b[0m\u001b[31m \u001b[0micevision installation finished! {'status': 'ok', 'restart': True}","title":"Install IceVision"},{"location":"inference/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import * \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m67\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmseg configs\u001b[0m | \u001b[36micevision.models.mmseg.download_configs\u001b[0m:\u001b[36mdownload_mmseg_configs\u001b[0m:\u001b[36m33\u001b[0m 0%| | 0/334331 [00:00<?, ?B/s]","title":"Imports"},{"location":"inference/#list-of-images-for-inference","text":"Please store your images in a folder, and populate the path_to_folder variable with the corresponding folder name. # Pick your images folder path_to_image_folder = \"./images\" img_files = get_image_files ( path_to_image_folder ) img_files img = PIL . Image . open ( img_files [ 0 ]) img (#5) [Path('images/boy_skating.png'),Path('images/clock_shop.jpeg'),Path('images/donuts.jpeg'),Path('images/toothbrushes.jpeg'),Path('images/umbrellas.jpg')]","title":"List of images for inference"},{"location":"inference/#loading-a-checkpoint-and-creating-the-corresponding-model","text":"The checkpoint file can be either a local file or an URL # checkpoint_path = 'checkpoints/fridge-retinanet-save-checkpoint-full.pth' checkpoint_path = 'http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth' from icevision.models import * # The model is automatically recreated in the evaluation mode. To unset that mode, you only need to pass `eval_mode=Fales`) checkpoint_and_model = model_from_checkpoint ( checkpoint_path , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , img_size = 640 , is_coco = True ) load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth 2022-08-12 19:06:38,025 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'} 2022-08-12 19:06:38,029 - mmcv - INFO - load model from: torchvision://resnet50 2022-08-12 19:06:38,030 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50 2022-08-12 19:06:38,124 - mmcv - WARNING - The model and loaded state dict do not match exactly unexpected key in source state_dict: fc.weight, fc.bias # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type backbone class_map img_size 2022-08-12 19:06:38,142 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'} 2022-08-12 19:06:38,181 - mmcv - INFO - initialize RetinaHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'retina_cls', 'std': 0.01, 'bias_prob': 0.01}} 2022-08-12 19:06:38,223 - mmcv - INFO - backbone.conv1.weight - torch.Size([64, 3, 7, 7]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,223 - mmcv - INFO - backbone.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,224 - mmcv - INFO - backbone.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,224 - mmcv - INFO - backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,225 - mmcv - INFO - backbone.layer1.0.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,225 - mmcv - INFO - backbone.layer1.0.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,226 - mmcv - INFO - backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,226 - mmcv - INFO - backbone.layer1.0.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,227 - mmcv - INFO - backbone.layer1.0.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,228 - mmcv - INFO - backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,228 - mmcv - INFO - backbone.layer1.0.downsample.1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,228 - mmcv - INFO - backbone.layer1.0.downsample.1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,229 - mmcv - INFO - backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,229 - mmcv - INFO - backbone.layer1.1.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,229 - mmcv - INFO - backbone.layer1.1.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,230 - mmcv - INFO - backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,230 - mmcv - INFO - backbone.layer1.1.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,231 - mmcv - INFO - backbone.layer1.1.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,232 - mmcv - INFO - backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,232 - mmcv - INFO - backbone.layer1.2.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,233 - mmcv - INFO - backbone.layer1.2.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,234 - mmcv - INFO - backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,234 - mmcv - INFO - backbone.layer1.2.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,234 - mmcv - INFO - backbone.layer1.2.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,235 - mmcv - INFO - backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,236 - mmcv - INFO - backbone.layer2.0.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,236 - mmcv - INFO - backbone.layer2.0.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,236 - mmcv - INFO - backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,237 - mmcv - INFO - backbone.layer2.0.downsample.1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,238 - mmcv - INFO - backbone.layer2.0.downsample.1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,238 - mmcv - INFO - backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,238 - mmcv - INFO - backbone.layer2.1.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,239 - mmcv - INFO - backbone.layer2.1.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,240 - mmcv - INFO - backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,240 - mmcv - INFO - backbone.layer2.1.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,240 - mmcv - INFO - backbone.layer2.1.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,241 - mmcv - INFO - backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,241 - mmcv - INFO - backbone.layer2.2.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,241 - mmcv - INFO - backbone.layer2.2.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,242 - mmcv - INFO - backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,242 - mmcv - INFO - backbone.layer2.2.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,242 - mmcv - INFO - backbone.layer2.2.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.2.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.2.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,243 - mmcv - INFO - backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,244 - mmcv - INFO - backbone.layer2.3.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,244 - mmcv - INFO - backbone.layer2.3.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,244 - mmcv - INFO - backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,245 - mmcv - INFO - backbone.layer2.3.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,245 - mmcv - INFO - backbone.layer2.3.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,245 - mmcv - INFO - backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer2.3.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer2.3.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,246 - mmcv - INFO - backbone.layer3.0.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,247 - mmcv - INFO - backbone.layer3.0.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,247 - mmcv - INFO - backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,247 - mmcv - INFO - backbone.layer3.0.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,248 - mmcv - INFO - backbone.layer3.0.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,248 - mmcv - INFO - backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,248 - mmcv - INFO - backbone.layer3.0.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,249 - mmcv - INFO - backbone.layer3.0.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,249 - mmcv - INFO - backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,249 - mmcv - INFO - backbone.layer3.0.downsample.1.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,250 - mmcv - INFO - backbone.layer3.0.downsample.1.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,250 - mmcv - INFO - backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,250 - mmcv - INFO - backbone.layer3.1.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,251 - mmcv - INFO - backbone.layer3.1.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,251 - mmcv - INFO - backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,251 - mmcv - INFO - backbone.layer3.1.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,252 - mmcv - INFO - backbone.layer3.1.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,252 - mmcv - INFO - backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,253 - mmcv - INFO - backbone.layer3.1.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,253 - mmcv - INFO - backbone.layer3.1.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,254 - mmcv - INFO - backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,254 - mmcv - INFO - backbone.layer3.2.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,254 - mmcv - INFO - backbone.layer3.2.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,255 - mmcv - INFO - backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,255 - mmcv - INFO - backbone.layer3.2.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,256 - mmcv - INFO - backbone.layer3.2.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,257 - mmcv - INFO - backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,257 - mmcv - INFO - backbone.layer3.3.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,257 - mmcv - INFO - backbone.layer3.3.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,258 - mmcv - INFO - backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,258 - mmcv - INFO - backbone.layer3.3.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,259 - mmcv - INFO - backbone.layer3.3.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,259 - mmcv - INFO - backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,259 - mmcv - INFO - backbone.layer3.3.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,260 - mmcv - INFO - backbone.layer3.3.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,260 - mmcv - INFO - backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,260 - mmcv - INFO - backbone.layer3.4.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,261 - mmcv - INFO - backbone.layer3.4.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,261 - mmcv - INFO - backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,261 - mmcv - INFO - backbone.layer3.4.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,262 - mmcv - INFO - backbone.layer3.4.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,262 - mmcv - INFO - backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,263 - mmcv - INFO - backbone.layer3.4.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,263 - mmcv - INFO - backbone.layer3.4.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,263 - mmcv - INFO - backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,264 - mmcv - INFO - backbone.layer3.5.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,264 - mmcv - INFO - backbone.layer3.5.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,264 - mmcv - INFO - backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,265 - mmcv - INFO - backbone.layer3.5.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,265 - mmcv - INFO - backbone.layer3.5.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,265 - mmcv - INFO - backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,266 - mmcv - INFO - backbone.layer3.5.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,266 - mmcv - INFO - backbone.layer3.5.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,267 - mmcv - INFO - backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,267 - mmcv - INFO - backbone.layer4.0.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,267 - mmcv - INFO - backbone.layer4.0.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,268 - mmcv - INFO - backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,268 - mmcv - INFO - backbone.layer4.0.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,269 - mmcv - INFO - backbone.layer4.0.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,270 - mmcv - INFO - backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,270 - mmcv - INFO - backbone.layer4.0.downsample.1.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,270 - mmcv - INFO - backbone.layer4.0.downsample.1.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,271 - mmcv - INFO - backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,271 - mmcv - INFO - backbone.layer4.1.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,271 - mmcv - INFO - backbone.layer4.1.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,272 - mmcv - INFO - backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,272 - mmcv - INFO - backbone.layer4.1.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,272 - mmcv - INFO - backbone.layer4.1.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,273 - mmcv - INFO - backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,273 - mmcv - INFO - backbone.layer4.1.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,273 - mmcv - INFO - backbone.layer4.1.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,274 - mmcv - INFO - backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,275 - mmcv - INFO - backbone.layer4.2.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,275 - mmcv - INFO - backbone.layer4.2.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,275 - mmcv - INFO - backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,276 - mmcv - INFO - backbone.layer4.2.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,276 - mmcv - INFO - backbone.layer4.2.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 19:06:38,276 - mmcv - INFO - neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,277 - mmcv - INFO - neck.lateral_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,277 - mmcv - INFO - neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,277 - mmcv - INFO - neck.lateral_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,278 - mmcv - INFO - neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,278 - mmcv - INFO - neck.lateral_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,279 - mmcv - INFO - neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,279 - mmcv - INFO - neck.fpn_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,279 - mmcv - INFO - neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,280 - mmcv - INFO - neck.fpn_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,280 - mmcv - INFO - neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,280 - mmcv - INFO - neck.fpn_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 19:06:38,281 - mmcv - INFO - neck.fpn_convs.4.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,282 - mmcv - INFO - bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,282 - mmcv - INFO - bbox_head.cls_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,283 - mmcv - INFO - bbox_head.cls_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,284 - mmcv - INFO - bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,284 - mmcv - INFO - bbox_head.cls_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,284 - mmcv - INFO - bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,285 - mmcv - INFO - bbox_head.reg_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,285 - mmcv - INFO - bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,285 - mmcv - INFO - bbox_head.reg_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,286 - mmcv - INFO - bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,286 - mmcv - INFO - bbox_head.reg_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,286 - mmcv - INFO - bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,287 - mmcv - INFO - bbox_head.reg_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 19:06:38,287 - mmcv - INFO - bbox_head.retina_cls.weight - torch.Size([720, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 19:06:38,287 - mmcv - INFO - bbox_head.retina_cls.bias - torch.Size([720]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 19:06:38,288 - mmcv - INFO - bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 19:06:38,288 - mmcv - INFO - bbox_head.retina_reg.bias - torch.Size([36]): NormalInit: mean=0, std=0.01, bias=0 <module 'icevision.models.mmdet.models.retinanet' from '/notebooks/icevision/models/mmdet/models/retinanet/__init__.py'> <icevision.models.mmdet.models.retinanet.backbones.backbone_config.MMDetRetinanetBackboneConfig at 0x7fad702b70d0> <ClassMap: {'background': 0, 'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 12, 'parking meter': 13, 'bench': 14, 'bird': 15, 'cat': 16, 'dog': 17, 'horse': 18, 'sheep': 19, 'cow': 20, 'elephant': 21, 'bear': 22, 'zebra': 23, 'giraffe': 24, 'backpack': 25, 'umbrella': 26, 'handbag': 27, 'tie': 28, 'suitcase': 29, 'frisbee': 30, 'skis': 31, 'snowboard': 32, 'sports ball': 33, 'kite': 34, 'baseball bat': 35, 'baseball glove': 36, 'skateboard': 37, 'surfboard': 38, 'tennis racket': 39, 'bottle': 40, 'wine glass': 41, 'cup': 42, 'fork': 43, 'knife': 44, 'spoon': 45, 'bowl': 46, 'banana': 47, 'apple': 48, 'sandwich': 49, 'orange': 50, 'broccoli': 51, 'carrot': 52, 'hot dog': 53, 'pizza': 54, 'donut': 55, 'cake': 56, 'chair': 57, 'couch': 58, 'potted plant': 59, 'bed': 60, 'dining table': 61, 'toilet': 62, 'tv': 63, 'laptop': 64, 'mouse': 65, 'remote': 66, 'keyboard': 67, 'cell phone': 68, 'microwave': 69, 'oven': 70, 'toaster': 71, 'sink': 72, 'refrigerator': 73, 'book': 74, 'clock': 75, 'vase': 76, 'scissors': 77, 'teddy bear': 78, 'hair drier': 79, 'toothbrush': 80}> 640","title":"Loading a checkpoint and creating the corresponding model"},{"location":"inference/#get-model-object","text":"model_from_checkpoint(checkpoint_path) returns a dictionary: checkpoint_and_model . The model object is stored in checkpoint_and_model[\"model\"] . # Get model object # The model is automatically set in the evaluation mode model = checkpoint_and_model [ \"model\" ] # Check device device = next ( model . parameters ()) . device device # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) device(type='cpu')","title":"Get Model Object"},{"location":"inference/#single-image-inference","text":"The end2end_detect() not only compute predictions for a single image but also automatically adjust predicted boxes to the original image size img = PIL . Image . open ( img_files [ 0 ]) pred_dict = model_type . end2end_detect ( img , valid_tfms , model , class_map = class_map , detection_threshold = 0.5 ) pred_dict [ 'img' ]","title":"Single Image Inference"},{"location":"inference/#batch-inference","text":"The following option shows to do generate inference for a set of images. The latter is processed in batches. # Create a dataset imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True , detection_threshold = 0.33 ) # You may need to un-comment this in certain environments to preview images: # %matplotlib inline show_preds ( preds = preds [ 0 : 5 ]) 0%| | 0/2 [00:00<?, ?it/s]","title":"Batch Inference"},{"location":"inference/#how-to-export-inferences-as-coco-annotations","text":"These will match the dimensions of the original images. This could be useful if you'd like to create preliminary pseudo-annotations for unlabeled data: from icevision.data.convert_records_to_coco_style import export_batch_inferences_as_coco_annotations info = { \"year\" : \"2022\" , \"version\" : \"1\" , \"description\" : \"Exported from IceVision\" , \"contributor\" : \"Awesome contributor\" , \"url\" : \"https://lazyannotator.fun\" , \"date_created\" : \"2022-08-05T20:13:09+00:00\" } licenses = [ { \"name\" : \"Creative Commons Attribution 4.0\" , \"id\" : 0 , \"url\" : \"https://creativecommons.org/licenses/by/4.0/legalcode\" , } ] export_batch_inferences_as_coco_annotations ( preds = preds , img_files = img_files , transforms = valid_tfms , class_map = class_map , output_filepath = \"../inferences_for_pseudo_labels.json\" , info = info , licenses = licenses , ) New COCO annotation file saved to ../inferences_for_pseudo_labels.json","title":"How to export inferences as COCO annotations"},{"location":"inference/#preview-predictions-on-original-image","text":"# Index of image you'd like to check i = 4 this_pred = preds [ i ] record = this_pred . pred # Draw that image pred_img = draw_record ( record = this_pred , class_map = class_map , display_label = True , display_score = True , display_bbox = True , font_path = get_default_font (), font_size = 12 , label_color = ( \"#FF59D6\" ), return_as_pil_img = True , ) pred_img","title":"Preview predictions on original image:"},{"location":"inference/#how-to-save-a-model-and-its-metadata-in-icevision","text":"When saving a model weights, we could also store the model metadata that are retrieved by the model_from_checkpoint(checkpoint_path) method # How to save a model and its metadata checkpoint_path = 'coco-retinanet-checkpoint-full.pth' save_icevision_checkpoint ( model , model_name = 'mmdet.retinanet' , backbone_name = 'resnet50_fpn_1x' , classes = class_map . get_classes (), img_size = img_size , filename = checkpoint_path , meta = { 'icevision_version' : '0.9.1' })","title":"How to save a model and its metadata in IceVision"},{"location":"inference/#loading-models-already-containing-metadata","text":"If you have saved your model weights with the model metadata, you only need to call model_from_checkpoint(checkpoint_path) : No other arguments ( model_name, backbone_name, classes, img_size ) are needed. All the information is already embedded in the checkpoint file. checkpoint_path = 'https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth' checkpoint_and_model = model_from_checkpoint ( checkpoint_path ) load checkpoint from http path: https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth Downloading: \"https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth\" to /root/.cache/torch/hub/checkpoints/fridge-retinanet-checkpoint-full.pth 0%| | 0.00/139M [00:00<?, ?B/s] 2022-08-12 18:56:49,593 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'} 2022-08-12 18:56:49,594 - mmcv - INFO - load model from: torchvision://resnet50 2022-08-12 18:56:49,594 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50 2022-08-12 18:56:49,687 - mmcv - WARNING - The model and loaded state dict do not match exactly unexpected key in source state_dict: fc.weight, fc.bias # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size # Inference # Model model = checkpoint_and_model [ \"model\" ] # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) # Pick your images folder path_to_image_folder = \"../samples/fridge/odFridgeObjects/images\" img_files = get_image_files ( path_to_image_folder ) # Create a dataset with appropriate images imgs_array = [ PIL . Image . open ( Path ( fname )) for fname in img_files ] infer_ds = Dataset . from_images ( imgs_array , valid_tfms , class_map = class_map ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [ 0 : 2 ]) 2022-08-12 18:56:49,704 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'} 2022-08-12 18:56:49,744 - mmcv - INFO - initialize RetinaHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'retina_cls', 'std': 0.01, 'bias_prob': 0.01}} 2022-08-12 18:56:49,772 - mmcv - INFO - backbone.conv1.weight - torch.Size([64, 3, 7, 7]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,774 - mmcv - INFO - backbone.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,776 - mmcv - INFO - backbone.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,777 - mmcv - INFO - backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,779 - mmcv - INFO - backbone.layer1.0.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,781 - mmcv - INFO - backbone.layer1.0.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,781 - mmcv - INFO - backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,781 - mmcv - INFO - backbone.layer1.0.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,783 - mmcv - INFO - backbone.layer1.0.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,784 - mmcv - INFO - backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,784 - mmcv - INFO - backbone.layer1.0.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,786 - mmcv - INFO - backbone.layer1.0.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,787 - mmcv - INFO - backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,787 - mmcv - INFO - backbone.layer1.0.downsample.1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.0.downsample.1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.1.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,788 - mmcv - INFO - backbone.layer1.1.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,791 - mmcv - INFO - backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,792 - mmcv - INFO - backbone.layer1.1.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,792 - mmcv - INFO - backbone.layer1.1.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,792 - mmcv - INFO - backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.1.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.1.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,793 - mmcv - INFO - backbone.layer1.2.bn1.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.bn1.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.bn2.weight - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,794 - mmcv - INFO - backbone.layer1.2.bn2.bias - torch.Size([64]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,795 - mmcv - INFO - backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,795 - mmcv - INFO - backbone.layer1.2.bn3.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,795 - mmcv - INFO - backbone.layer1.2.bn3.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,796 - mmcv - INFO - backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,797 - mmcv - INFO - backbone.layer2.0.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,797 - mmcv - INFO - backbone.layer2.0.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,797 - mmcv - INFO - backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,798 - mmcv - INFO - backbone.layer2.0.downsample.1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.0.downsample.1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.1.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,799 - mmcv - INFO - backbone.layer2.1.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,800 - mmcv - INFO - backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,801 - mmcv - INFO - backbone.layer2.1.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,801 - mmcv - INFO - backbone.layer2.1.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,801 - mmcv - INFO - backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,802 - mmcv - INFO - backbone.layer2.2.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,802 - mmcv - INFO - backbone.layer2.2.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,802 - mmcv - INFO - backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,803 - mmcv - INFO - backbone.layer2.2.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.2.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.3.bn1.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,804 - mmcv - INFO - backbone.layer2.3.bn1.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,805 - mmcv - INFO - backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,805 - mmcv - INFO - backbone.layer2.3.bn2.weight - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,805 - mmcv - INFO - backbone.layer2.3.bn2.bias - torch.Size([128]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,806 - mmcv - INFO - backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,806 - mmcv - INFO - backbone.layer2.3.bn3.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,806 - mmcv - INFO - backbone.layer2.3.bn3.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,807 - mmcv - INFO - backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,808 - mmcv - INFO - backbone.layer3.0.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,808 - mmcv - INFO - backbone.layer3.0.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,808 - mmcv - INFO - backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,809 - mmcv - INFO - backbone.layer3.0.downsample.1.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,810 - mmcv - INFO - backbone.layer3.0.downsample.1.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,810 - mmcv - INFO - backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,810 - mmcv - INFO - backbone.layer3.1.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,811 - mmcv - INFO - backbone.layer3.1.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,811 - mmcv - INFO - backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,812 - mmcv - INFO - backbone.layer3.1.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,812 - mmcv - INFO - backbone.layer3.1.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,812 - mmcv - INFO - backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,813 - mmcv - INFO - backbone.layer3.1.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,813 - mmcv - INFO - backbone.layer3.1.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,813 - mmcv - INFO - backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,814 - mmcv - INFO - backbone.layer3.2.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,814 - mmcv - INFO - backbone.layer3.2.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,814 - mmcv - INFO - backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,815 - mmcv - INFO - backbone.layer3.2.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,815 - mmcv - INFO - backbone.layer3.2.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,816 - mmcv - INFO - backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,816 - mmcv - INFO - backbone.layer3.2.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,816 - mmcv - INFO - backbone.layer3.2.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,817 - mmcv - INFO - backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,818 - mmcv - INFO - backbone.layer3.3.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,819 - mmcv - INFO - backbone.layer3.3.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,820 - mmcv - INFO - backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,820 - mmcv - INFO - backbone.layer3.4.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,820 - mmcv - INFO - backbone.layer3.4.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,821 - mmcv - INFO - backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,822 - mmcv - INFO - backbone.layer3.4.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,822 - mmcv - INFO - backbone.layer3.4.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.bn1.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.bn1.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,823 - mmcv - INFO - backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,824 - mmcv - INFO - backbone.layer3.5.bn2.weight - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,824 - mmcv - INFO - backbone.layer3.5.bn2.bias - torch.Size([256]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,824 - mmcv - INFO - backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,826 - mmcv - INFO - backbone.layer3.5.bn3.weight - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,826 - mmcv - INFO - backbone.layer3.5.bn3.bias - torch.Size([1024]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,826 - mmcv - INFO - backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,827 - mmcv - INFO - backbone.layer4.0.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,828 - mmcv - INFO - backbone.layer4.0.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,828 - mmcv - INFO - backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,828 - mmcv - INFO - backbone.layer4.0.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,829 - mmcv - INFO - backbone.layer4.0.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,829 - mmcv - INFO - backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,829 - mmcv - INFO - backbone.layer4.0.downsample.1.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.0.downsample.1.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.1.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,830 - mmcv - INFO - backbone.layer4.1.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,831 - mmcv - INFO - backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,831 - mmcv - INFO - backbone.layer4.1.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,831 - mmcv - INFO - backbone.layer4.1.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,832 - mmcv - INFO - backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,832 - mmcv - INFO - backbone.layer4.1.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,833 - mmcv - INFO - backbone.layer4.1.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,833 - mmcv - INFO - backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,833 - mmcv - INFO - backbone.layer4.2.bn1.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,834 - mmcv - INFO - backbone.layer4.2.bn1.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,834 - mmcv - INFO - backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,834 - mmcv - INFO - backbone.layer4.2.bn2.weight - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.bn2.bias - torch.Size([512]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.bn3.weight - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,835 - mmcv - INFO - backbone.layer4.2.bn3.bias - torch.Size([2048]): PretrainedInit: load from torchvision://resnet50 2022-08-12 18:56:49,836 - mmcv - INFO - neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,836 - mmcv - INFO - neck.lateral_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,836 - mmcv - INFO - neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,837 - mmcv - INFO - neck.lateral_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,837 - mmcv - INFO - neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,837 - mmcv - INFO - neck.lateral_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,838 - mmcv - INFO - neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,838 - mmcv - INFO - neck.fpn_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,838 - mmcv - INFO - neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,839 - mmcv - INFO - neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,840 - mmcv - INFO - neck.fpn_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,840 - mmcv - INFO - neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): XavierInit: gain=1, distribution=uniform, bias=0 2022-08-12 18:56:49,840 - mmcv - INFO - neck.fpn_convs.4.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,841 - mmcv - INFO - bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,841 - mmcv - INFO - bbox_head.cls_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,842 - mmcv - INFO - bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,842 - mmcv - INFO - bbox_head.cls_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,843 - mmcv - INFO - bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,843 - mmcv - INFO - bbox_head.cls_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,843 - mmcv - INFO - bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.cls_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.reg_convs.0.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,844 - mmcv - INFO - bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,845 - mmcv - INFO - bbox_head.reg_convs.1.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,845 - mmcv - INFO - bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,845 - mmcv - INFO - bbox_head.reg_convs.2.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,846 - mmcv - INFO - bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,846 - mmcv - INFO - bbox_head.reg_convs.3.conv.bias - torch.Size([256]): The value is the same before and after calling `init_weights` of RetinaNet 2022-08-12 18:56:49,846 - mmcv - INFO - bbox_head.retina_cls.weight - torch.Size([36, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 18:56:49,847 - mmcv - INFO - bbox_head.retina_cls.bias - torch.Size([36]): NormalInit: mean=0, std=0.01, bias=-4.59511985013459 2022-08-12 18:56:49,847 - mmcv - INFO - bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): NormalInit: mean=0, std=0.01, bias=0 2022-08-12 18:56:49,847 - mmcv - INFO - bbox_head.retina_reg.bias - torch.Size([36]): NormalInit: mean=0, std=0.01, bias=0 (<module 'icevision.models.mmdet.models.retinanet' from '/notebooks/icevision/models/mmdet/models/retinanet/__init__.py'>, <icevision.models.mmdet.models.retinanet.backbones.backbone_config.MMDetRetinanetBackboneConfig at 0x7fad702b70d0>, <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>, 384) 0%| | 0/2 [00:00<?, ?it/s]","title":"Loading models already containing metadata"},{"location":"inference/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"install/","text":"$ pip install torch == 1 .10.0+cu102 torchvision == 0 .11.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html $ pip install mmcv-full == 1 .3.17 -f https://download.openmmlab.com/mmcv/dist/cu102/torch1.10.0/index.html $ pip install mmdet == 2 .17.0 $ pip install icevision [ all ] Important We currently only support Linux/MacOS installations installation using pip torch Depending on what version of cuda driver you'd like to use, you can install different versions of torch builds. If you're not sure which version to choose, we advise to use the current torch default cuda-10.2 cuda-10.2 cuda-11.1 cpu pip install torch==1.10.0+cu102 torchvision==0.11.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html pip install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html pip install torch==1.10.0+cpu torchvision==0.11.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html checking your torch - cuda version To see what version of torch and cuda is installed in your current environment, run: python -c \"import torch;print(torch.__version__, torch.version.cuda)\" output: 1.10.1+cu102 10.2 Your installed torch version will determine which version of mmcv-full you can install. mmcv-full (optional) Installing mmcv-full is optional, yet it will let you unleash the full potential of icevision and allow you to use the large library of models available in mmdet , therefore we strongly recommend doing it. cuda-10.2 cuda-11.1 cpu pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu102/torch1.10.0/index.html pip install mmdet==2.17.0 pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html pip install mmdet==2.17.0 pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.10.0/index.html pip install mmdet==2.17.0 testing mmcv installation Installing mmcv-full can be tricky as it depends on both the exact torch and cuda version. We highly recommend that you test your installation. You can verify it by executing the following command inside your virtual environment: curl -sSL https://raw.githubusercontent.com/open-mmlab/mmcv/master/.dev_scripts/check_installation.py | python - If everything went fine, you should see something like the following: Start checking the installation of mmcv-full ... CPU ops were compiled successfully. CUDA ops were compiled successfully. mmcv-full has been installed successfully. Environment information: ----------------------------------------------------------- sys.platform: linux Python: 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] CUDA available: True GPU 0: GeForce RTX 2060 CUDA_HOME: /usr/local/cuda NVCC: Build cuda_11.1.TC455_06.29069683_0 GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 PyTorch: 1.10.0+cu111 PyTorch compiling details: PyTorch built with: - GCC 7.3 - C++ Version: 201402 - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740) - OpenMP 201511 (a.k.a. OpenMP 4.5) - LAPACK is enabled (usually provided by MKL) - NNPACK is enabled - CPU capability usage: AVX2 - CUDA Runtime 11.1 - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86 - CuDNN 8.0.5 - Magma 2.5.2 - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, TorchVision: 0.11.1+cu111 OpenCV: 4.5.4 MMCV: 1.3.17 MMCV Compiler: GCC 7.3 MMCV CUDA Compiler: 11.1 ----------------------------------------------------------- icevision Icevision is distributed in 2 different eggs: icevision[all] - recommended - complete icevision package with all dependencies icevision[inference] - minimal dependencies, useful for deployment or simply parsing and viewing your dataset we recommend to install the stable release but if you want to use the most recent, bleeding edge version of the library or would like to contribute, here is how to do it: stable pip install icevision [ all ] bleeding edge pip install git+https://github.com/airctic/icevision.git@master#egg = icevision [ all ] --upgrade editable mode ( for developers ) git clone --depth = 1 https://github.com/airctic/icevision.git cd icevision pip install -e . [ dev ] pre-commit install installing using different cuda version Installing icevision with different cuda version is possible, however it is only recommended for more experienced users. The main constraint here is mmcv-full and torch versions compatibility. In short, torch is build for a specific cuda driver version, mmcv-full on the other hand is distributed for a specific torch build. To see which mmcv-full wheels are available for which versions of torch, check the table at mmcv installation guide . Note running pip install icevision will install icevision[inference] by default installation using conda The easiest way to install icevision with all its dependencies is to use our conda environment.yml file. Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. $ curl -O https://raw.githubusercontent.com/airctic/icevision/master/environment.yml $ conda env create -f environment.yml Note please note that installation may take up to 5 mins. Warning using the environment.yml works only on cuda-10.2 enabled devices. If your GPU architecture is Ampere or newer, you have to use the pip installation method. troubleshooting MMCV is not installing with cuda support If you are installing MMCV from the wheel like described above and still are having problems with CUDA you will probably have to compile it locally. Do that by running: pip install mmcv-full If you encounter the following error it means you will have to install CUDA manually (the one that comes with conda installation will not do). OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root. Try installing it with: sudo apt install nvidia-cuda-toolkit Check the installation by running: nvcc --version Error: Failed building wheel for pycocotools If you encounter the following error, when installation process is building wheel for pycocotools: unable to execute 'gcc': No such file or directory error: command 'gcc' failed with exit status 1 Try installing gcc with: sudo apt install gcc Check the installation by running: gcc --version It should return something similar: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. After that try installing icevision again.","title":"Installation"},{"location":"install/#installation-using-pip","text":"","title":"installation using pip"},{"location":"install/#torch","text":"Depending on what version of cuda driver you'd like to use, you can install different versions of torch builds. If you're not sure which version to choose, we advise to use the current torch default cuda-10.2 cuda-10.2 cuda-11.1 cpu pip install torch==1.10.0+cu102 torchvision==0.11.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html pip install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html pip install torch==1.10.0+cpu torchvision==0.11.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html checking your torch - cuda version To see what version of torch and cuda is installed in your current environment, run: python -c \"import torch;print(torch.__version__, torch.version.cuda)\" output: 1.10.1+cu102 10.2 Your installed torch version will determine which version of mmcv-full you can install.","title":"torch"},{"location":"install/#mmcv-full-optional","text":"Installing mmcv-full is optional, yet it will let you unleash the full potential of icevision and allow you to use the large library of models available in mmdet , therefore we strongly recommend doing it. cuda-10.2 cuda-11.1 cpu pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu102/torch1.10.0/index.html pip install mmdet==2.17.0 pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html pip install mmdet==2.17.0 pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.10.0/index.html pip install mmdet==2.17.0 testing mmcv installation Installing mmcv-full can be tricky as it depends on both the exact torch and cuda version. We highly recommend that you test your installation. You can verify it by executing the following command inside your virtual environment: curl -sSL https://raw.githubusercontent.com/open-mmlab/mmcv/master/.dev_scripts/check_installation.py | python - If everything went fine, you should see something like the following: Start checking the installation of mmcv-full ... CPU ops were compiled successfully. CUDA ops were compiled successfully. mmcv-full has been installed successfully. Environment information: ----------------------------------------------------------- sys.platform: linux Python: 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] CUDA available: True GPU 0: GeForce RTX 2060 CUDA_HOME: /usr/local/cuda NVCC: Build cuda_11.1.TC455_06.29069683_0 GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 PyTorch: 1.10.0+cu111 PyTorch compiling details: PyTorch built with: - GCC 7.3 - C++ Version: 201402 - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740) - OpenMP 201511 (a.k.a. OpenMP 4.5) - LAPACK is enabled (usually provided by MKL) - NNPACK is enabled - CPU capability usage: AVX2 - CUDA Runtime 11.1 - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86 - CuDNN 8.0.5 - Magma 2.5.2 - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, TorchVision: 0.11.1+cu111 OpenCV: 4.5.4 MMCV: 1.3.17 MMCV Compiler: GCC 7.3 MMCV CUDA Compiler: 11.1 -----------------------------------------------------------","title":"mmcv-full (optional)"},{"location":"install/#icevision","text":"Icevision is distributed in 2 different eggs: icevision[all] - recommended - complete icevision package with all dependencies icevision[inference] - minimal dependencies, useful for deployment or simply parsing and viewing your dataset we recommend to install the stable release but if you want to use the most recent, bleeding edge version of the library or would like to contribute, here is how to do it: stable pip install icevision [ all ] bleeding edge pip install git+https://github.com/airctic/icevision.git@master#egg = icevision [ all ] --upgrade editable mode ( for developers ) git clone --depth = 1 https://github.com/airctic/icevision.git cd icevision pip install -e . [ dev ] pre-commit install installing using different cuda version Installing icevision with different cuda version is possible, however it is only recommended for more experienced users. The main constraint here is mmcv-full and torch versions compatibility. In short, torch is build for a specific cuda driver version, mmcv-full on the other hand is distributed for a specific torch build. To see which mmcv-full wheels are available for which versions of torch, check the table at mmcv installation guide . Note running pip install icevision will install icevision[inference] by default","title":"icevision"},{"location":"install/#installation-using-conda","text":"The easiest way to install icevision with all its dependencies is to use our conda environment.yml file. Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. $ curl -O https://raw.githubusercontent.com/airctic/icevision/master/environment.yml $ conda env create -f environment.yml Note please note that installation may take up to 5 mins. Warning using the environment.yml works only on cuda-10.2 enabled devices. If your GPU architecture is Ampere or newer, you have to use the pip installation method.","title":"installation using conda"},{"location":"install/#troubleshooting","text":"","title":"troubleshooting"},{"location":"install/#mmcv-is-not-installing-with-cuda-support","text":"If you are installing MMCV from the wheel like described above and still are having problems with CUDA you will probably have to compile it locally. Do that by running: pip install mmcv-full If you encounter the following error it means you will have to install CUDA manually (the one that comes with conda installation will not do). OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root. Try installing it with: sudo apt install nvidia-cuda-toolkit Check the installation by running: nvcc --version","title":"MMCV is not installing with cuda support"},{"location":"install/#error-failed-building-wheel-for-pycocotools","text":"If you encounter the following error, when installation process is building wheel for pycocotools: unable to execute 'gcc': No such file or directory error: command 'gcc' failed with exit status 1 Try installing gcc with: sudo apt install gcc Check the installation by running: gcc --version It should return something similar: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Copyright (C) 2019 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. After that try installing icevision again.","title":"Error: Failed building wheel for pycocotools"},{"location":"mask_rcnn/","text":"[source] model icevision . models . torchvision . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source] train_dl icevision . models . torchvision . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . torchvision . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . torchvision . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . torchvision . mask_rcnn . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . torchvision . mask_rcnn . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . torchvision . mask_rcnn . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"mask_rcnn/#model","text":"icevision . models . torchvision . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[icevision.models.torchvision.backbone_config.TorchvisionBackboneConfig] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source]","title":"model"},{"location":"mask_rcnn/#train_dl","text":"icevision . models . torchvision . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"mask_rcnn/#valid_dl","text":"icevision . models . torchvision . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"mask_rcnn/#infer_dl","text":"icevision . models . torchvision . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"mask_rcnn/#build_train_batch","text":"icevision . models . torchvision . mask_rcnn . dataloaders . build_train_batch ( records ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"mask_rcnn/#build_valid_batch","text":"icevision . models . torchvision . mask_rcnn . dataloaders . build_valid_batch ( records ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"mask_rcnn/#build_infer_batch","text":"icevision . models . torchvision . mask_rcnn . dataloaders . build_infer_batch ( records ) Builds a batch in the format required by the model when doing inference. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be a list of the input records. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"mask_rcnn_fastai/","text":"[source] learner icevision . models . torchvision . mask_rcnn . fastai . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"mask_rcnn_fastai/#learner","text":"icevision . models . torchvision . mask_rcnn . fastai . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"mask_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . torchvision . mask_rcnn . lightning . ModelAdapter ( model , metrics = None ) Lightning module specialized for mask_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"mask_rcnn_lightning/#modeladapter","text":"icevision . models . torchvision . mask_rcnn . lightning . ModelAdapter ( model , metrics = None ) Lightning module specialized for mask_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"mmdet_custom_config/","text":"MMDetection Custom Config When creating an MMDetection model , the model config object is stored in model.cfg and the pretrained weight file path is stored in model.weights_path . In order to update model config attribute, you need to create an cfg_options dictionary: Example: Changing loss weights for loss_bbox and loss_cls You can pass the cfg_options argument when creation a model model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x 'cfg_options' = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 } # Passing cfg_options to the `model()` method to update loss weights model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options ) Install IceVision and IceData The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * Download and prepare a dataset Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) Parse the dataset # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map Creating datasets with augmentations and transforms # Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) Creating a model This shows how to customize a retinanet model by using the cfg_options object: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True, cfg_options=cfg_options) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how to easily customize your model. # Just the config options you would like to update selection = 0 extra_args = {} # Example: Changing both loss weights: loss_bbox, loss_cls if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 , } # Example: changing anchor boxes ratios: elif selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.anchor_generator.ratios' : [ 1.0 ] } cfg_options {'model.bbox_head.loss_bbox.loss_weight': 2, 'model.bbox_head.loss_cls.loss_weight': 0.8} Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model and pass the `cfg_options` dictionary model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options ) print ( model . bbox_head . loss_bbox . loss_weight ) print ( model . bbox_head . loss_cls . loss_weight ) print ( model . bbox_head . anchor_generator . ratios ) print ( model . cfg . model . bbox_head . loss_cls . loss_weight ) print ( model . cfg . model . bbox_head . loss_bbox . loss_weight ) print ( model . cfg . model . bbox_head . anchor_generator . ratios ) 2 0.8 tensor([0.50000, 1.00000, 2.00000]) 0.8 2 [0.5, 1.0, 2.0] Print config settings # You have access to model's weights_path model . weights_path # You also have access to the whole config object model . cfg . __dict__ # Double checking model new attributes for `loss_cls` model . cfg . model . bbox_head . loss_cls # Double checkingmodel new attributes for `anchor_generator` model . cfg . model . bbox_head . anchor_generator Path('checkpoints/retinanet/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth') {'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 0.8, 'type': 'FocalLoss', 'use_sigmoid': True} {'octave_base_scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'} Data Loader The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) Metrics The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning . Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 20 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.229304 0.983039 0.092624 00:06 epoch train_loss valid_loss COCOMetric time 0 0.899269 0.741807 0.156066 00:05 1 0.810302 0.558525 0.365440 00:05 2 0.728987 0.518454 0.478187 00:05 3 0.663082 0.367077 0.609001 00:06 4 0.604693 0.365837 0.717380 00:05 5 0.555109 0.308284 0.837230 00:05 6 0.507140 0.260290 0.871975 00:05 7 0.464969 0.248942 0.881214 00:05 8 0.429737 0.238713 0.864140 00:05 9 0.398711 0.215525 0.916083 00:05 10 0.372950 0.206479 0.911718 00:05 11 0.356441 0.191768 0.909661 00:05 12 0.343991 0.215923 0.911696 00:05 13 0.331874 0.218360 0.887766 00:05 14 0.313435 0.194519 0.908697 00:05 15 0.299692 0.212282 0.900371 00:05 16 0.286332 0.189399 0.916208 00:05 17 0.282384 0.195011 0.911806 00:05 18 0.275653 0.200540 0.904800 00:05 19 0.273256 0.200690 0.904800 00:05 Training using Pytorch Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Using the model - inference and showing results The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Prediction NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) Happy Learning! If you need any assistance, feel free to join our forum .","title":"MMDetection Custom Config"},{"location":"mmdet_custom_config/#mmdetection-custom-config","text":"When creating an MMDetection model , the model config object is stored in model.cfg and the pretrained weight file path is stored in model.weights_path . In order to update model config attribute, you need to create an cfg_options dictionary: Example: Changing loss weights for loss_bbox and loss_cls You can pass the cfg_options argument when creation a model model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x 'cfg_options' = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 } # Passing cfg_options to the `model()` method to update loss weights model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options )","title":"MMDetection Custom Config"},{"location":"mmdet_custom_config/#install-icevision-and-icedata","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision and IceData"},{"location":"mmdet_custom_config/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import *","title":"Imports"},{"location":"mmdet_custom_config/#download-and-prepare-a-dataset","text":"Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir )","title":"Download and prepare a dataset"},{"location":"mmdet_custom_config/#parse-the-dataset","text":"# Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map","title":"Parse the dataset"},{"location":"mmdet_custom_config/#creating-datasets-with-augmentations-and-transforms","text":"# Transforms # size is set to 384 because EfficientDet requires its inputs to be divisible by 128 image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Creating datasets with augmentations and transforms"},{"location":"mmdet_custom_config/#creating-a-model","text":"This shows how to customize a retinanet model by using the cfg_options object: model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True, cfg_options=cfg_options) As pretrained models are used by default, we typically leave this out of the backbone creation step. We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of selection . This shows you how to easily customize your model. # Just the config options you would like to update selection = 0 extra_args = {} # Example: Changing both loss weights: loss_bbox, loss_cls if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.loss_bbox.loss_weight' : 2 , 'model.bbox_head.loss_cls.loss_weight' : 0.8 , } # Example: changing anchor boxes ratios: elif selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x cfg_options = { 'model.bbox_head.anchor_generator.ratios' : [ 1.0 ] } cfg_options {'model.bbox_head.loss_bbox.loss_weight': 2, 'model.bbox_head.loss_cls.loss_weight': 0.8} Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model and pass the `cfg_options` dictionary model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), cfg_options = cfg_options ) print ( model . bbox_head . loss_bbox . loss_weight ) print ( model . bbox_head . loss_cls . loss_weight ) print ( model . bbox_head . anchor_generator . ratios ) print ( model . cfg . model . bbox_head . loss_cls . loss_weight ) print ( model . cfg . model . bbox_head . loss_bbox . loss_weight ) print ( model . cfg . model . bbox_head . anchor_generator . ratios ) 2 0.8 tensor([0.50000, 1.00000, 2.00000]) 0.8 2 [0.5, 1.0, 2.0]","title":"Creating a model"},{"location":"mmdet_custom_config/#print-config-settings","text":"# You have access to model's weights_path model . weights_path # You also have access to the whole config object model . cfg . __dict__ # Double checking model new attributes for `loss_cls` model . cfg . model . bbox_head . loss_cls # Double checkingmodel new attributes for `anchor_generator` model . cfg . model . bbox_head . anchor_generator Path('checkpoints/retinanet/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth') {'alpha': 0.25, 'gamma': 2.0, 'loss_weight': 0.8, 'type': 'FocalLoss', 'use_sigmoid': True} {'octave_base_scale': 4, 'ratios': [0.5, 1.0, 2.0], 'scales_per_octave': 3, 'strides': [8, 16, 32, 64, 128], 'type': 'AnchorGenerator'}","title":"Print config settings"},{"location":"mmdet_custom_config/#data-loader","text":"The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False )","title":"Data Loader"},{"location":"mmdet_custom_config/#metrics","text":"The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"mmdet_custom_config/#training","text":"IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning .","title":"Training"},{"location":"mmdet_custom_config/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find () # For Sparse-RCNN, use lower `end_lr` # learn.lr_find(end_lr=0.005) SuggestedLRs(lr_min=8.317637839354575e-05, lr_steep=0.00010964782268274575) learn . fine_tune ( 20 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.229304 0.983039 0.092624 00:06 epoch train_loss valid_loss COCOMetric time 0 0.899269 0.741807 0.156066 00:05 1 0.810302 0.558525 0.365440 00:05 2 0.728987 0.518454 0.478187 00:05 3 0.663082 0.367077 0.609001 00:06 4 0.604693 0.365837 0.717380 00:05 5 0.555109 0.308284 0.837230 00:05 6 0.507140 0.260290 0.871975 00:05 7 0.464969 0.248942 0.881214 00:05 8 0.429737 0.238713 0.864140 00:05 9 0.398711 0.215525 0.916083 00:05 10 0.372950 0.206479 0.911718 00:05 11 0.356441 0.191768 0.909661 00:05 12 0.343991 0.215923 0.911696 00:05 13 0.331874 0.218360 0.887766 00:05 14 0.313435 0.194519 0.908697 00:05 15 0.299692 0.212282 0.900371 00:05 16 0.286332 0.189399 0.916208 00:05 17 0.282384 0.195011 0.911806 00:05 18 0.275653 0.200540 0.904800 00:05 19 0.273256 0.200690 0.904800 00:05","title":"Training using fastai"},{"location":"mmdet_custom_config/#training-using-pytorch-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Pytorch Lightning"},{"location":"mmdet_custom_config/#using-the-model-inference-and-showing-results","text":"The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Using the model - inference and showing results"},{"location":"mmdet_custom_config/#prediction","text":"NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ])","title":"Prediction"},{"location":"mmdet_custom_config/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"models/","text":"Models Source IceVision offers a large number of models by supporting the following Object Detection Libraries: Torchvision MMDetection Ross Wightman's EfficientDet You will enjoy using our unified API while having access to a large repertoire of SOTA models. Switching models is as easy as changing one word. There is no need to be familiar with all the quirks that new models and implementations introduce. Creating a model In order to create a model, we need to: Choose one of the libraries supported by IceVision Choose one of the models supported by the library Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Selecting a model only takes two simple lines of code. Check out the following examples illustrating some of the models libraries we support: MMDetection model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) Torchvision model_type = models.torchvision.retinanet backbone = model_type.backbones.resnet50_fpn # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) EfficientDet model_type = models.ross.efficientdet backbone = model_type.backbones.tf_lite0 # The efficientdet model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size) YOLOv5 model_type = models.ultralytics.yolov5 backbone = model_type.backbones.small # The yolov5 model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size) As pretrained models are used by default, we typically leave this out.","title":"Models"},{"location":"models/#models","text":"Source IceVision offers a large number of models by supporting the following Object Detection Libraries: Torchvision MMDetection Ross Wightman's EfficientDet You will enjoy using our unified API while having access to a large repertoire of SOTA models. Switching models is as easy as changing one word. There is no need to be familiar with all the quirks that new models and implementations introduce.","title":"Models"},{"location":"models/#creating-a-model","text":"In order to create a model, we need to: Choose one of the libraries supported by IceVision Choose one of the models supported by the library Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Selecting a model only takes two simple lines of code. Check out the following examples illustrating some of the models libraries we support:","title":"Creating a model"},{"location":"models/#mmdetection","text":"model_type = models.mmdet.retinanet backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True) # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map))","title":"MMDetection"},{"location":"models/#torchvision","text":"model_type = models.torchvision.retinanet backbone = model_type.backbones.resnet50_fpn # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map))","title":"Torchvision"},{"location":"models/#efficientdet","text":"model_type = models.ross.efficientdet backbone = model_type.backbones.tf_lite0 # The efficientdet model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size)","title":"EfficientDet"},{"location":"models/#yolov5","text":"model_type = models.ultralytics.yolov5 backbone = model_type.backbones.small # The yolov5 model requires an img_size parameter # Instantiate the mdoel model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), img_size=img_size) As pretrained models are used by default, we typically leave this out.","title":"YOLOv5"},{"location":"negative_samples/","text":"How to use negative examples In some scenarios it might be useful to explicitly show the model images that should be considered background, these are called \"negative examples\" and are images that do not contain any annotations. In this tutorial we're going to be training two raccoons detectors and observe how they perform on images of dogs and cats. One of the models will be trained only with images of raccoons, while the other will also have access to images of dogs and cats (the negative examples). As you might already have imagined, the model that was trained with raccoon images predicts all animals to be raccoons! Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * Raccoon dataset The dataset is stored on github, so a simple git clone will do. ! git clone https : // github . com / datitran / raccoon_dataset fatal: destination path 'raccoon_dataset' already exists and is not an empty directory. The raccoon dataset uses the VOC annotation format, icevision natively supports this format: raccoon_data_dir = Path ( 'raccoon_dataset' ) raccoon_parser = parsers . VOCBBoxParser ( annotations_dir = raccoon_data_dir / 'annotations' , images_dir = raccoon_data_dir / 'images' ) Let's go ahead and parse our data with the default 80% train, 20% valid, split. raccoon_train_records , raccoon_valid_records = raccoon_parser . parse () show_records ( random . choices ( raccoon_train_records , k = 3 ), ncols = 3 , class_map = class_map ) 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s] Pets dataset With icedata we can easily download the pets dataset: pets_data_dir = icedata . pets . load_data () / 'images' Here we have a twist, instead of using the standard parser ( icedata.pets.parser ) which would parse all annotations, we will instead create a custom parser that only parsers the images. Remember the steps for generating a custom parser (check this tutorial for more information). pets_template_record = ObjectDetectionRecord () Parser . generate_template ( pets_template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_img_size(<ImgSize>) record.set_filepath(<Union[str, Path]>) record.detection.add_bboxes(<Sequence[BBox]>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) And now we use that to fill the required methods. We don't have to use the .detection methods since we don't want bboxes for these images. class PetsImageParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . image_filepaths = get_image_files ( data_dir ) def __iter__ ( self ) -> Any : yield from self . image_filepaths def __len__ ( self ) -> int : return len ( self . image_filepaths ) def record_id ( self , o ) -> Hashable : return o . stem def parse_fields ( self , o , record , is_new ): if is_new : record . set_img_size ( get_img_size ( o )) record . set_filepath ( o ) Now we're ready to instantiate the parser and parse the data: pets_parser = PetsImageParser ( pets_template_record , pets_data_dir ) pets_train_records , pets_valid_records = pets_parser . parse () show_records ( random . choices ( pets_train_records , k = 3 ), ncols = 3 ) 0%| | 0/7390 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7390 [00:00<?, ?it/s] Transforms Let's define a simple list of transforms, they are the same for both datasets. presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) Datasets and DataLoaders We create the raccoon dataset and dataloader as normal: model_type = models . ross . efficientdet batch_size = 8 raccoon_train_ds = Dataset ( raccoon_train_records , train_tfms ) raccoon_valid_ds = Dataset ( raccoon_valid_records , valid_tfms ) raccoon_train_dl = model_type . train_dl ( raccoon_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) raccoon_valid_dl = model_type . valid_dl ( raccoon_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) For adding the pets data, we simply have to combine the list of records. Note that the pets dataset contains a lot more images than the raccoon dataset, so we'll get only 100 images for train and 30 for valid, feel free to change these numbers and explore the results! combined_train_ds = Dataset ( raccoon_train_records + pets_train_records [: 100 ], train_tfms ) combined_valid_ds = Dataset ( raccoon_valid_records + pets_valid_records [: 30 ], valid_tfms ) combined_train_dl = model_type . train_dl ( combined_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) combined_valid_dl = model_type . valid_dl ( combined_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) Let's take a look at the combined dataset: show_samples ( random . choices ( combined_train_ds , k = 6 ), class_map = class_map , ncols = 3 ) Metrics As usual, let's stick with our COCOMetric : metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Models We're now ready to train a separate model for each dataset and see how the results change! Raccoons only backbone = model_type . backbones . tf_lite0 raccoon_model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) raccoon_learn = model_type . fastai . learner ( dls = [ raccoon_train_dl , raccoon_valid_dl ], model = raccoon_model , metrics = metrics ) raccoon_learn . lr_find () SuggestedLRs(valley=0.005248074419796467) raccoon_learn . fine_tune ( 30 , 0.005 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.597212 1.354181 0.000307 00:05 1 1.480324 1.354777 0.001507 00:04 2 1.319088 1.254850 0.034810 00:04 3 1.087830 1.081156 0.053211 00:04 4 0.928458 0.898133 0.098449 00:04 /home/ppotrykus/anaconda3/envs/icevision-dev/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes epoch train_loss valid_loss COCOMetric time 0 0.579946 0.861432 0.069980 00:05 1 0.571890 0.824743 0.077679 00:05 2 0.546770 0.768505 0.090791 00:05 3 0.518292 0.676042 0.382004 00:05 4 0.507741 0.644902 0.433709 00:05 5 0.495793 0.692048 0.240042 00:05 6 0.471785 0.577972 0.508806 00:05 7 0.456929 0.624429 0.383962 00:05 8 0.436571 0.644672 0.429864 00:05 9 0.426388 0.581951 0.486592 00:05 10 0.398668 0.562105 0.508463 00:05 11 0.384981 0.508337 0.543620 00:05 12 0.366980 0.497647 0.554876 00:05 13 0.357205 0.494502 0.553882 00:05 14 0.343215 0.491536 0.568322 00:05 15 0.326071 0.490430 0.584215 00:05 16 0.322718 0.501275 0.553775 00:05 17 0.311050 0.486727 0.592019 00:05 18 0.299104 0.476103 0.588887 00:05 19 0.297170 0.500982 0.536913 00:05 20 0.286925 0.482961 0.560512 00:05 21 0.278372 0.486913 0.544892 00:05 22 0.272823 0.497099 0.557630 00:05 23 0.271815 0.485606 0.560342 00:05 24 0.261464 0.475296 0.577226 00:05 25 0.260394 0.479651 0.566950 00:05 26 0.256806 0.477946 0.575284 00:05 27 0.257800 0.480531 0.576717 00:05 28 0.257753 0.484309 0.560253 00:05 29 0.253717 0.482582 0.566893 00:05 If only raccoon photos are showed during training, everything is a raccoon! model_type . show_results ( raccoon_model , combined_valid_ds ) Raccoons + pets combined_model = model_type . model ( backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) combined_learn = model_type . fastai . learner ( dls = [ combined_train_dl , combined_valid_dl ], model = combined_model , metrics = metrics ) combined_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 2.676468 20.114677 0.001932 00:07 1 1.999201 2.396560 0.075933 00:06 2 1.456598 32.068226 0.075432 00:06 3 1.105171 27.473427 0.141161 00:06 4 0.951076 24.071144 0.057629 00:06 epoch train_loss valid_loss COCOMetric time 0 0.610488 9.588224 0.115317 00:08 1 0.580771 6.247436 0.336951 00:07 2 0.559531 4.243937 0.357085 00:08 3 0.520752 1.651748 0.401737 00:08 4 0.519708 2.837212 0.371190 00:08 5 0.521373 6.292931 0.328725 00:09 6 0.476777 2.089493 0.456670 00:08 7 0.499804 6.462561 0.316811 00:08 8 0.981566 21.373594 0.285337 00:09 9 0.986519 12.665498 0.379301 00:08 10 0.717527 8.835299 0.468607 00:08 11 0.584880 7.107671 0.488671 00:08 12 0.497443 6.248025 0.465282 00:08 13 0.495668 4.710539 0.476700 00:08 14 0.484720 1.786158 0.453496 00:08 15 0.419404 2.497736 0.438899 00:08 16 0.379092 2.484356 0.502668 00:08 17 0.362614 1.563345 0.490237 00:09 18 0.337136 1.657581 0.538952 00:09 19 0.316169 1.152076 0.509543 00:08 20 0.304117 1.089841 0.521393 00:07 21 0.299743 1.460610 0.500326 00:08 22 0.278153 1.020550 0.489060 00:08 23 0.265477 0.737581 0.546207 00:08 24 0.252644 0.702136 0.494025 00:09 25 0.248898 0.650576 0.548857 00:08 26 0.273597 0.597172 0.544202 00:08 27 0.263164 0.943378 0.550043 00:08 28 0.243946 0.758281 0.553848 00:08 29 0.245725 0.694360 0.548657 00:09 When negative samples are used during training, the model is much better at understanding what is not a raccoon. model_type . show_results ( combined_model , combined_valid_ds ) Happy Learning! That's it folks! If you need any assistance, feel free to join our forum .","title":"How to use negative samples"},{"location":"negative_samples/#how-to-use-negative-examples","text":"In some scenarios it might be useful to explicitly show the model images that should be considered background, these are called \"negative examples\" and are images that do not contain any annotations. In this tutorial we're going to be training two raccoons detectors and observe how they perform on images of dogs and cats. One of the models will be trained only with images of raccoons, while the other will also have access to images of dogs and cats (the negative examples). As you might already have imagined, the model that was trained with raccoon images predicts all animals to be raccoons!","title":"How to use negative examples"},{"location":"negative_samples/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"negative_samples/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"negative_samples/#raccoon-dataset","text":"The dataset is stored on github, so a simple git clone will do. ! git clone https : // github . com / datitran / raccoon_dataset fatal: destination path 'raccoon_dataset' already exists and is not an empty directory. The raccoon dataset uses the VOC annotation format, icevision natively supports this format: raccoon_data_dir = Path ( 'raccoon_dataset' ) raccoon_parser = parsers . VOCBBoxParser ( annotations_dir = raccoon_data_dir / 'annotations' , images_dir = raccoon_data_dir / 'images' ) Let's go ahead and parse our data with the default 80% train, 20% valid, split. raccoon_train_records , raccoon_valid_records = raccoon_parser . parse () show_records ( random . choices ( raccoon_train_records , k = 3 ), ncols = 3 , class_map = class_map ) 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s]","title":"Raccoon dataset"},{"location":"negative_samples/#pets-dataset","text":"With icedata we can easily download the pets dataset: pets_data_dir = icedata . pets . load_data () / 'images' Here we have a twist, instead of using the standard parser ( icedata.pets.parser ) which would parse all annotations, we will instead create a custom parser that only parsers the images. Remember the steps for generating a custom parser (check this tutorial for more information). pets_template_record = ObjectDetectionRecord () Parser . generate_template ( pets_template_record ) class MyParser(Parser): def __init__(self, template_record): super().__init__(template_record=template_record) def __iter__(self) -> Any: def __len__(self) -> int: def record_id(self, o: Any) -> Hashable: def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): record.set_img_size(<ImgSize>) record.set_filepath(<Union[str, Path]>) record.detection.add_bboxes(<Sequence[BBox]>) record.detection.set_class_map(<ClassMap>) record.detection.add_labels(<Sequence[Hashable]>) And now we use that to fill the required methods. We don't have to use the .detection methods since we don't want bboxes for these images. class PetsImageParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . image_filepaths = get_image_files ( data_dir ) def __iter__ ( self ) -> Any : yield from self . image_filepaths def __len__ ( self ) -> int : return len ( self . image_filepaths ) def record_id ( self , o ) -> Hashable : return o . stem def parse_fields ( self , o , record , is_new ): if is_new : record . set_img_size ( get_img_size ( o )) record . set_filepath ( o ) Now we're ready to instantiate the parser and parse the data: pets_parser = PetsImageParser ( pets_template_record , pets_data_dir ) pets_train_records , pets_valid_records = pets_parser . parse () show_records ( random . choices ( pets_train_records , k = 3 ), ncols = 3 ) 0%| | 0/7390 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/7390 [00:00<?, ?it/s]","title":"Pets dataset"},{"location":"negative_samples/#transforms","text":"Let's define a simple list of transforms, they are the same for both datasets. presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"negative_samples/#datasets-and-dataloaders","text":"We create the raccoon dataset and dataloader as normal: model_type = models . ross . efficientdet batch_size = 8 raccoon_train_ds = Dataset ( raccoon_train_records , train_tfms ) raccoon_valid_ds = Dataset ( raccoon_valid_records , valid_tfms ) raccoon_train_dl = model_type . train_dl ( raccoon_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) raccoon_valid_dl = model_type . valid_dl ( raccoon_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) For adding the pets data, we simply have to combine the list of records. Note that the pets dataset contains a lot more images than the raccoon dataset, so we'll get only 100 images for train and 30 for valid, feel free to change these numbers and explore the results! combined_train_ds = Dataset ( raccoon_train_records + pets_train_records [: 100 ], train_tfms ) combined_valid_ds = Dataset ( raccoon_valid_records + pets_valid_records [: 30 ], valid_tfms ) combined_train_dl = model_type . train_dl ( combined_train_ds , batch_size = batch_size , num_workers = 4 , shuffle = True ) combined_valid_dl = model_type . valid_dl ( combined_valid_ds , batch_size = batch_size , num_workers = 4 , shuffle = False ) Let's take a look at the combined dataset: show_samples ( random . choices ( combined_train_ds , k = 6 ), class_map = class_map , ncols = 3 )","title":"Datasets and DataLoaders"},{"location":"negative_samples/#metrics","text":"As usual, let's stick with our COCOMetric : metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"negative_samples/#models","text":"We're now ready to train a separate model for each dataset and see how the results change!","title":"Models"},{"location":"negative_samples/#raccoons-only","text":"backbone = model_type . backbones . tf_lite0 raccoon_model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) raccoon_learn = model_type . fastai . learner ( dls = [ raccoon_train_dl , raccoon_valid_dl ], model = raccoon_model , metrics = metrics ) raccoon_learn . lr_find () SuggestedLRs(valley=0.005248074419796467) raccoon_learn . fine_tune ( 30 , 0.005 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 1.597212 1.354181 0.000307 00:05 1 1.480324 1.354777 0.001507 00:04 2 1.319088 1.254850 0.034810 00:04 3 1.087830 1.081156 0.053211 00:04 4 0.928458 0.898133 0.098449 00:04 /home/ppotrykus/anaconda3/envs/icevision-dev/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes epoch train_loss valid_loss COCOMetric time 0 0.579946 0.861432 0.069980 00:05 1 0.571890 0.824743 0.077679 00:05 2 0.546770 0.768505 0.090791 00:05 3 0.518292 0.676042 0.382004 00:05 4 0.507741 0.644902 0.433709 00:05 5 0.495793 0.692048 0.240042 00:05 6 0.471785 0.577972 0.508806 00:05 7 0.456929 0.624429 0.383962 00:05 8 0.436571 0.644672 0.429864 00:05 9 0.426388 0.581951 0.486592 00:05 10 0.398668 0.562105 0.508463 00:05 11 0.384981 0.508337 0.543620 00:05 12 0.366980 0.497647 0.554876 00:05 13 0.357205 0.494502 0.553882 00:05 14 0.343215 0.491536 0.568322 00:05 15 0.326071 0.490430 0.584215 00:05 16 0.322718 0.501275 0.553775 00:05 17 0.311050 0.486727 0.592019 00:05 18 0.299104 0.476103 0.588887 00:05 19 0.297170 0.500982 0.536913 00:05 20 0.286925 0.482961 0.560512 00:05 21 0.278372 0.486913 0.544892 00:05 22 0.272823 0.497099 0.557630 00:05 23 0.271815 0.485606 0.560342 00:05 24 0.261464 0.475296 0.577226 00:05 25 0.260394 0.479651 0.566950 00:05 26 0.256806 0.477946 0.575284 00:05 27 0.257800 0.480531 0.576717 00:05 28 0.257753 0.484309 0.560253 00:05 29 0.253717 0.482582 0.566893 00:05 If only raccoon photos are showed during training, everything is a raccoon! model_type . show_results ( raccoon_model , combined_valid_ds )","title":"Raccoons only"},{"location":"negative_samples/#raccoons-pets","text":"combined_model = model_type . model ( backbone ( pretrained = True ), num_classes = len ( raccoon_parser . class_map ), img_size = size ) combined_learn = model_type . fastai . learner ( dls = [ combined_train_dl , combined_valid_dl ], model = combined_model , metrics = metrics ) combined_learn . fine_tune ( 30 , 1e-2 , freeze_epochs = 5 ) epoch train_loss valid_loss COCOMetric time 0 2.676468 20.114677 0.001932 00:07 1 1.999201 2.396560 0.075933 00:06 2 1.456598 32.068226 0.075432 00:06 3 1.105171 27.473427 0.141161 00:06 4 0.951076 24.071144 0.057629 00:06 epoch train_loss valid_loss COCOMetric time 0 0.610488 9.588224 0.115317 00:08 1 0.580771 6.247436 0.336951 00:07 2 0.559531 4.243937 0.357085 00:08 3 0.520752 1.651748 0.401737 00:08 4 0.519708 2.837212 0.371190 00:08 5 0.521373 6.292931 0.328725 00:09 6 0.476777 2.089493 0.456670 00:08 7 0.499804 6.462561 0.316811 00:08 8 0.981566 21.373594 0.285337 00:09 9 0.986519 12.665498 0.379301 00:08 10 0.717527 8.835299 0.468607 00:08 11 0.584880 7.107671 0.488671 00:08 12 0.497443 6.248025 0.465282 00:08 13 0.495668 4.710539 0.476700 00:08 14 0.484720 1.786158 0.453496 00:08 15 0.419404 2.497736 0.438899 00:08 16 0.379092 2.484356 0.502668 00:08 17 0.362614 1.563345 0.490237 00:09 18 0.337136 1.657581 0.538952 00:09 19 0.316169 1.152076 0.509543 00:08 20 0.304117 1.089841 0.521393 00:07 21 0.299743 1.460610 0.500326 00:08 22 0.278153 1.020550 0.489060 00:08 23 0.265477 0.737581 0.546207 00:08 24 0.252644 0.702136 0.494025 00:09 25 0.248898 0.650576 0.548857 00:08 26 0.273597 0.597172 0.544202 00:08 27 0.263164 0.943378 0.550043 00:08 28 0.243946 0.758281 0.553848 00:08 29 0.245725 0.694360 0.548657 00:09 When negative samples are used during training, the model is much better at understanding what is not a raccoon. model_type . show_results ( combined_model , combined_valid_ds )","title":"Raccoons + pets"},{"location":"negative_samples/#happy-learning","text":"That's it folks! If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"ochuman_keypoint_detection/","text":"OCHuman dataset From the OCHuman repo: This dataset focus on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study. Disclaimer : it is currently not possible to run this notebook in Colab right away, given you need to download the OCHuman dataset manually. We advise running the notebook locally, as soon as you get access to the dataset. Installing IceVision Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Defining OCHuman parser from icevision.all import * _ = icedata . ochuman . load_data () \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m MANUALLY download AND unzip the dataset from https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman. You will need the path to the `ochuman.json` annotations file and the `images` directory. \u001b[0m | \u001b[36micedata.datasets.ochuman.data\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m7\u001b[0m Parse data Note : you might need to change the ../../ path used from this point onwards, according to your filesystem (e.g. according to where you stored the dataset). parser = icedata . ochuman . parser ( \"../../OCHuman/ochuman.json\" , \"../../OCHuman/images/\" ) train_records , valid_records = parser . parse ( data_splitter = RandomSplitter ([ 0.8 , 0.2 ]), cache_filepath = \"../../OCHuman/ochuman.pkl\" ) len ( train_records ), len ( valid_records ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLoading cached records from ../../OCHuman/ochuman.pkl\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m113\u001b[0m (4064, 1017) Datasets + augmentations presize = 1024 size = 512 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) len ( train_ds ), len ( valid_ds ) (4064, 1017) Dataloaders model_type = models . torchvision . keypoint_rcnn train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model model = model_type . model ( num_keypoints = 19 ) Train a fastai learner from fastai.callback.tracker import SaveModelCallback learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , cbs = [ SaveModelCallback ()]) learn . lr_find () /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] SuggestedLRs(valley=4.365158383734524e-05) learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 4.737989 4.609861 08:19 Better model found at epoch 0 with valid_loss value: 4.609861373901367. epoch train_loss valid_loss time 0 4.346546 4.307302 09:02 1 4.303442 4.230606 08:46 2 4.201407 4.191602 08:37 3 4.194221 4.123021 08:27 4 4.168465 4.063463 08:32 5 4.112132 4.037125 08:24 6 4.047480 3.952349 08:20 7 3.980796 3.875872 08:29 8 3.898531 3.818884 08:25 9 3.856582 3.771754 08:27 10 3.770988 3.699221 08:24 11 3.736982 3.637545 08:18 12 3.645181 3.561272 08:12 13 3.570732 3.501793 08:20 14 3.529509 3.464969 08:20 15 3.480687 3.416519 08:20 16 3.416651 3.388196 08:26 17 3.375072 3.358102 08:21 18 3.355783 3.351155 08:21 19 3.344901 3.357507 08:17 learn . recorder . plot_loss () Better model found at epoch 0 with valid_loss value: 4.3073015213012695. Better model found at epoch 1 with valid_loss value: 4.230605602264404. Better model found at epoch 2 with valid_loss value: 4.1916022300720215. Better model found at epoch 3 with valid_loss value: 4.123020648956299. Better model found at epoch 4 with valid_loss value: 4.063462734222412. Better model found at epoch 5 with valid_loss value: 4.037125110626221. Better model found at epoch 6 with valid_loss value: 3.9523494243621826. Better model found at epoch 7 with valid_loss value: 3.8758718967437744. Better model found at epoch 8 with valid_loss value: 3.8188838958740234. Better model found at epoch 9 with valid_loss value: 3.771754026412964. Better model found at epoch 10 with valid_loss value: 3.699220657348633. Better model found at epoch 11 with valid_loss value: 3.637545347213745. Better model found at epoch 12 with valid_loss value: 3.561272144317627. Better model found at epoch 13 with valid_loss value: 3.5017926692962646. Better model found at epoch 14 with valid_loss value: 3.464968681335449. Better model found at epoch 15 with valid_loss value: 3.4165189266204834. Better model found at epoch 16 with valid_loss value: 3.388195753097534. Better model found at epoch 17 with valid_loss value: 3.3581018447875977. Better model found at epoch 18 with valid_loss value: 3.3511552810668945. Show model results model_type . show_results ( model , valid_ds ) Save model torch . save ( model . state_dict (), \"../../OCHuman/model.pth\" ) model = model_type . model ( num_keypoints = 19 ) state_dict = torch . load ( \"../../OCHuman/model.pth\" ) model . load_state_dict ( state_dict ) <All keys matched successfully> Running inference on validation set infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) show_preds ( preds = preds [ 68 : 70 ], show = True , display_label = False , figsize = ( 10 , 10 )) 0%| | 0/128 [00:00<?, ?it/s] plot_top_losses #model.train() sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_total\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/1017 [00:00<?, ?it/s] 0%| | 0/128 [00:00<?, ?it/s]","title":"Advanced Keypoint Detection"},{"location":"ochuman_keypoint_detection/#ochuman-dataset","text":"From the OCHuman repo: This dataset focus on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study. Disclaimer : it is currently not possible to run this notebook in Colab right away, given you need to download the OCHuman dataset manually. We advise running the notebook locally, as soon as you get access to the dataset.","title":"OCHuman dataset"},{"location":"ochuman_keypoint_detection/#installing-icevision","text":"Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision"},{"location":"ochuman_keypoint_detection/#defining-ochuman-parser","text":"from icevision.all import * _ = icedata . ochuman . load_data () \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m MANUALLY download AND unzip the dataset from https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman. You will need the path to the `ochuman.json` annotations file and the `images` directory. \u001b[0m | \u001b[36micedata.datasets.ochuman.data\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m7\u001b[0m","title":"Defining OCHuman parser"},{"location":"ochuman_keypoint_detection/#parse-data","text":"Note : you might need to change the ../../ path used from this point onwards, according to your filesystem (e.g. according to where you stored the dataset). parser = icedata . ochuman . parser ( \"../../OCHuman/ochuman.json\" , \"../../OCHuman/images/\" ) train_records , valid_records = parser . parse ( data_splitter = RandomSplitter ([ 0.8 , 0.2 ]), cache_filepath = \"../../OCHuman/ochuman.pkl\" ) len ( train_records ), len ( valid_records ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLoading cached records from ../../OCHuman/ochuman.pkl\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m113\u001b[0m (4064, 1017)","title":"Parse data"},{"location":"ochuman_keypoint_detection/#datasets-augmentations","text":"presize = 1024 size = 512 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) len ( train_ds ), len ( valid_ds ) (4064, 1017)","title":"Datasets + augmentations"},{"location":"ochuman_keypoint_detection/#dataloaders","text":"model_type = models . torchvision . keypoint_rcnn train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"Dataloaders"},{"location":"ochuman_keypoint_detection/#model","text":"model = model_type . model ( num_keypoints = 19 )","title":"Model"},{"location":"ochuman_keypoint_detection/#train-a-fastai-learner","text":"from fastai.callback.tracker import SaveModelCallback learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , cbs = [ SaveModelCallback ()]) learn . lr_find () /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] SuggestedLRs(valley=4.365158383734524e-05) learn . fine_tune ( 20 , 3e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss time 0 4.737989 4.609861 08:19 Better model found at epoch 0 with valid_loss value: 4.609861373901367. epoch train_loss valid_loss time 0 4.346546 4.307302 09:02 1 4.303442 4.230606 08:46 2 4.201407 4.191602 08:37 3 4.194221 4.123021 08:27 4 4.168465 4.063463 08:32 5 4.112132 4.037125 08:24 6 4.047480 3.952349 08:20 7 3.980796 3.875872 08:29 8 3.898531 3.818884 08:25 9 3.856582 3.771754 08:27 10 3.770988 3.699221 08:24 11 3.736982 3.637545 08:18 12 3.645181 3.561272 08:12 13 3.570732 3.501793 08:20 14 3.529509 3.464969 08:20 15 3.480687 3.416519 08:20 16 3.416651 3.388196 08:26 17 3.375072 3.358102 08:21 18 3.355783 3.351155 08:21 19 3.344901 3.357507 08:17 learn . recorder . plot_loss () Better model found at epoch 0 with valid_loss value: 4.3073015213012695. Better model found at epoch 1 with valid_loss value: 4.230605602264404. Better model found at epoch 2 with valid_loss value: 4.1916022300720215. Better model found at epoch 3 with valid_loss value: 4.123020648956299. Better model found at epoch 4 with valid_loss value: 4.063462734222412. Better model found at epoch 5 with valid_loss value: 4.037125110626221. Better model found at epoch 6 with valid_loss value: 3.9523494243621826. Better model found at epoch 7 with valid_loss value: 3.8758718967437744. Better model found at epoch 8 with valid_loss value: 3.8188838958740234. Better model found at epoch 9 with valid_loss value: 3.771754026412964. Better model found at epoch 10 with valid_loss value: 3.699220657348633. Better model found at epoch 11 with valid_loss value: 3.637545347213745. Better model found at epoch 12 with valid_loss value: 3.561272144317627. Better model found at epoch 13 with valid_loss value: 3.5017926692962646. Better model found at epoch 14 with valid_loss value: 3.464968681335449. Better model found at epoch 15 with valid_loss value: 3.4165189266204834. Better model found at epoch 16 with valid_loss value: 3.388195753097534. Better model found at epoch 17 with valid_loss value: 3.3581018447875977. Better model found at epoch 18 with valid_loss value: 3.3511552810668945.","title":"Train a fastai learner"},{"location":"ochuman_keypoint_detection/#show-model-results","text":"model_type . show_results ( model , valid_ds )","title":"Show model results"},{"location":"ochuman_keypoint_detection/#save-model","text":"torch . save ( model . state_dict (), \"../../OCHuman/model.pth\" ) model = model_type . model ( num_keypoints = 19 ) state_dict = torch . load ( \"../../OCHuman/model.pth\" ) model . load_state_dict ( state_dict ) <All keys matched successfully>","title":"Save model"},{"location":"ochuman_keypoint_detection/#running-inference-on-validation-set","text":"infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) show_preds ( preds = preds [ 68 : 70 ], show = True , display_label = False , figsize = ( 10 , 10 )) 0%| | 0/128 [00:00<?, ?it/s]","title":"Running inference on validation set"},{"location":"ochuman_keypoint_detection/#plot_top_losses","text":"#model.train() sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_total\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/1017 [00:00<?, ?it/s] 0%| | 0/128 [00:00<?, ?it/s]","title":"plot_top_losses"},{"location":"parser/","text":"[source] Parser icevision . parsers . Parser ( template_record , class_map = None , idmap = None ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required fields for all parsers are image_id and image_width_height . Arguments idmap Optional[icevision.core.id_map.IDMap] : Maps from filenames to unique ids, pass an IDMap() if you need this information. Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source] parse Parser . parse ( data_splitter = None , autofix = True , show_pbar = True , cache_filepath = None ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. show_pbar bool : Whether or not to show a progress bar while parsing the data. cache_filepath Union[str, pathlib.Path] : Path to save records in pickle format. Defaults to None, e.g. if the user does not specify a path, no saving nor loading happens. Returns A list of records for each split defined by data_splitter .","title":"Parser"},{"location":"parser/#parser","text":"icevision . parsers . Parser ( template_record , class_map = None , idmap = None ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required fields for all parsers are image_id and image_width_height . Arguments idmap Optional[icevision.core.id_map.IDMap] : Maps from filenames to unique ids, pass an IDMap() if you need this information. Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source]","title":"Parser"},{"location":"parser/#parse","text":"Parser . parse ( data_splitter = None , autofix = True , show_pbar = True , cache_filepath = None ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. show_pbar bool : Whether or not to show a progress bar while parsing the data. cache_filepath Union[str, pathlib.Path] : Path to save records in pickle format. Defaults to None, e.g. if the user does not specify a path, no saving nor loading happens. Returns A list of records for each split defined by data_splitter .","title":"parse"},{"location":"plot_top_losses/","text":"The purpose of this notebook is to showcase the newly added plot_top_losses functionality, which allows users to inspect models' results by plotting images sorted by various combinations of losses. This API makes it easy to immediately spot pictures the model struggles the most with, giving the practitioner the opportunity to take swift action to correct this behaviour (remove wrong samples, correct mis-labellings, etc). plot_top_losses is available for all IceVision models, as the below notebook shows. Install IceVision Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Object Detection Load fridge dataset from icevision.all import * # Loading Data url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 1 , num_workers = 4 , shuffle = False ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[33m\u001b[1m\u001b[1mWARNING \u001b[0m\u001b[33m\u001b[1m\u001b[0m - \u001b[33m\u001b[1mThis function will be deprecated, instantiate the concrete classes instead: `VOCBBoxParser`, `VOCMaskParser`\u001b[0m | \u001b[36micevision.parsers.voc_parser\u001b[0m:\u001b[36mvoc\u001b[0m:\u001b[36m17\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] Train faster_rcnn model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map )) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.995082 1.145674 0.000057 00:09 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] epoch train_loss valid_loss COCOMetric time 0 1.193951 1.648449 0.032289 00:13 1 0.985826 0.735072 0.000160 00:10 2 0.924974 0.794665 0.018933 00:10 3 0.893960 0.844737 0.052620 00:10 4 0.893976 0.812259 0.183922 00:11 5 0.880687 0.681532 0.173614 00:09 6 0.858650 0.665555 0.247655 00:10 7 0.818621 0.510320 0.337507 00:10 8 0.776634 0.518227 0.360398 00:10 9 0.736172 0.486833 0.372902 00:09 Run top_plot_losses on faster_rcnn model results Values allowed to pass to sort_by are (for faster_rcnn ): \"loss_classifier\" \"loss_box_reg\" \"loss_objectness\" \"loss_rpn_box_reg\" \"loss_total\" (sum of the previous 4 losses) {\"method\": \"weighted\", \"weights\": {\"loss_box_reg\": 0.25, \"loss_classifier\": 0.25, \"loss_objectness\": 0.25, \"loss_rpn_box_reg\": 0.25,}} (calculates weighted sum of the 4 losses - Note : I have set weights to 0.25 for example purposes) Below we show several ways of invoking the same API on the trained model, sorting samples by different losses combinations. samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_total\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_classifier\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # in this case `loss_weighted` will be equal to `loss_box_reg` by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 1 , \"loss_classifier\" : 0 , \"loss_objectness\" : 0 , \"loss_rpn_box_reg\" : 0 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 0.25 , \"loss_classifier\" : 0.25 , \"loss_objectness\" : 0.25 , \"loss_rpn_box_reg\" : 0.25 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # `losses_stats` contains useful statistics for each computed loss in the dataset losses_stats # we can easily extract losses per image and display them in a pandas DataFrame for further analysis import pandas as pd from icevision.models.interpretation import get_samples_losses loss_per_image = get_samples_losses ( samples_plus_losses ) pd . DataFrame ( loss_per_image ) {'loss_classifier': {'min': 0.06532751768827438, 'max': 0.3570514917373657, 'mean': 0.22153257807860008, '1ile': 0.06532751768827438, '25ile': 0.17143525183200836, '50ile': 0.23105227947235107, '75ile': 0.2811724841594696, '99ile': 0.3570514917373657}, 'loss_box_reg': {'min': 0.04955608770251274, 'max': 0.41760146617889404, 'mean': 0.24227395410147998, '1ile': 0.04955608770251274, '25ile': 0.16739314794540405, '50ile': 0.23787736147642136, '75ile': 0.3278003931045532, '99ile': 0.41760146617889404}, 'loss_objectness': {'min': 0.0017869938164949417, 'max': 0.10180316120386124, 'mean': 0.016076406804271616, '1ile': 0.0017869938164949417, '25ile': 0.007386505138128996, '50ile': 0.010415146127343178, '75ile': 0.018654515966773033, '99ile': 0.10180316120386124}, 'loss_rpn_box_reg': {'min': 0.001359554473310709, 'max': 0.02417410910129547, 'mean': 0.008536153078938905, '1ile': 0.001359554473310709, '25ile': 0.0056922342628240585, '50ile': 0.007792716380208731, '75ile': 0.010672150179743767, '99ile': 0.02417410910129547}, 'loss_total': {'min': 0.11875081108883023, 'max': 0.7295006141066551, 'mean': 0.4884190920632906, '1ile': 0.11875081108883023, '25ile': 0.3626828184351325, '50ile': 0.5204635132104158, '75ile': 0.6233997480012476, '99ile': 0.7295006141066551}} .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg loss_total loss_weighted filepath 0 0.296700 0.417601 0.007554 0.007645 0.729501 0.182375 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/86.jpg 1 0.345282 0.344900 0.021045 0.006102 0.717329 0.179332 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/90.jpg 2 0.357051 0.327800 0.020415 0.008122 0.713388 0.178347 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/80.jpg 3 0.301462 0.354785 0.015013 0.015807 0.687068 0.171767 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/65.jpg 4 0.232049 0.370708 0.019768 0.010672 0.633197 0.158299 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/41.jpg 5 0.296007 0.310065 0.014165 0.008247 0.628484 0.157121 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/81.jpg 6 0.211539 0.393251 0.011715 0.006894 0.623400 0.155850 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/87.jpg 7 0.271977 0.331191 0.012193 0.007269 0.622630 0.155657 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/48.jpg 8 0.281172 0.282685 0.026252 0.014193 0.604303 0.151076 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/37.jpg 9 0.260208 0.324138 0.008174 0.011745 0.604265 0.151066 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/74.jpg 10 0.278057 0.261131 0.018655 0.007381 0.565223 0.141306 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/28.jpg 11 0.235263 0.310668 0.002415 0.007940 0.556287 0.139072 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/46.jpg 12 0.299006 0.214321 0.009115 0.011385 0.533828 0.133457 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/36.jpg 13 0.230056 0.258252 0.008532 0.010260 0.507099 0.126775 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/40.jpg 14 0.220261 0.210022 0.007387 0.005692 0.443363 0.110841 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/115.jpg 15 0.171435 0.145327 0.101803 0.024174 0.442740 0.110685 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/109.jpg 16 0.233022 0.204496 0.003368 0.001360 0.442246 0.110562 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/47.jpg 17 0.202497 0.217503 0.008341 0.010180 0.438521 0.109630 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/31.jpg 18 0.206598 0.164179 0.051936 0.015769 0.438482 0.109621 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/25.jpg 19 0.145833 0.187863 0.018361 0.010626 0.362683 0.090671 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/59.jpg 20 0.118557 0.171712 0.002909 0.001784 0.294962 0.073740 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/127.jpg 21 0.103582 0.167393 0.014592 0.006817 0.292384 0.073096 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/12.jpg 22 0.180912 0.102142 0.002801 0.002640 0.288496 0.072124 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/19.jpg 23 0.118964 0.102546 0.007565 0.004681 0.233756 0.058439 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/110.jpg 24 0.097024 0.074887 0.002125 0.002476 0.176512 0.044128 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/100.jpg 25 0.065328 0.049556 0.001787 0.002080 0.118751 0.029688 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/106.jpg Run top_plot_losses on a efficientdet pretrained (but not finetuned) model extra_args = {} model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = 384 model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"class_loss\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['effdet_total_loss', 'class_loss', 'box_loss']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Instance Segmentation plot_top_losses in action with a mask_rcnn model on the pennfudan dataset # Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x () # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) model = model_type . model ( backbone = backbone , num_classes = icedata . pennfudan . NUM_CLASSES ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 3e-4 , freeze_epochs = 2 ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 1.950177 0.444139 00:32 1 0.936212 0.439992 00:28 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` warnings.warn( epoch train_loss valid_loss time 0 0.374806 0.344675 00:27 1 0.347379 0.350029 00:27 2 0.336716 0.364372 00:25 3 0.329699 0.333309 00:28 4 0.315138 0.344930 00:27 5 0.308043 0.340804 00:27 6 0.289543 0.312908 00:27 7 0.278561 0.327445 00:26 8 0.266237 0.310647 00:25 9 0.262186 0.310190 00:26 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_mask\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_rpn_cls', 'loss_rpn_bbox', 'loss_cls', 'loss_bbox', 'loss_mask']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/34 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s] Keypoint Detection plot_top_losses in action with a keypoint_rcnn model on the biwi dataset model_type = models . torchvision . keypoint_rcnn data_dir = icedata . biwi . load_data () parser = icedata . biwi . parser ( data_dir ) train_records , valid_records = parser . parse () presize = 240 size = 120 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = model_type . train_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = False ) backbone = model_type . backbones . resnet18_fpn ( pretrained = True ) model = model_type . model ( backbone = backbone , num_keypoints = 1 ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 5 , 1e-4 , freeze_epochs = 2 ) 0%| | 0/593774 [00:00<?, ?B/s] 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 9.133101 7.838448 00:57 1 8.182481 6.926270 00:28 epoch train_loss valid_loss time 0 6.294706 5.840428 00:46 1 5.892668 5.038722 00:25 2 5.503245 4.533072 00:27 3 5.199810 4.395931 00:28 4 4.986382 4.209587 00:22 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_keypoint\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/40 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s]","title":"Model Debugging with Plot Top Losses"},{"location":"plot_top_losses/#install-icevision","text":"Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision"},{"location":"plot_top_losses/#object-detection","text":"","title":"Object Detection"},{"location":"plot_top_losses/#load-fridge-dataset","text":"from icevision.all import * # Loading Data url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) model_type = models . torchvision . faster_rcnn backbone = model_type . backbones . resnet50_fpn # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 1 , num_workers = 4 , shuffle = False ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /home/ubuntu/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m \u001b[33m\u001b[1m\u001b[1mWARNING \u001b[0m\u001b[33m\u001b[1m\u001b[0m - \u001b[33m\u001b[1mThis function will be deprecated, instantiate the concrete classes instead: `VOCBBoxParser`, `VOCMaskParser`\u001b[0m | \u001b[36micevision.parsers.voc_parser\u001b[0m:\u001b[36mvoc\u001b[0m:\u001b[36m17\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s]","title":"Load fridge dataset"},{"location":"plot_top_losses/#train-faster_rcnn-model","text":"model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map )) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 3.995082 1.145674 0.000057 00:09 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:2157.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] epoch train_loss valid_loss COCOMetric time 0 1.193951 1.648449 0.032289 00:13 1 0.985826 0.735072 0.000160 00:10 2 0.924974 0.794665 0.018933 00:10 3 0.893960 0.844737 0.052620 00:10 4 0.893976 0.812259 0.183922 00:11 5 0.880687 0.681532 0.173614 00:09 6 0.858650 0.665555 0.247655 00:10 7 0.818621 0.510320 0.337507 00:10 8 0.776634 0.518227 0.360398 00:10 9 0.736172 0.486833 0.372902 00:09","title":"Train faster_rcnn model"},{"location":"plot_top_losses/#run-top_plot_losses-on-faster_rcnn-model-results","text":"Values allowed to pass to sort_by are (for faster_rcnn ): \"loss_classifier\" \"loss_box_reg\" \"loss_objectness\" \"loss_rpn_box_reg\" \"loss_total\" (sum of the previous 4 losses) {\"method\": \"weighted\", \"weights\": {\"loss_box_reg\": 0.25, \"loss_classifier\": 0.25, \"loss_objectness\": 0.25, \"loss_rpn_box_reg\": 0.25,}} (calculates weighted sum of the 4 losses - Note : I have set weights to 0.25 for example purposes) Below we show several ways of invoking the same API on the trained model, sorting samples by different losses combinations. samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_total\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = \"loss_classifier\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # in this case `loss_weighted` will be equal to `loss_box_reg` by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 1 , \"loss_classifier\" : 0 , \"loss_objectness\" : 0 , \"loss_rpn_box_reg\" : 0 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] by = { \"method\" : \"weighted\" , \"weights\" : { \"loss_box_reg\" : 0.25 , \"loss_classifier\" : 0.25 , \"loss_objectness\" : 0.25 , \"loss_rpn_box_reg\" : 0.25 , }, } samples_plus_losses , preds , losses_stats = model_type . interp . plot_top_losses ( model = model , dataset = valid_ds , sort_by = by , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] # `losses_stats` contains useful statistics for each computed loss in the dataset losses_stats # we can easily extract losses per image and display them in a pandas DataFrame for further analysis import pandas as pd from icevision.models.interpretation import get_samples_losses loss_per_image = get_samples_losses ( samples_plus_losses ) pd . DataFrame ( loss_per_image ) {'loss_classifier': {'min': 0.06532751768827438, 'max': 0.3570514917373657, 'mean': 0.22153257807860008, '1ile': 0.06532751768827438, '25ile': 0.17143525183200836, '50ile': 0.23105227947235107, '75ile': 0.2811724841594696, '99ile': 0.3570514917373657}, 'loss_box_reg': {'min': 0.04955608770251274, 'max': 0.41760146617889404, 'mean': 0.24227395410147998, '1ile': 0.04955608770251274, '25ile': 0.16739314794540405, '50ile': 0.23787736147642136, '75ile': 0.3278003931045532, '99ile': 0.41760146617889404}, 'loss_objectness': {'min': 0.0017869938164949417, 'max': 0.10180316120386124, 'mean': 0.016076406804271616, '1ile': 0.0017869938164949417, '25ile': 0.007386505138128996, '50ile': 0.010415146127343178, '75ile': 0.018654515966773033, '99ile': 0.10180316120386124}, 'loss_rpn_box_reg': {'min': 0.001359554473310709, 'max': 0.02417410910129547, 'mean': 0.008536153078938905, '1ile': 0.001359554473310709, '25ile': 0.0056922342628240585, '50ile': 0.007792716380208731, '75ile': 0.010672150179743767, '99ile': 0.02417410910129547}, 'loss_total': {'min': 0.11875081108883023, 'max': 0.7295006141066551, 'mean': 0.4884190920632906, '1ile': 0.11875081108883023, '25ile': 0.3626828184351325, '50ile': 0.5204635132104158, '75ile': 0.6233997480012476, '99ile': 0.7295006141066551}} .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg loss_total loss_weighted filepath 0 0.296700 0.417601 0.007554 0.007645 0.729501 0.182375 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/86.jpg 1 0.345282 0.344900 0.021045 0.006102 0.717329 0.179332 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/90.jpg 2 0.357051 0.327800 0.020415 0.008122 0.713388 0.178347 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/80.jpg 3 0.301462 0.354785 0.015013 0.015807 0.687068 0.171767 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/65.jpg 4 0.232049 0.370708 0.019768 0.010672 0.633197 0.158299 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/41.jpg 5 0.296007 0.310065 0.014165 0.008247 0.628484 0.157121 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/81.jpg 6 0.211539 0.393251 0.011715 0.006894 0.623400 0.155850 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/87.jpg 7 0.271977 0.331191 0.012193 0.007269 0.622630 0.155657 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/48.jpg 8 0.281172 0.282685 0.026252 0.014193 0.604303 0.151076 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/37.jpg 9 0.260208 0.324138 0.008174 0.011745 0.604265 0.151066 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/74.jpg 10 0.278057 0.261131 0.018655 0.007381 0.565223 0.141306 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/28.jpg 11 0.235263 0.310668 0.002415 0.007940 0.556287 0.139072 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/46.jpg 12 0.299006 0.214321 0.009115 0.011385 0.533828 0.133457 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/36.jpg 13 0.230056 0.258252 0.008532 0.010260 0.507099 0.126775 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/40.jpg 14 0.220261 0.210022 0.007387 0.005692 0.443363 0.110841 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/115.jpg 15 0.171435 0.145327 0.101803 0.024174 0.442740 0.110685 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/109.jpg 16 0.233022 0.204496 0.003368 0.001360 0.442246 0.110562 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/47.jpg 17 0.202497 0.217503 0.008341 0.010180 0.438521 0.109630 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/31.jpg 18 0.206598 0.164179 0.051936 0.015769 0.438482 0.109621 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/25.jpg 19 0.145833 0.187863 0.018361 0.010626 0.362683 0.090671 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/59.jpg 20 0.118557 0.171712 0.002909 0.001784 0.294962 0.073740 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/127.jpg 21 0.103582 0.167393 0.014592 0.006817 0.292384 0.073096 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/12.jpg 22 0.180912 0.102142 0.002801 0.002640 0.288496 0.072124 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/19.jpg 23 0.118964 0.102546 0.007565 0.004681 0.233756 0.058439 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/110.jpg 24 0.097024 0.074887 0.002125 0.002476 0.176512 0.044128 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/100.jpg 25 0.065328 0.049556 0.001787 0.002080 0.118751 0.029688 /home/ubuntu/.icevision/data/fridge/odFridgeObjects/images/106.jpg","title":"Run top_plot_losses on faster_rcnn model results"},{"location":"plot_top_losses/#run-top_plot_losses-on-a-efficientdet-pretrained-but-not-finetuned-model","text":"extra_args = {} model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = 384 model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"class_loss\" , n_samples = 4 ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['effdet_total_loss', 'class_loss', 'box_loss']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/26 [00:00<?, ?it/s] 0%| | 0/4 [00:00<?, ?it/s] /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes","title":"Run top_plot_losses on a efficientdet pretrained (but not finetuned) model"},{"location":"plot_top_losses/#instance-segmentation","text":"","title":"Instance Segmentation"},{"location":"plot_top_losses/#plot_top_losses-in-action-with-a-mask_rcnn-model-on-the-pennfudan-dataset","text":"# Loading Data data_dir = icedata . pennfudan . load_data () parser = icedata . pennfudan . parser ( data_dir ) # train_ds, valid_ds = icedata.pennfudan.dataset(data_dir) train_rs , valid_rs = parser . parse () # Transforms image_size = 512 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 1024 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_rs , train_tfms ) valid_ds = Dataset ( valid_rs , valid_tfms ) model_type = models . mmdet . mask_rcnn backbone = model_type . backbones . resnet50_fpn_1x () # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) model = model_type . model ( backbone = backbone , num_classes = icedata . pennfudan . NUM_CLASSES ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 3e-4 , freeze_epochs = 2 ) 0%| | 0/170 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/170 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 1.950177 0.444139 00:32 1 0.936212 0.439992 00:28 /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:324: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` warnings.warn('``grid_anchors`` would be deprecated soon. ' /home/ubuntu/anaconda3/envs/ice/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` warnings.warn( epoch train_loss valid_loss time 0 0.374806 0.344675 00:27 1 0.347379 0.350029 00:27 2 0.336716 0.364372 00:25 3 0.329699 0.333309 00:28 4 0.315138 0.344930 00:27 5 0.308043 0.340804 00:27 6 0.289543 0.312908 00:27 7 0.278561 0.327445 00:26 8 0.266237 0.310647 00:25 9 0.262186 0.310190 00:26 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_mask\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_rpn_cls', 'loss_rpn_bbox', 'loss_cls', 'loss_bbox', 'loss_mask']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/34 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s]","title":"plot_top_losses in action with a mask_rcnn model on the pennfudan dataset"},{"location":"plot_top_losses/#keypoint-detection","text":"","title":"Keypoint Detection"},{"location":"plot_top_losses/#plot_top_losses-in-action-with-a-keypoint_rcnn-model-on-the-biwi-dataset","text":"model_type = models . torchvision . keypoint_rcnn data_dir = icedata . biwi . load_data () parser = icedata . biwi . parser ( data_dir ) train_records , valid_records = parser . parse () presize = 240 size = 120 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize , crop_fn = None ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) train_dl = model_type . train_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( train_ds , batch_size = 32 , num_workers = 4 , shuffle = False ) backbone = model_type . backbones . resnet18_fpn ( pretrained = True ) model = model_type . model ( backbone = backbone , num_keypoints = 1 ) learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 5 , 1e-4 , freeze_epochs = 2 ) 0%| | 0/593774 [00:00<?, ?B/s] 0%| | 0/200 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/200 [00:00<?, ?it/s] epoch train_loss valid_loss time 0 9.133101 7.838448 00:57 1 8.182481 6.926270 00:28 epoch train_loss valid_loss time 0 6.294706 5.840428 00:46 1 5.892668 5.038722 00:25 2 5.503245 4.533072 00:27 3 5.199810 4.395931 00:28 4 4.986382 4.209587 00:22 sorted_samples , sorted_preds , losses_stats = model_type . interp . plot_top_losses ( model , valid_ds , sort_by = \"loss_keypoint\" ) \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mLosses returned by model: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg', 'loss_keypoint']\u001b[0m | \u001b[36micevision.models.interpretation\u001b[0m:\u001b[36mplot_top_losses\u001b[0m:\u001b[36m218\u001b[0m 0%| | 0/40 [00:00<?, ?it/s] 0%| | 0/5 [00:00<?, ?it/s]","title":"plot_top_losses in action with a keypoint_rcnn model on the biwi dataset"},{"location":"progressive_resizing/","text":"Training using progressive resizing Quote from Fastbook: jargon: progressive resizing: Gradually using larger and larger images as you train. Progressive resizing is a very effective technique to train model from scratch or using transfer learning. IceVision now offers a good support for that technique. For more information about the progressive resizing technique, please check out the reference, here below: Fastai Fastbook Chapter: https://github.com/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb Check out the section: Progressive Resizing Paper highlighting the importance of progressive resizing: Rethinking Training from Scratch for Object Detection EfficientNetV2: Smaller Models and Faster Training Introduction This tutorial walk you through the different steps of training the fridge dataset using the progressive resizing technique. The main differences with IceVision standard training are the use of: The get_dataloaders() method # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) replacing dataloaders (corresponding to different image sizes) in either a Fastai Learner object or a Pytorch-Lightning Trainer object as follow: For Fastai: # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls # Standard training learn . lr_find () learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) Pytorch-Lightning: # Replace current dataloaders by the new ones (corresponding to the new size) trainer . train_dataloader = dls [ 0 ] trainer . valid_dataloader = dls [ 1 ] # Standard training trainer . fit ( light_model , dls [ 0 ], dls [ 1 ]) Installing IceVision and IceData Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * from icevision.models.utils import get_dataloaders from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /Users/fra/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map Creating a model # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) Train and Validation Dataset Transforms Initial size: First size (size = 384) # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) # dls[0].dataset[0] ds [ 0 ][ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=384, height=384) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 384x384x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:131.17572064203858, ymin:239.0191364865004, xmax:325.2123962126736, ymax:312.68812907493447)>, <BBox (xmin:82.22909604282495, ymin:82.72545127450111, xmax:181.53168623735334, ymax:288.81880204425784)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1] DataLoader model_type . show_batch ( first ( dls [ 0 ]), ncols = 4 ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = model_type . fastai . learner ( dls = dls , model = model , metrics = metrics , cbs = SaveModelCallback ( monitor = 'COCOMetric' )) learn . lr_find () SuggestedLRs(lr_min=0.00010000000474974513, lr_steep=0.00015848931798245758) First Pass: First Training with the size = 384 learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.303019 1.198495 0.018335 00:07 Better model found at epoch 0 with COCOMetric value: 0.018334512022630832. epoch train_loss valid_loss COCOMetric time 0 1.146912 1.077046 0.076843 00:06 1 1.078911 0.835020 0.172249 00:06 2 0.936218 0.585196 0.317453 00:06 3 0.803767 0.500256 0.409220 00:06 4 0.701060 0.394725 0.598500 00:06 5 0.623153 0.357143 0.649460 00:06 6 0.564359 0.360178 0.691023 00:06 7 0.518424 0.343954 0.707619 00:06 8 0.478071 0.333479 0.735202 00:06 9 0.447511 0.331000 0.739396 00:06 Better model found at epoch 0 with COCOMetric value: 0.07684290821192277. Better model found at epoch 1 with COCOMetric value: 0.17224854342050785. Better model found at epoch 2 with COCOMetric value: 0.31745342072333244. Better model found at epoch 3 with COCOMetric value: 0.4092202239902201. Better model found at epoch 4 with COCOMetric value: 0.5985001096045487. Better model found at epoch 5 with COCOMetric value: 0.6494598763585641. Better model found at epoch 6 with COCOMetric value: 0.6910227038257398. Better model found at epoch 7 with COCOMetric value: 0.7076194302452169. Better model found at epoch 8 with COCOMetric value: 0.735202499994352. Better model found at epoch 9 with COCOMetric value: 0.7393958039433105. Restart resizing from here Subsequent Pass: Subsequent Training with the size = 512 # Second Pass (size = 512) # presize = 640 # size = 512 # Third Pass (size = 640) presize = 768 size = 640 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) dls [ 0 ] . dataset [ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=640, height=640) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 640x640x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:98.14351388888991, ymin:376.93381905417573, xmax:576.7811863685447, ymax:620.2620189083259)>, <BBox (xmin:5.201505318565632, ymin:12.437821266274758, xmax:329.20034254651534, ymax:498.3316159707134)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1] Convert Pytorch DataLoaders to Fastai DataLoaders from icevision.engines.fastai import * fastai_dls = convert_dataloaders_to_fastai ( dls = dls ) # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls print ( fastai_dls [ 0 ]) print ( learn . dls [ 0 ]) learn . lr_find () <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> SuggestedLRs(lr_min=2.2908675418875645e-07, lr_steep=9.12010818865383e-07) learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) # learn.fit_one_cycle(10, 2e-4) epoch train_loss valid_loss COCOMetric time 0 0.200535 0.183777 0.857028 00:13 Better model found at epoch 0 with COCOMetric value: 0.8570280948418122. epoch train_loss valid_loss COCOMetric time 0 0.192144 0.188399 0.846013 00:10 1 0.184424 0.167971 0.865162 00:10 2 0.192361 0.194635 0.853915 00:10 3 0.191424 0.195987 0.837771 00:10 4 0.192338 0.186021 0.828367 00:10 5 0.189835 0.152657 0.869115 00:10 6 0.184088 0.163762 0.859548 00:10 7 0.179976 0.155070 0.868776 00:10 8 0.175261 0.156499 0.867725 00:10 9 0.171284 0.156758 0.863302 00:10 model_type . show_results ( model , ds [ 1 ], detection_threshold = .5 ) Better model found at epoch 0 with COCOMetric value: 0.8460132624669883. Better model found at epoch 1 with COCOMetric value: 0.8651620055945506. Better model found at epoch 5 with COCOMetric value: 0.8691152429678355. Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( ds [ 1 ], batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s] Training using Pytorch Lightning You have to follow the same procedure as for the Fastai example. It is quite similar to the Fastai one except we don't need to convert dataloaders like in Fastai. PL dataloaders are just pytorch dataloaders. # Create a model class LightModel(model_type.lightning.ModelAdapter): def configure_optimizers(self): return SGD(self.parameters(), lr=1e-4) light_model = LightModel(model, metrics=metrics) # Create a trainer trainer = pl.Trainer(max_epochs=10, gpus=1) # First Pass (size = 384) # Transforms presize = 512 size = 384 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) First training trainer.fit(light_model, dls[0], dls[1]) # Second Pass (size = 512) presize = 640 size = 512 # Third Pass (size = 640) # presize = 768 # size = 640 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) # Replace current dataloaders by the new ones (corresponding to the new size) trainer.train_dataloader = dls[0] trainer.valid_dataloader = dls[1] # Subsequent training trainer.fit(light_model, dls[0], dls[1]) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_retinanet_prog_resizing_1.pth' ) Mounted at /content/gdrive Happy Learning! If you need any assistance, feel free to join our forum .","title":"Progressive resizing"},{"location":"progressive_resizing/#training-using-progressive-resizing","text":"Quote from Fastbook: jargon: progressive resizing: Gradually using larger and larger images as you train. Progressive resizing is a very effective technique to train model from scratch or using transfer learning. IceVision now offers a good support for that technique. For more information about the progressive resizing technique, please check out the reference, here below: Fastai Fastbook Chapter: https://github.com/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb Check out the section: Progressive Resizing","title":"Training using progressive resizing"},{"location":"progressive_resizing/#paper-highlighting-the-importance-of-progressive-resizing","text":"Rethinking Training from Scratch for Object Detection EfficientNetV2: Smaller Models and Faster Training","title":"Paper highlighting the importance of progressive resizing:"},{"location":"progressive_resizing/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset using the progressive resizing technique. The main differences with IceVision standard training are the use of: The get_dataloaders() method # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) replacing dataloaders (corresponding to different image sizes) in either a Fastai Learner object or a Pytorch-Lightning Trainer object as follow: For Fastai: # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls # Standard training learn . lr_find () learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) Pytorch-Lightning: # Replace current dataloaders by the new ones (corresponding to the new size) trainer . train_dataloader = dls [ 0 ] trainer . valid_dataloader = dls [ 1 ] # Standard training trainer . fit ( light_model , dls [ 0 ], dls [ 1 ])","title":"Introduction"},{"location":"progressive_resizing/#installing-icevision-and-icedata","text":"Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"progressive_resizing/#imports","text":"from icevision.all import * from icevision.models.utils import get_dataloaders from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mThe mmdet config folder already exists. No need to downloaded it. Path : /Users/fra/.icevision/mmdetection_configs/mmdetection_configs-2.16.0/configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m17\u001b[0m","title":"Imports"},{"location":"progressive_resizing/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) # Parser # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map","title":"Datasets : Fridge Objects dataset"},{"location":"progressive_resizing/#creating-a-model","text":"# Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . retinanet backbone = model_type . backbones . resnet50_fpn_1x elif selection == 1 : # The Retinanet model is also implemented in the torchvision library model_type = models . torchvision . retinanet backbone = model_type . backbones . resnet50_fpn elif selection == 2 : model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 # The efficientdet model requires an img_size parameter extra_args [ 'img_size' ] = image_size elif selection == 3 : model_type = models . ultralytics . yolov5 backbone = model_type . backbones . small # The yolov5 model requires an img_size parameter extra_args [ 'img_size' ] = image_size model_type , backbone , extra_args # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args )","title":"Creating a model"},{"location":"progressive_resizing/#train-and-validation-dataset-transforms","text":"","title":"Train and Validation Dataset Transforms"},{"location":"progressive_resizing/#initial-size-first-size-size-384","text":"# Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # DataLoaders ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) # dls[0].dataset[0] ds [ 0 ][ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=384, height=384) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 384x384x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:131.17572064203858, ymin:239.0191364865004, xmax:325.2123962126736, ymax:312.68812907493447)>, <BBox (xmin:82.22909604282495, ymin:82.72545127450111, xmax:181.53168623735334, ymax:288.81880204425784)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1]","title":"Initial size: First size (size = 384)"},{"location":"progressive_resizing/#dataloader","text":"model_type . show_batch ( first ( dls [ 0 ]), ncols = 4 )","title":"DataLoader"},{"location":"progressive_resizing/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"progressive_resizing/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"progressive_resizing/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = dls , model = model , metrics = metrics , cbs = SaveModelCallback ( monitor = 'COCOMetric' )) learn . lr_find () SuggestedLRs(lr_min=0.00010000000474974513, lr_steep=0.00015848931798245758)","title":"Training using fastai"},{"location":"progressive_resizing/#first-pass-first-training-with-the-size-384","text":"learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 1.303019 1.198495 0.018335 00:07 Better model found at epoch 0 with COCOMetric value: 0.018334512022630832. epoch train_loss valid_loss COCOMetric time 0 1.146912 1.077046 0.076843 00:06 1 1.078911 0.835020 0.172249 00:06 2 0.936218 0.585196 0.317453 00:06 3 0.803767 0.500256 0.409220 00:06 4 0.701060 0.394725 0.598500 00:06 5 0.623153 0.357143 0.649460 00:06 6 0.564359 0.360178 0.691023 00:06 7 0.518424 0.343954 0.707619 00:06 8 0.478071 0.333479 0.735202 00:06 9 0.447511 0.331000 0.739396 00:06 Better model found at epoch 0 with COCOMetric value: 0.07684290821192277. Better model found at epoch 1 with COCOMetric value: 0.17224854342050785. Better model found at epoch 2 with COCOMetric value: 0.31745342072333244. Better model found at epoch 3 with COCOMetric value: 0.4092202239902201. Better model found at epoch 4 with COCOMetric value: 0.5985001096045487. Better model found at epoch 5 with COCOMetric value: 0.6494598763585641. Better model found at epoch 6 with COCOMetric value: 0.6910227038257398. Better model found at epoch 7 with COCOMetric value: 0.7076194302452169. Better model found at epoch 8 with COCOMetric value: 0.735202499994352. Better model found at epoch 9 with COCOMetric value: 0.7393958039433105.","title":"First Pass: First Training with the size = 384"},{"location":"progressive_resizing/#restart-resizing-from-here","text":"","title":"Restart resizing from here"},{"location":"progressive_resizing/#subsequent-pass-subsequent-training-with-the-size-512","text":"# Second Pass (size = 512) # presize = 640 # size = 512 # Third Pass (size = 640) presize = 768 size = 640 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) ds , dls = get_dataloaders ( model_type , [ train_records , valid_records ], [ train_tfms , valid_tfms ], batch_size = 16 , num_workers = 2 ) dls [ 0 ] . dataset [ 0 ] BaseRecord samples = [ ds [ 0 ][ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) common: - Image size ImgSize(width=640, height=640) - Filepath: /root/.icevision/data/fridge/odFridgeObjects/images/112.jpg - Img: 640x640x3 <np.ndarray> Image - Record ID: 15 detection: - BBoxes: [<BBox (xmin:98.14351388888991, ymin:376.93381905417573, xmax:576.7811863685447, ymax:620.2620189083259)>, <BBox (xmin:5.201505318565632, ymin:12.437821266274758, xmax:329.20034254651534, ymax:498.3316159707134)>] - Class Map: <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> - Labels: [4, 1]","title":"Subsequent Pass: Subsequent Training with the size = 512"},{"location":"progressive_resizing/#convert-pytorch-dataloaders-to-fastai-dataloaders","text":"from icevision.engines.fastai import * fastai_dls = convert_dataloaders_to_fastai ( dls = dls ) # Replace current dataloaders by the new ones (corresponding to the new size) learn . dls = fastai_dls print ( fastai_dls [ 0 ]) print ( learn . dls [ 0 ]) learn . lr_find () <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> <icevision.engines.fastai.adapters.convert_dataloader_to_fastai.convert_dataloader_to_fastai.<locals>.FastaiDataLoaderWithCollate object at 0x7fe804b4b710> SuggestedLRs(lr_min=2.2908675418875645e-07, lr_steep=9.12010818865383e-07) learn . fine_tune ( 10 , 1e-4 , freeze_epochs = 1 ) # learn.fit_one_cycle(10, 2e-4) epoch train_loss valid_loss COCOMetric time 0 0.200535 0.183777 0.857028 00:13 Better model found at epoch 0 with COCOMetric value: 0.8570280948418122. epoch train_loss valid_loss COCOMetric time 0 0.192144 0.188399 0.846013 00:10 1 0.184424 0.167971 0.865162 00:10 2 0.192361 0.194635 0.853915 00:10 3 0.191424 0.195987 0.837771 00:10 4 0.192338 0.186021 0.828367 00:10 5 0.189835 0.152657 0.869115 00:10 6 0.184088 0.163762 0.859548 00:10 7 0.179976 0.155070 0.868776 00:10 8 0.175261 0.156499 0.867725 00:10 9 0.171284 0.156758 0.863302 00:10 model_type . show_results ( model , ds [ 1 ], detection_threshold = .5 ) Better model found at epoch 0 with COCOMetric value: 0.8460132624669883. Better model found at epoch 1 with COCOMetric value: 0.8651620055945506. Better model found at epoch 5 with COCOMetric value: 0.8691152429678355.","title":"Convert Pytorch DataLoaders to Fastai DataLoaders"},{"location":"progressive_resizing/#inference","text":"","title":"Inference"},{"location":"progressive_resizing/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = model_type . infer_dl ( ds [ 1 ], batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s]","title":"Predicting a batch of images"},{"location":"progressive_resizing/#training-using-pytorch-lightning","text":"You have to follow the same procedure as for the Fastai example. It is quite similar to the Fastai one except we don't need to convert dataloaders like in Fastai. PL dataloaders are just pytorch dataloaders. # Create a model class LightModel(model_type.lightning.ModelAdapter): def configure_optimizers(self): return SGD(self.parameters(), lr=1e-4) light_model = LightModel(model, metrics=metrics) # Create a trainer trainer = pl.Trainer(max_epochs=10, gpus=1) # First Pass (size = 384) # Transforms presize = 512 size = 384 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) First training trainer.fit(light_model, dls[0], dls[1]) # Second Pass (size = 512) presize = 640 size = 512 # Third Pass (size = 640) # presize = 768 # size = 640 train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]) valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()]) # Dataloaders ds, dls = get_dataloaders(model_type, [train_records, valid_records], [train_tfms, valid_tfms], batch_size=16, num_workers=2) # Replace current dataloaders by the new ones (corresponding to the new size) trainer.train_dataloader = dls[0] trainer.valid_dataloader = dls[1] # Subsequent training trainer.fit(light_model, dls[0], dls[1])","title":"Training using Pytorch Lightning"},{"location":"progressive_resizing/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_retinanet_prog_resizing_1.pth' ) Mounted at /content/gdrive","title":"Saving Model on Google Drive"},{"location":"progressive_resizing/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"readme/","text":"IceVision Documentation The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs . Building the documentation Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"IceVision Documentation"},{"location":"readme/#icevision-documentation","text":"The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs .","title":"IceVision Documentation"},{"location":"readme/#building-the-documentation","text":"Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Building the documentation"},{"location":"readme_mkdocs/","text":"IceVision Documentation The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs . Building the documentation Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Generating Docs"},{"location":"readme_mkdocs/#icevision-documentation","text":"The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs .","title":"IceVision Documentation"},{"location":"readme_mkdocs/#building-the-documentation","text":"Locally install the package as described here From the root directory, cd into the docs/ folder and run: python autogen.py mkdocs serve # Starts a local webserver: localhost:8000","title":"Building the documentation"},{"location":"transition/","text":"How to transition Parser The major modification was in the Parser API, the concept of a ParserMixin was completely removed. Now, instead of starting by defining the parser, we start by defining the record: Before: class MyParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): After: template_record = BaseRecord ( ( FilepathRecordComponent (), InstancesLabelsRecordComponent (), BBoxesRecordComponent (), ) ) We then continue to generate the parser template: Before: MyParser . generate_template () After: Parser . generate_template ( template_record ) And here's how the parser class looks: Before: class ChessParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): def __init__ ( self , data_dir ): self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) super () . __init__ ( class_map = class_map ) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def filepath ( self , o ) -> Union [ str , Path ]: return self . data_dir / 'images' / o . filename def image_width_height ( self , o ) -> Tuple [ int , int ]: return o . width , o . height def labels ( self , o ) -> List [ int ]: return [ o . label ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )] After: class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record ): record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detect . set_class_map ( self . class_map ) record . detect . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detect . add_labels ([ o . label ]) Record The attributes on the record are now separated by task, to access bboxes you now have to do record.detection.bboxes instead of record.bboxes . Common attributes (not specific to one task) can still be accessed directly: record.filepath . You can use print to check the attributes on the record and how to access them: BaseRecord common : - Image size ImgSize ( width = 416 , height = 416 ) - Image ID : 3 - Filepath : / home / lgvaz /. icevision / data / chess_sample / chess_sample - master / images / e79deba8fe520409790b601ad61da4ee_jpg . rf .016 bc04dee292f80d1f975931f32bc21 . jpg - Image : None detection : - BBoxes : [ < BBox ( xmin : 208 , ymin : 88 , xmax : 226 , ymax : 128 ) > ] - Labels : [ 5 ] All fields under common can be accessed directly, the others have to be specified by it's corresponding task.","title":"How to transition"},{"location":"transition/#how-to-transition","text":"","title":"How to transition"},{"location":"transition/#parser","text":"The major modification was in the Parser API, the concept of a ParserMixin was completely removed. Now, instead of starting by defining the parser, we start by defining the record: Before: class MyParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): After: template_record = BaseRecord ( ( FilepathRecordComponent (), InstancesLabelsRecordComponent (), BBoxesRecordComponent (), ) ) We then continue to generate the parser template: Before: MyParser . generate_template () After: Parser . generate_template ( template_record ) And here's how the parser class looks: Before: class ChessParser ( parsers . Parser , parsers . FilepathMixin , parsers . LabelsMixin , parsers . BBoxesMixins , ): def __init__ ( self , data_dir ): self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) super () . __init__ ( class_map = class_map ) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def filepath ( self , o ) -> Union [ str , Path ]: return self . data_dir / 'images' / o . filename def image_width_height ( self , o ) -> Tuple [ int , int ]: return o . width , o . height def labels ( self , o ) -> List [ int ]: return [ o . label ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )] After: class ChessParser ( Parser ): def __init__ ( self , template_record , data_dir ): super () . __init__ ( template_record = template_record ) self . data_dir = data_dir self . df = pd . read_csv ( data_dir / \"annotations.csv\" ) self . class_map = ClassMap ( list ( self . df [ 'label' ] . unique ())) def __iter__ ( self ) -> Any : yield from self . df . itertuples () def __len__ ( self ) -> int : return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . filename def parse_fields ( self , o , record ): record . set_filepath ( self . data_dir / 'images' / o . filename ) record . set_img_size ( ImgSize ( width = o . width , height = o . height )) record . detect . set_class_map ( self . class_map ) record . detect . add_bboxes ([ BBox . from_xyxy ( o . xmin , o . ymin , o . xmax , o . ymax )]) record . detect . add_labels ([ o . label ])","title":"Parser"},{"location":"transition/#record","text":"The attributes on the record are now separated by task, to access bboxes you now have to do record.detection.bboxes instead of record.bboxes . Common attributes (not specific to one task) can still be accessed directly: record.filepath . You can use print to check the attributes on the record and how to access them: BaseRecord common : - Image size ImgSize ( width = 416 , height = 416 ) - Image ID : 3 - Filepath : / home / lgvaz /. icevision / data / chess_sample / chess_sample - master / images / e79deba8fe520409790b601ad61da4ee_jpg . rf .016 bc04dee292f80d1f975931f32bc21 . jpg - Image : None detection : - BBoxes : [ < BBox ( xmin : 208 , ymin : 88 , xmax : 226 , ymax : 128 ) > ] - Labels : [ 5 ] All fields under common can be accessed directly, the others have to be specified by it's corresponding task.","title":"Record"},{"location":"using_fiftyone_in_icevision/","text":"Disclaimer This notebook won't work in Colab, due to an incompatibility with the fiftyone library. Using Fiftyone in IceVision Install IceVision The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master fiftyone is not part of IceVision. We need to install it separately. # Install fiftyone % pip install fiftyone - U # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * from icevision.models import * # Needed for inference later import icedata # Needed for sample data import fiftyone as fo Visualizing datasets The fiftyone integration of IceVision can be used on either IceVision Datasets or IceVision Prediction. Let's start by visualizing a dataset. If you don't know fiftyone yet, visit the website and read about the concepts: https://voxel51.com/docs/fiftyone. Fiftyone Summary Fiftyone is a tool to analyze datasets and detections of all forms. For object detection these concepts are most relevant: Fiftyone is structured into fo.Datasets . So every viewable entity is related to a fo.Dataset . An image is represented by a fo.Sample . After you created a fo.Sample you can add fo.Detections , which is constructed by a list of fo.Detection . Finally, you need to add a fo.Sample to your fo.Dataset and then launch your app by calling fo.launch_app(dataset) . IceVision enables you to create all of these fo objects from IceVision classes. Before we train or execute inference, we need to create an icevision.Dataset . From this dataset, we can create a fo.Dataset . Since fiftyone operates on filepaths, you dataset needs filepath as component and you cannot use Dataset.from_images as it stores the images in RAM. We use the fridge object dataset available from IceData . # List all available fiftyone datasets on your machine: fo . list_datasets () # Download dataset infer_ds_path = icedata . fridge . load_data () train_records , valid_records = icedata . fridge . parser ( infer_ds_path ) . parse () # Set fo dataset name fo_dataset_name = \"inference_dataset\" #RandomSplitter Create fiftyone dataset fo_dataset = data . create_fo_dataset ( valid_records , fo_dataset_name ) # See your new dataset in the lists fo . list_datasets () fo . launch_app ( fo_dataset ) [] ['inference_dataset'] @import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\"); #focontainer-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1 { position: relative; display: block !important; } #foactivate-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1 { font-weight: bold; cursor: pointer; font-size: 24px; border-radius: 3px; text-align: center; padding: 0.5em; color: rgb(255, 255, 255); font-family: \"Palanquin\", sans-serif; position: absolute; left: 50%; top: 50%; width: 160px; margin-left: -80px; margin-top: -23px; background: hsla(210,11%,15%, 0.8); border: none; } #foactivate-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1:focus { outline: none; } #fooverlay-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1 { width: 100%; height: 100%; background: hsla(208, 7%, 46%, 0.7); position: absolute; top: 0; left: 0; display: none; cursor: pointer; } Activate (function() { var container = document.getElementById(\"focontainer-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1\"); var overlay = document.getElementById(\"fooverlay-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1\"); fetch(`http://localhost:5151/notebook?handleId=b4172dc6-4d34-4153-bbdc-6c8b53ad89f1`) .then((response) => response.json()) .then(() => { overlay.addEventListener(\"click\", () => { fetch(`http://localhost:5151/reactivate?handleId=b4172dc6-4d34-4153-bbdc-6c8b53ad89f1`) }); container.addEventListener(\"mouseenter\", () => overlay.style.display = \"block\"); container.addEventListener(\"mouseleave\", () => overlay.style.display = \"none\"); }); })(); Dataset: inference_dataset Media type: image Num samples: 26 Selected samples: 0 Selected labels: 0 Session URL: http://localhost:5151/ Visualizing predictions Besides Datasets, you can also visualize predictions by calling the icevision.data.create_fo_dataset with an icevision.Prediction . The icevision.data.create_fo_dataset function allows you to append prediction to an existing dataset by setting the exist_ok argument to True . When you display images in fiftyone it always refers to the original image. However, Icevision keeps its data already transformed, which means that the bounding box needs to be post-processed to match the original image size. You have two options for this post-processing: Either use IceVisions internal function by using the transformation argument in the icevision.data.create_fo_dataset function Or u se your custom post-process function by using the undo_bbox_tfms_fn argument Loading the fridge model checkpoint_path = 'https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth' checkpoint_and_model = model_from_checkpoint ( checkpoint_path ) # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size # Inference # Model model = checkpoint_and_model [ \"model\" ] # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) # Create dataset infer_ds = Dataset ( valid_records , valid_tfms ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) # Lets add the predictions to our previous dataset. fo_dataset = data . create_fo_dataset ( detections = preds , dataset_name = fo_dataset_name , exist_ok = True , transformations = valid_tfms . tfms_list ) # Use IceVisions automatic postprocess bbox function by adding the tfms_list # List datasets, to see that no new is created fo . list_datasets () fo . launch_app ( fo_dataset ) ['inference_dataset'] @import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\"); #focontainer-fc918db8-8910-4361-bcf0-9b0ceb9e226a { position: relative; display: block !important; } #foactivate-fc918db8-8910-4361-bcf0-9b0ceb9e226a { font-weight: bold; cursor: pointer; font-size: 24px; border-radius: 3px; text-align: center; padding: 0.5em; color: rgb(255, 255, 255); font-family: \"Palanquin\", sans-serif; position: absolute; left: 50%; top: 50%; width: 160px; margin-left: -80px; margin-top: -23px; background: hsla(210,11%,15%, 0.8); border: none; } #foactivate-fc918db8-8910-4361-bcf0-9b0ceb9e226a:focus { outline: none; } #fooverlay-fc918db8-8910-4361-bcf0-9b0ceb9e226a { width: 100%; height: 100%; background: hsla(208, 7%, 46%, 0.7); position: absolute; top: 0; left: 0; display: none; cursor: pointer; } Activate (function() { var container = document.getElementById(\"focontainer-fc918db8-8910-4361-bcf0-9b0ceb9e226a\"); var overlay = document.getElementById(\"fooverlay-fc918db8-8910-4361-bcf0-9b0ceb9e226a\"); fetch(`http://localhost:5151/notebook?handleId=fc918db8-8910-4361-bcf0-9b0ceb9e226a`) .then((response) => response.json()) .then(() => { overlay.addEventListener(\"click\", () => { fetch(`http://localhost:5151/reactivate?handleId=fc918db8-8910-4361-bcf0-9b0ceb9e226a`) }); container.addEventListener(\"mouseenter\", () => overlay.style.display = \"block\"); container.addEventListener(\"mouseleave\", () => overlay.style.display = \"none\"); }); })(); Dataset: inference_dataset Media type: image Num samples: 52 Selected samples: 0 Selected labels: 0 Session URL: http://localhost:5151/ Merging samples You can see that images are not automatically matched by filepaths. Therefore use fo.Dataset.merge_samples . Create your own fo.Dataset with fo.Sample This is handy if you need more control over individual samples. For example, you want to compare several training runs, you can create your own fo.Sample and add records manually to avoid merging samples afterward. We provide 2 functions to create fo.Samples : data.convert_prediction_to_fo_sample data.convert_record_to_fo_sample # Create custom dataset custom_fo_dataset_name = \"custom_fo_dataset\" custom_dataset = fo . Dataset ( custom_fo_dataset_name ) # Load dataset infer_ds_path = icedata . fridge . load_data () train_records , valid_records = icedata . fridge . parser ( infer_ds_path ) . parse () sample_list = [] # Iter over dataset and create samples based on records # The field_name refers to the fields of fiftyone, which is used to structure samples in fiftyone for record in train_records [: 1 ]: sample = fo . Sample ( record . common . filepath ) # Create sample and use it in the function below sample_list . append ( data . convert_record_to_fo_sample ( record = record , field_name = \"train_set\" , sample = sample ) ) # Prediction have their own convert function for pred in preds [: 1 ]: sample_list . append ( data . convert_prediction_to_fo_sample ( prediction = pred , transformations = valid_tfms . tfms_list )) # Print our samples print ( sample_list ) # Add our samples to the dataset custom_dataset . add_samples ( samples = sample_list ) fo . launch_app ( custom_dataset ) 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] [<Sample: { 'id': None, 'media_type': 'image', 'filepath': '/home/laurenz/.icevision/data/fridge/odFridgeObjects/images/94.jpg', 'tags': [], 'metadata': None, 'train_set': <Detections: { 'detections': BaseList([ <Detection: { 'id': '61efa9f66ae4a8a68dd47c92', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'carton', 'bounding_box': BaseList([ 0.036072144288577156, 0.3033033033033033, 0.3226452905811623, 0.506006006006006, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c93', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'water_bottle', 'bounding_box': BaseList([ 0.32665330661322645, 0.3633633633633634, 0.21442885771543085, 0.42492492492492495, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c94', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'can', 'bounding_box': BaseList([ 0.5210420841683366, 0.503003003003003, 0.20641282565130262, 0.2747747747747748, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c95', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'milk_bottle', 'bounding_box': BaseList([ 0.7034068136272545, 0.3963963963963964, 0.20240480961923848, 0.3768768768768769, ]), 'mask': None, 'confidence': None, 'index': None, }>, ]), }>, }>, <Sample: { 'id': None, 'media_type': 'image', 'filepath': '/home/laurenz/.icevision/data/fridge/odFridgeObjects/images/76.jpg', 'tags': [], 'metadata': None, 'ground_truth': <Detections: { 'detections': BaseList([ <Detection: { 'id': '61efa9f66ae4a8a68dd47c96', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'milk_bottle', 'bounding_box': BaseList([ 0.06212424849699399, 0.36936936936936937, 0.24248496993987975, 0.3978978978978979, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c97', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'carton', 'bounding_box': BaseList([ 0.218436873747495, 0.23423423423423423, 0.2565130260521042, 0.4519519519519519, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c98', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'water_bottle', 'bounding_box': BaseList([ 0.11022044088176353, 0.5870870870870871, 0.593186372745491, 0.3078078078078078, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c99', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'can', 'bounding_box': BaseList([ 0.6933867735470942, 0.44744744744744747, 0.2124248496993988, 0.2627627627627628, ]), 'mask': None, 'confidence': None, 'index': None, }>, ]), }>, 'prediction': <Detections: { 'detections': BaseList([ <Detection: { 'id': '61efa9f66ae4a8a68dd47c9a', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'milk_bottle', 'bounding_box': BaseList([ 0.21442885771543085, 0.23123123123123124, 0.26452905811623245, 0.46396396396396394, ]), 'mask': None, 'confidence': 0.9985953, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c9b', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'carton', 'bounding_box': BaseList([ 0.06212424849699399, 0.36936936936936937, 0.24248496993987975, 0.4009009009009009, ]), 'mask': None, 'confidence': 0.99891174, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c9c', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'can', 'bounding_box': BaseList([ 0.6973947895791583, 0.44744744744744747, 0.20440881763527055, 0.2627627627627628, ]), 'mask': None, 'confidence': 0.9997763, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c9d', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'water_bottle', 'bounding_box': BaseList([ 0.11022044088176353, 0.5855855855855856, 0.593186372745491, 0.3153153153153153, ]), 'mask': None, 'confidence': 0.9862601, 'index': None, }>, ]), }>, }>] ['61efa9f66ae4a8a68dd47c9e', '61efa9f66ae4a8a68dd47ca3'] Dataset: custom_fo_dataset Media type: image Num samples: 2 Selected samples: 0 Selected labels: 0 Session URL: http://localhost:5151/ Cleanup fo . delete_dataset ( fo_dataset_name ) fo . delete_dataset ( custom_fo_dataset_name ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Using FiftyOne in icevision"},{"location":"using_fiftyone_in_icevision/#disclaimer","text":"This notebook won't work in Colab, due to an incompatibility with the fiftyone library.","title":"Disclaimer"},{"location":"using_fiftyone_in_icevision/#using-fiftyone-in-icevision","text":"","title":"Using Fiftyone in IceVision"},{"location":"using_fiftyone_in_icevision/#install-icevision","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master fiftyone is not part of IceVision. We need to install it separately. # Install fiftyone % pip install fiftyone - U # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision"},{"location":"using_fiftyone_in_icevision/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import * from icevision.models import * # Needed for inference later import icedata # Needed for sample data import fiftyone as fo","title":"Imports"},{"location":"using_fiftyone_in_icevision/#visualizing-datasets","text":"The fiftyone integration of IceVision can be used on either IceVision Datasets or IceVision Prediction. Let's start by visualizing a dataset. If you don't know fiftyone yet, visit the website and read about the concepts: https://voxel51.com/docs/fiftyone.","title":"Visualizing datasets"},{"location":"using_fiftyone_in_icevision/#fiftyone-summary","text":"Fiftyone is a tool to analyze datasets and detections of all forms. For object detection these concepts are most relevant: Fiftyone is structured into fo.Datasets . So every viewable entity is related to a fo.Dataset . An image is represented by a fo.Sample . After you created a fo.Sample you can add fo.Detections , which is constructed by a list of fo.Detection . Finally, you need to add a fo.Sample to your fo.Dataset and then launch your app by calling fo.launch_app(dataset) . IceVision enables you to create all of these fo objects from IceVision classes. Before we train or execute inference, we need to create an icevision.Dataset . From this dataset, we can create a fo.Dataset . Since fiftyone operates on filepaths, you dataset needs filepath as component and you cannot use Dataset.from_images as it stores the images in RAM. We use the fridge object dataset available from IceData . # List all available fiftyone datasets on your machine: fo . list_datasets () # Download dataset infer_ds_path = icedata . fridge . load_data () train_records , valid_records = icedata . fridge . parser ( infer_ds_path ) . parse () # Set fo dataset name fo_dataset_name = \"inference_dataset\" #RandomSplitter Create fiftyone dataset fo_dataset = data . create_fo_dataset ( valid_records , fo_dataset_name ) # See your new dataset in the lists fo . list_datasets () fo . launch_app ( fo_dataset ) [] ['inference_dataset'] @import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\"); #focontainer-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1 { position: relative; display: block !important; } #foactivate-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1 { font-weight: bold; cursor: pointer; font-size: 24px; border-radius: 3px; text-align: center; padding: 0.5em; color: rgb(255, 255, 255); font-family: \"Palanquin\", sans-serif; position: absolute; left: 50%; top: 50%; width: 160px; margin-left: -80px; margin-top: -23px; background: hsla(210,11%,15%, 0.8); border: none; } #foactivate-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1:focus { outline: none; } #fooverlay-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1 { width: 100%; height: 100%; background: hsla(208, 7%, 46%, 0.7); position: absolute; top: 0; left: 0; display: none; cursor: pointer; } Activate (function() { var container = document.getElementById(\"focontainer-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1\"); var overlay = document.getElementById(\"fooverlay-b4172dc6-4d34-4153-bbdc-6c8b53ad89f1\"); fetch(`http://localhost:5151/notebook?handleId=b4172dc6-4d34-4153-bbdc-6c8b53ad89f1`) .then((response) => response.json()) .then(() => { overlay.addEventListener(\"click\", () => { fetch(`http://localhost:5151/reactivate?handleId=b4172dc6-4d34-4153-bbdc-6c8b53ad89f1`) }); container.addEventListener(\"mouseenter\", () => overlay.style.display = \"block\"); container.addEventListener(\"mouseleave\", () => overlay.style.display = \"none\"); }); })(); Dataset: inference_dataset Media type: image Num samples: 26 Selected samples: 0 Selected labels: 0 Session URL: http://localhost:5151/","title":"Fiftyone Summary"},{"location":"using_fiftyone_in_icevision/#visualizing-predictions","text":"Besides Datasets, you can also visualize predictions by calling the icevision.data.create_fo_dataset with an icevision.Prediction . The icevision.data.create_fo_dataset function allows you to append prediction to an existing dataset by setting the exist_ok argument to True . When you display images in fiftyone it always refers to the original image. However, Icevision keeps its data already transformed, which means that the bounding box needs to be post-processed to match the original image size. You have two options for this post-processing: Either use IceVisions internal function by using the transformation argument in the icevision.data.create_fo_dataset function Or u se your custom post-process function by using the undo_bbox_tfms_fn argument","title":"Visualizing predictions"},{"location":"using_fiftyone_in_icevision/#loading-the-fridge-model","text":"checkpoint_path = 'https://github.com/airctic/model_zoo/releases/download/m6/fridge-retinanet-checkpoint-full.pth' checkpoint_and_model = model_from_checkpoint ( checkpoint_path ) # Just logging the info model_type = checkpoint_and_model [ \"model_type\" ] backbone = checkpoint_and_model [ \"backbone\" ] class_map = checkpoint_and_model [ \"class_map\" ] img_size = checkpoint_and_model [ \"img_size\" ] model_type , backbone , class_map , img_size # Inference # Model model = checkpoint_and_model [ \"model\" ] # Transforms img_size = checkpoint_and_model [ \"img_size\" ] valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( img_size ), tfms . A . Normalize ()]) # Create dataset infer_ds = Dataset ( valid_records , valid_tfms ) # Batch Inference infer_dl = model_type . infer_dl ( infer_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) # Lets add the predictions to our previous dataset. fo_dataset = data . create_fo_dataset ( detections = preds , dataset_name = fo_dataset_name , exist_ok = True , transformations = valid_tfms . tfms_list ) # Use IceVisions automatic postprocess bbox function by adding the tfms_list # List datasets, to see that no new is created fo . list_datasets () fo . launch_app ( fo_dataset ) ['inference_dataset'] @import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\"); #focontainer-fc918db8-8910-4361-bcf0-9b0ceb9e226a { position: relative; display: block !important; } #foactivate-fc918db8-8910-4361-bcf0-9b0ceb9e226a { font-weight: bold; cursor: pointer; font-size: 24px; border-radius: 3px; text-align: center; padding: 0.5em; color: rgb(255, 255, 255); font-family: \"Palanquin\", sans-serif; position: absolute; left: 50%; top: 50%; width: 160px; margin-left: -80px; margin-top: -23px; background: hsla(210,11%,15%, 0.8); border: none; } #foactivate-fc918db8-8910-4361-bcf0-9b0ceb9e226a:focus { outline: none; } #fooverlay-fc918db8-8910-4361-bcf0-9b0ceb9e226a { width: 100%; height: 100%; background: hsla(208, 7%, 46%, 0.7); position: absolute; top: 0; left: 0; display: none; cursor: pointer; } Activate (function() { var container = document.getElementById(\"focontainer-fc918db8-8910-4361-bcf0-9b0ceb9e226a\"); var overlay = document.getElementById(\"fooverlay-fc918db8-8910-4361-bcf0-9b0ceb9e226a\"); fetch(`http://localhost:5151/notebook?handleId=fc918db8-8910-4361-bcf0-9b0ceb9e226a`) .then((response) => response.json()) .then(() => { overlay.addEventListener(\"click\", () => { fetch(`http://localhost:5151/reactivate?handleId=fc918db8-8910-4361-bcf0-9b0ceb9e226a`) }); container.addEventListener(\"mouseenter\", () => overlay.style.display = \"block\"); container.addEventListener(\"mouseleave\", () => overlay.style.display = \"none\"); }); })(); Dataset: inference_dataset Media type: image Num samples: 52 Selected samples: 0 Selected labels: 0 Session URL: http://localhost:5151/","title":"Loading the fridge model"},{"location":"using_fiftyone_in_icevision/#merging-samples","text":"You can see that images are not automatically matched by filepaths. Therefore use fo.Dataset.merge_samples .","title":"Merging samples"},{"location":"using_fiftyone_in_icevision/#create-your-own-fodataset-with-fosample","text":"This is handy if you need more control over individual samples. For example, you want to compare several training runs, you can create your own fo.Sample and add records manually to avoid merging samples afterward. We provide 2 functions to create fo.Samples : data.convert_prediction_to_fo_sample data.convert_record_to_fo_sample # Create custom dataset custom_fo_dataset_name = \"custom_fo_dataset\" custom_dataset = fo . Dataset ( custom_fo_dataset_name ) # Load dataset infer_ds_path = icedata . fridge . load_data () train_records , valid_records = icedata . fridge . parser ( infer_ds_path ) . parse () sample_list = [] # Iter over dataset and create samples based on records # The field_name refers to the fields of fiftyone, which is used to structure samples in fiftyone for record in train_records [: 1 ]: sample = fo . Sample ( record . common . filepath ) # Create sample and use it in the function below sample_list . append ( data . convert_record_to_fo_sample ( record = record , field_name = \"train_set\" , sample = sample ) ) # Prediction have their own convert function for pred in preds [: 1 ]: sample_list . append ( data . convert_prediction_to_fo_sample ( prediction = pred , transformations = valid_tfms . tfms_list )) # Print our samples print ( sample_list ) # Add our samples to the dataset custom_dataset . add_samples ( samples = sample_list ) fo . launch_app ( custom_dataset ) 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] [<Sample: { 'id': None, 'media_type': 'image', 'filepath': '/home/laurenz/.icevision/data/fridge/odFridgeObjects/images/94.jpg', 'tags': [], 'metadata': None, 'train_set': <Detections: { 'detections': BaseList([ <Detection: { 'id': '61efa9f66ae4a8a68dd47c92', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'carton', 'bounding_box': BaseList([ 0.036072144288577156, 0.3033033033033033, 0.3226452905811623, 0.506006006006006, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c93', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'water_bottle', 'bounding_box': BaseList([ 0.32665330661322645, 0.3633633633633634, 0.21442885771543085, 0.42492492492492495, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c94', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'can', 'bounding_box': BaseList([ 0.5210420841683366, 0.503003003003003, 0.20641282565130262, 0.2747747747747748, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c95', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'milk_bottle', 'bounding_box': BaseList([ 0.7034068136272545, 0.3963963963963964, 0.20240480961923848, 0.3768768768768769, ]), 'mask': None, 'confidence': None, 'index': None, }>, ]), }>, }>, <Sample: { 'id': None, 'media_type': 'image', 'filepath': '/home/laurenz/.icevision/data/fridge/odFridgeObjects/images/76.jpg', 'tags': [], 'metadata': None, 'ground_truth': <Detections: { 'detections': BaseList([ <Detection: { 'id': '61efa9f66ae4a8a68dd47c96', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'milk_bottle', 'bounding_box': BaseList([ 0.06212424849699399, 0.36936936936936937, 0.24248496993987975, 0.3978978978978979, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c97', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'carton', 'bounding_box': BaseList([ 0.218436873747495, 0.23423423423423423, 0.2565130260521042, 0.4519519519519519, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c98', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'water_bottle', 'bounding_box': BaseList([ 0.11022044088176353, 0.5870870870870871, 0.593186372745491, 0.3078078078078078, ]), 'mask': None, 'confidence': None, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c99', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'can', 'bounding_box': BaseList([ 0.6933867735470942, 0.44744744744744747, 0.2124248496993988, 0.2627627627627628, ]), 'mask': None, 'confidence': None, 'index': None, }>, ]), }>, 'prediction': <Detections: { 'detections': BaseList([ <Detection: { 'id': '61efa9f66ae4a8a68dd47c9a', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'milk_bottle', 'bounding_box': BaseList([ 0.21442885771543085, 0.23123123123123124, 0.26452905811623245, 0.46396396396396394, ]), 'mask': None, 'confidence': 0.9985953, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c9b', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'carton', 'bounding_box': BaseList([ 0.06212424849699399, 0.36936936936936937, 0.24248496993987975, 0.4009009009009009, ]), 'mask': None, 'confidence': 0.99891174, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c9c', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'can', 'bounding_box': BaseList([ 0.6973947895791583, 0.44744744744744747, 0.20440881763527055, 0.2627627627627628, ]), 'mask': None, 'confidence': 0.9997763, 'index': None, }>, <Detection: { 'id': '61efa9f66ae4a8a68dd47c9d', 'attributes': BaseDict({}), 'tags': BaseList([]), 'label': 'water_bottle', 'bounding_box': BaseList([ 0.11022044088176353, 0.5855855855855856, 0.593186372745491, 0.3153153153153153, ]), 'mask': None, 'confidence': 0.9862601, 'index': None, }>, ]), }>, }>] ['61efa9f66ae4a8a68dd47c9e', '61efa9f66ae4a8a68dd47ca3'] Dataset: custom_fo_dataset Media type: image Num samples: 2 Selected samples: 0 Selected labels: 0 Session URL: http://localhost:5151/","title":"Create your own fo.Dataset with fo.Sample"},{"location":"using_fiftyone_in_icevision/#cleanup","text":"fo . delete_dataset ( fo_dataset_name ) fo . delete_dataset ( custom_fo_dataset_name )","title":"Cleanup"},{"location":"using_fiftyone_in_icevision/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"using_swin_transformer_as_backbone/","text":"Using Swin Transformer as backbone in IceVision Exciting News: Now you can use both VFNet and RetinaNet with 3 different pretrained Swin Transformer backbones. Spectacular results with VFNet. COCO metric reached 66% vs 34% for RetinaNet when training the Fridge Object Dataset Both the neck (FPN) and the head are trained from scratch! Introduction Swin Transformer currently holds state-of-the-art results in prominent computer vision benchmark datasets: Image classification: 87.3 top-1 accuracy on ImageNet-1K Object detection: 58.7 box AP and 51.1 mask AP on COCO testdev Semantic segmentation: 53.5 mIoU on ADE20K val Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. IceVision now supports using Swin Transformer as backbone for some models. In this tutorial, you will learn how to: 1. Use the IceVision mmdet models with the Swin Transformer as backbone 2. Instantiate the model, and then train it with both the fastai and pytorch lightning engines. 3. And finally, use the model to identify objects in images. Install IceVision and IceData The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports All of the IceVision components can be easily imported with a single line. from icevision.all import * Download and prepare a dataset Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir ) Parse the dataset The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible autofix capability that identifies common errors in annotation files, reports, and often corrects them automatically. The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed. The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations. # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> Creating datasets with augmentations and transforms Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. # Transforms image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Understanding the transforms The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 ) Select a library, model, and backbone In order to create a model, we need to: * Choose one of the libraries supported by IceVision * Choose one of the models supported by the library * Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library. Creating a model Presently IceVision only supports the mmdet implementation of RetinaNet and VFNet (for object detection) and MaskRCNN (for instance segmentation) to use the Swin Transformer backbone. The model and backbone can be selected as follows: model_type = models.mmdet.vfnet backbone = model_type.backbones.swin_t_p4_w7_fpn_1x_coco There are currently 3 available backbones namely swin_t_p4_w7_fpn_1x_coco , swin_s_p4_w7_fpn_1x_coco and swin_b_p4_w7_fpn_1x_coco . # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . swin_t_p4_w7_fpn_1x_coco if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . swin_s_p4_w7_fpn_1x_coco if selection == 2 : model_type = models . mmdet . vfnet backbone = model_type . backbones . swin_b_p4_w7_fpn_1x_coco model_type , backbone , extra_args backbone . __dict__ Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) The current Swin Transformer implementation only provides pretrained weights for the backbone. Note: For the Swin Transformer model pretrained=True indicates that only the backbone is pretrained. The neck and the head of the model will be trained from scratch. Data Loader The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 ) Metrics The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning . Training using fastai learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find ( end_lr = 0.005 ) SuggestedLRs(valley=4.279797212802805e-05) learn . fine_tune ( 20 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 2.715207 2.318473 0.079426 00:03 epoch train_loss valid_loss COCOMetric time 0 2.378009 2.299067 0.043078 00:04 1 2.305085 2.170340 0.081073 00:04 2 2.240926 2.094887 0.142760 00:04 3 2.171739 2.070704 0.239026 00:04 4 2.124321 1.890665 0.315633 00:04 5 2.068015 1.825514 0.433803 00:04 6 2.004862 1.739451 0.419219 00:04 7 1.948798 1.746644 0.406612 00:04 8 1.900563 1.639521 0.513550 00:04 9 1.848227 1.623993 0.496804 00:04 10 1.803940 1.603808 0.550723 00:04 11 1.767351 1.545463 0.501292 00:04 12 1.725547 1.519361 0.598455 00:04 13 1.687097 1.539462 0.545318 00:04 14 1.658265 1.403311 0.653802 00:04 15 1.626446 1.466561 0.641094 00:04 16 1.593588 1.378689 0.674150 00:04 17 1.570487 1.356180 0.644711 00:04 18 1.551844 1.396569 0.623112 00:04 19 1.533261 1.419828 0.624261 00:04 In our tests, the VFNet + Swin-T backbone scored over 66% on the COCOMetric for this dataset. Not bad at all for a model with an untrained neck and head! Training using Pytorch Lightning class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -------------------------------- 0 | model | VFNet | 36.2 M -------------------------------- 36.2 M Trainable params 0 Non-trainable params 36.2 M Total params 144.622 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Using the model - inference and showing results The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 ) Prediction Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s] Happy Learning! If you need any assistance, feel free to join our forum .","title":"Using Swin Transformer as Backbone"},{"location":"using_swin_transformer_as_backbone/#using-swin-transformer-as-backbone-in-icevision","text":"Exciting News: Now you can use both VFNet and RetinaNet with 3 different pretrained Swin Transformer backbones. Spectacular results with VFNet. COCO metric reached 66% vs 34% for RetinaNet when training the Fridge Object Dataset Both the neck (FPN) and the head are trained from scratch!","title":"Using Swin Transformer as backbone in IceVision"},{"location":"using_swin_transformer_as_backbone/#introduction","text":"Swin Transformer currently holds state-of-the-art results in prominent computer vision benchmark datasets: Image classification: 87.3 top-1 accuracy on ImageNet-1K Object detection: 58.7 box AP and 51.1 mask AP on COCO testdev Semantic segmentation: 53.5 mIoU on ADE20K val Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. IceVision now supports using Swin Transformer as backbone for some models. In this tutorial, you will learn how to: 1. Use the IceVision mmdet models with the Swin Transformer as backbone 2. Instantiate the model, and then train it with both the fastai and pytorch lightning engines. 3. And finally, use the model to identify objects in images.","title":"Introduction"},{"location":"using_swin_transformer_as_backbone/#install-icevision-and-icedata","text":"The following downloads and runs a short shell script. The script installs IceVision, IceData, the MMDetection library, and Yolo v5 as well as the fastai and pytorch lightning engines. Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Install IceVision and IceData"},{"location":"using_swin_transformer_as_backbone/#imports","text":"All of the IceVision components can be easily imported with a single line. from icevision.all import *","title":"Imports"},{"location":"using_swin_transformer_as_backbone/#download-and-prepare-a-dataset","text":"Now we can start by downloading the Fridge Objects dataset. This tiny dataset contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides methods to load a dataset, parse annotation files, and more. For more information about how the fridge dataset as well as its corresponding parser, check out the fridge folder in icedata. # Download the dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir )","title":"Download and prepare a dataset"},{"location":"using_swin_transformer_as_backbone/#parse-the-dataset","text":"The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible autofix capability that identifies common errors in annotation files, reports, and often corrects them automatically. The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed. The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations. # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>","title":"Parse the dataset"},{"location":"using_swin_transformer_as_backbone/#creating-datasets-with-augmentations-and-transforms","text":"Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the Albumentations library for defining and executing transformations, but can be extended to use others. For this tutorial, we apply the Albumentation's default aug_tfms to the training set. aug_tfms randomly applies broadly useful transformations including rotation, cropping, horizontal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully. The validation set is only resized (with padding). We then create Datasets for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records. # Transforms image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Creating datasets with augmentations and transforms"},{"location":"using_swin_transformer_as_backbone/#understanding-the-transforms","text":"The Dataset transforms are only applied when we grab (get) an item. Several of the default aug_tfms have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation is randomly selected between +45 and -45 degrees. This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning. We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations. # Show an element of the train_ds with augmentation transformations applied samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 )","title":"Understanding the transforms"},{"location":"using_swin_transformer_as_backbone/#select-a-library-model-and-backbone","text":"In order to create a model, we need to: * Choose one of the libraries supported by IceVision * Choose one of the models supported by the library * Choose one of the backbones corresponding to a chosen model You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library.","title":"Select a library, model, and backbone"},{"location":"using_swin_transformer_as_backbone/#creating-a-model","text":"Presently IceVision only supports the mmdet implementation of RetinaNet and VFNet (for object detection) and MaskRCNN (for instance segmentation) to use the Swin Transformer backbone. The model and backbone can be selected as follows: model_type = models.mmdet.vfnet backbone = model_type.backbones.swin_t_p4_w7_fpn_1x_coco There are currently 3 available backbones namely swin_t_p4_w7_fpn_1x_coco , swin_s_p4_w7_fpn_1x_coco and swin_b_p4_w7_fpn_1x_coco . # Just change the value of selection to try another model selection = 0 extra_args = {} if selection == 0 : model_type = models . mmdet . vfnet backbone = model_type . backbones . swin_t_p4_w7_fpn_1x_coco if selection == 1 : model_type = models . mmdet . retinanet backbone = model_type . backbones . swin_s_p4_w7_fpn_1x_coco if selection == 2 : model_type = models . mmdet . vfnet backbone = model_type . backbones . swin_b_p4_w7_fpn_1x_coco model_type , backbone , extra_args backbone . __dict__ Now it is just a one-liner to instantiate the model. If you want to try another option , just edit the line at the top of the previous cell. # Instantiate the model model = model_type . model ( backbone = backbone ( pretrained = True ), num_classes = len ( parser . class_map ), ** extra_args ) The current Swin Transformer implementation only provides pretrained weights for the backbone. Note: For the Swin Transformer model pretrained=True indicates that only the backbone is pretrained. The neck and the head of the model will be trained from scratch.","title":"Creating a model"},{"location":"using_swin_transformer_as_backbone/#data-loader","text":"The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets. We can take a look at the first batch of items from the valid_dl . Remember that the valid_tfms only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training. # Data Loaders train_dl = model_type . train_dl ( train_ds , batch_size = 8 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 8 , num_workers = 4 , shuffle = False ) # show batch model_type . show_batch ( first ( valid_dl ), ncols = 4 )","title":"Data Loader"},{"location":"using_swin_transformer_as_backbone/#metrics","text":"The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries. The same metrics can be used for both fastai and pytorch lightning. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"using_swin_transformer_as_backbone/#training","text":"IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as fastai2 , and pytorch-lightning .","title":"Training"},{"location":"using_swin_transformer_as_backbone/#training-using-fastai","text":"learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . lr_find ( end_lr = 0.005 ) SuggestedLRs(valley=4.279797212802805e-05) learn . fine_tune ( 20 , 1e-4 , freeze_epochs = 1 ) epoch train_loss valid_loss COCOMetric time 0 2.715207 2.318473 0.079426 00:03 epoch train_loss valid_loss COCOMetric time 0 2.378009 2.299067 0.043078 00:04 1 2.305085 2.170340 0.081073 00:04 2 2.240926 2.094887 0.142760 00:04 3 2.171739 2.070704 0.239026 00:04 4 2.124321 1.890665 0.315633 00:04 5 2.068015 1.825514 0.433803 00:04 6 2.004862 1.739451 0.419219 00:04 7 1.948798 1.746644 0.406612 00:04 8 1.900563 1.639521 0.513550 00:04 9 1.848227 1.623993 0.496804 00:04 10 1.803940 1.603808 0.550723 00:04 11 1.767351 1.545463 0.501292 00:04 12 1.725547 1.519361 0.598455 00:04 13 1.687097 1.539462 0.545318 00:04 14 1.658265 1.403311 0.653802 00:04 15 1.626446 1.466561 0.641094 00:04 16 1.593588 1.378689 0.674150 00:04 17 1.570487 1.356180 0.644711 00:04 18 1.551844 1.396569 0.623112 00:04 19 1.533261 1.419828 0.624261 00:04 In our tests, the VFNet + Swin-T backbone scored over 66% on the COCOMetric for this dataset. Not bad at all for a model with an untrained neck and head!","title":"Training using fastai"},{"location":"using_swin_transformer_as_backbone/#training-using-pytorch-lightning","text":"class LightModel ( model_type . lightning . ModelAdapter ): def configure_optimizers ( self ): return Adam ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 5 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -------------------------------- 0 | model | VFNet | 36.2 M -------------------------------- 36.2 M Trainable params 0 Non-trainable params 36.2 M Total params 144.622 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s]","title":"Training using Pytorch Lightning"},{"location":"using_swin_transformer_as_backbone/#using-the-model-inference-and-showing-results","text":"The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the show_results function. model_type . show_results ( model , valid_ds , detection_threshold = .5 )","title":"Using the model - inference and showing results"},{"location":"using_swin_transformer_as_backbone/#prediction","text":"Sometimes you want to have more control than show_results provides. You can construct an inference dataloader using infer_dl from any IceVision dataset and pass this to predict_dl and use show_preds to look at the predictions. A prediction is returned as a dict with keys: scores , labels , bboxes , and possibly masks . Prediction functions that take a detection_threshold argument will only return the predictions whose score is above the threshold. Prediction functions that take a keep_images argument will only return the (tensor representation of the) image when it is True . In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves. NOTE: For a more detailed look at inference check out the inference tutorial infer_dl = model_type . infer_dl ( valid_ds , batch_size = 4 , shuffle = False ) preds = model_type . predict_from_dl ( model , infer_dl , keep_images = True ) show_preds ( preds = preds [: 4 ]) 0%| | 0/7 [00:00<?, ?it/s]","title":"Prediction"},{"location":"using_swin_transformer_as_backbone/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"voc_predefined_splits/","text":"How to parse a voc dataset using predefined splits Installing IceVision and IceData If on Colab run the following cell, else check the installation instructions Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) Imports from icevision.all import * Load Pascal VOC 2012 dataset path = icedata . voc . load_data () Set images, annotations and imagesets directories annotations_dir = path / \"Annotations\" images_dir = path / \"JPEGImages\" imagesets_dir = path / \"ImageSets/Main\" Define class_map class_map = icedata . voc . class_map () Split data using imagesets ImageSets directory contains text files containing subsets of the dataset. We will split our dataset using the train and validation sets for aeroplanes. ImageSets directory contains multiple text files containing subsets of images from JPEGImages. We can use these files to select subsets of our data ie aeroplanes. The values we need to pass to FixedSplitter are the values returned by record_id in our parser rather than the filenames. train = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_train.txt\" )] val = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_val.txt\" )] presplits = [ train , val ] data_splitter = FixedSplitter ( presplits ) Parser: use icevision predefined VOC parser parser = parsers . VOCBBoxParser ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) Train and validation records train_records , valid_records = parser . parse ( data_splitter , autofix = False ) show_records ( train_records [: 2 ], ncols = 2 ) 0%| | 0/17125 [00:00<?, ?it/s]","title":"Fixed Splitter"},{"location":"voc_predefined_splits/#how-to-parse-a-voc-dataset-using-predefined-splits","text":"","title":"How to parse a voc dataset using predefined splits"},{"location":"voc_predefined_splits/#installing-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True )","title":"Installing IceVision and IceData"},{"location":"voc_predefined_splits/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"voc_predefined_splits/#load-pascal-voc-2012-dataset","text":"path = icedata . voc . load_data ()","title":"Load Pascal VOC 2012 dataset"},{"location":"voc_predefined_splits/#set-images-annotations-and-imagesets-directories","text":"annotations_dir = path / \"Annotations\" images_dir = path / \"JPEGImages\" imagesets_dir = path / \"ImageSets/Main\"","title":"Set images, annotations and imagesets directories"},{"location":"voc_predefined_splits/#define-class_map","text":"class_map = icedata . voc . class_map ()","title":"Define class_map"},{"location":"voc_predefined_splits/#split-data-using-imagesets","text":"ImageSets directory contains text files containing subsets of the dataset. We will split our dataset using the train and validation sets for aeroplanes. ImageSets directory contains multiple text files containing subsets of images from JPEGImages. We can use these files to select subsets of our data ie aeroplanes. The values we need to pass to FixedSplitter are the values returned by record_id in our parser rather than the filenames. train = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_train.txt\" )] val = [( line . split ( \" \" , 1 )[ 0 ]) for line in open ( imagesets_dir / \"aeroplane_val.txt\" )] presplits = [ train , val ] data_splitter = FixedSplitter ( presplits )","title":"Split data using imagesets"},{"location":"voc_predefined_splits/#parser-use-icevision-predefined-voc-parser","text":"parser = parsers . VOCBBoxParser ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map )","title":"Parser: use icevision predefined VOC parser"},{"location":"voc_predefined_splits/#train-and-validation-records","text":"train_records , valid_records = parser . parse ( data_splitter , autofix = False ) show_records ( train_records [: 2 ], ncols = 2 ) 0%| | 0/17125 [00:00<?, ?it/s]","title":"Train and validation records"},{"location":"wandb_efficientdet/","text":"IceVision meets W&B IceVision + W&B = Agnostic Object Detection Framework with Outstanding Experiments Tracking IceVision fully supports W&B by providing a one-liner API that enables users to track their trained models and display both the predicted and ground truth bounding boxes. W&B makes visualizing and tracking different models performance a highly enjoyable task. Indeed, we are able to monitor the performance of several EfficientDet backbones by changing few lines of code and obtaining very intuitive and easy-to-interpret figures that highlights both the similarities and differences between the different backbones. For more information check the Report . Note, however, that the report refers to an older version of IceVision. This tutorial is updated for IceVision 0.7. This tutorial emphasizes the additional work required to integrated W&B. If you are new to IceVision, then we suggest that you look at the getting started with object detection tutorial . In this tutorial, we are using the fastai library training loop, the efficientdet object detection model, and a sample dataset with images of objects that you might find in a fridge. Following the usual practice with IceVision, you can use W&B with other training loops, model libraries, models and backbones. The W&B specific lines below would not need to be changed. Install IceVision and IceData If on Colab run the following cell, else check the installation instructions Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'} Imports from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... Load the Fridge Objects dataset The fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}> Train and Validation Datasets # Transforms image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Create the model In IceVision, we need to select the model type and backbone. For this tutorial, we are selecting efficientdet and the tf_lite0 backbone. Some models require additional information, such as the image_size . # Library and model selection model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 ( pretrained = True ) # The efficientdet model requires an img_size parameter extra_args = { 'img_size' : image_size } model = model_type . model ( backbone = backbone , num_classes = len ( parser . class_map ), ** extra_args ) Downloading: \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_lite0-f5f303a9.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientdet_lite0-f5f303a9.pth Create the dataloaders The dataloaders differ somewhat across the model_types, so creating them comes after selecting the model type. # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Training Intialize W&B At this point, we initialize W&B. This works in the jupyter notebook, but it is more typical run W&B from within a programme. This is partly because it enables you to track the progress of your training jobs from a custom dashboard from your browser, tablet, or phone. The full interface also makes it easy to compare multiple training runs, which can be very powerful when combined with IceVision. You can easily see which model is best suited to your problem. Initializing is a single line from the W&B library. wandb . init ( project = \"icevision-fridge\" , name = \"efficientdet_tf_lite0\" , reinit = True ) Create the learner This tutorial is using fastai , but IceVision lets you us other frameworks such as pytorch-lightning . In order to use W&B within fastai, you need to specify the WandbCallback , which results in logging the metrics as well as other key parameters, as well as the SaveModelCallback , which enables W&B to log the models. Logging the model is very powerful, as it ensures that you have a copy of the best version of the model as you train. If you are using W&B on-line, however, it causes your model to be transferred to the W&B database as well as saved in a local wandb directory. learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )], cbs = [ WandbCallback (), SaveModelCallback ()]) Train In this case, we use the fit_one_cycle training method from fastai, which uses a specific policy for adjusting the learning rate. This model is likely to take around 2-10 seconds per epoch, depending on your hardware. Training for 30 epochs on this small dataset typically reaches a level around 0.8 (COCOMetric), which is sufficient for our demonstration purposes and saves some time. learn . fit_one_cycle ( 30 , 1e-2 ) Could not gather input dimensions WandbCallback was not able to prepare a DataLoader for logging prediction samples -> 'Dataset' object has no attribute 'items' epoch train_loss valid_loss COCOMetric time 0 1.940700 1.267195 0.000069 00:12 1 1.685492 1.271025 0.000023 00:07 2 1.540221 1.264216 0.017094 00:07 3 1.424315 1.147959 0.155413 00:07 4 1.303290 1.082212 0.218895 00:07 5 1.184958 0.875454 0.310934 00:07 6 1.083239 0.831463 0.260008 00:07 7 0.993147 0.897370 0.278169 00:07 8 0.912498 0.779531 0.356216 00:07 9 0.844710 0.740413 0.360409 00:07 10 0.782819 0.723692 0.377731 00:07 11 0.729454 0.745544 0.323658 00:07 12 0.687574 0.653606 0.413525 00:07 13 0.651915 0.655265 0.417471 00:07 14 0.620716 0.651639 0.429272 00:07 15 0.589860 0.543764 0.468006 00:07 16 0.559970 0.516403 0.574289 00:07 17 0.531339 0.510701 0.534853 00:07 18 0.504182 0.490539 0.590217 00:07 19 0.482276 0.391926 0.669296 00:07 20 0.460442 0.391556 0.693068 00:07 21 0.444368 0.344799 0.738415 00:07 22 0.425590 0.352732 0.725862 00:07 23 0.409553 0.319861 0.769296 00:07 24 0.395036 0.308085 0.769391 00:07 25 0.383020 0.302642 0.775355 00:07 26 0.372328 0.293886 0.785769 00:07 27 0.362688 0.292389 0.796928 00:07 28 0.355098 0.289134 0.799669 00:07 29 0.347778 0.283973 0.806622 00:07 /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Better model found at epoch 0 with valid_loss value: 1.2671946287155151. Better model found at epoch 2 with valid_loss value: 1.2642161846160889. Better model found at epoch 3 with valid_loss value: 1.1479589939117432. Better model found at epoch 4 with valid_loss value: 1.0822120904922485. Better model found at epoch 5 with valid_loss value: 0.8754537105560303. Better model found at epoch 6 with valid_loss value: 0.8314631581306458. Better model found at epoch 8 with valid_loss value: 0.7795311808586121. Better model found at epoch 9 with valid_loss value: 0.7404130697250366. Better model found at epoch 10 with valid_loss value: 0.7236919403076172. Better model found at epoch 12 with valid_loss value: 0.6536057591438293. Better model found at epoch 14 with valid_loss value: 0.6516388654708862. Better model found at epoch 15 with valid_loss value: 0.5437638759613037. Better model found at epoch 16 with valid_loss value: 0.5164031982421875. Better model found at epoch 17 with valid_loss value: 0.5107009410858154. Better model found at epoch 18 with valid_loss value: 0.4905391037464142. Better model found at epoch 19 with valid_loss value: 0.3919256627559662. Better model found at epoch 20 with valid_loss value: 0.3915559649467468. Better model found at epoch 21 with valid_loss value: 0.34479889273643494. Better model found at epoch 23 with valid_loss value: 0.3198612332344055. Better model found at epoch 24 with valid_loss value: 0.3080853819847107. Better model found at epoch 25 with valid_loss value: 0.302642285823822. Better model found at epoch 26 with valid_loss value: 0.29388612508773804. Better model found at epoch 27 with valid_loss value: 0.2923893630504608. Better model found at epoch 28 with valid_loss value: 0.2891344130039215. Better model found at epoch 29 with valid_loss value: 0.28397276997566223. Show results We can now look athe results of the training in the notebook. model_type . show_results ( model , valid_ds ) /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Get predictions Let's get the list of predictions from our model. We do this by creating an infer_dl - a dataloader used for inference and then getting predictions from the data loader. Please note the keep_images=True . By default, the predictions include scores, labels, and bounding boxes. In our case, we want to keep the images so that we log them to W&B. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) 0%| | 0/4 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Log results to W&B Now comes the most important bit of this tutorial - actually logging the predictions to W&B. This takes one line specific to icevision and a second line to send the information to W&B. # Create wandb_images for each prediction wandb_images = wandb_img_preds ( preds , add_ground_truth = True ) # Log the wandb_images to wandb wandb . log ({ \"Predicted images\" : wandb_images }) After logging and finishing the training, it is good to mark the run as completed. This can take a few seconds, as we wait for the W&B processes to transfer data and finalize logging. # optional: mark the run as completed wandb . join () Happy Learning! If you need any assistance, feel free to join our forum .","title":"Model Tracking Using Wandb"},{"location":"wandb_efficientdet/#icevision-meets-wb","text":"IceVision + W&B = Agnostic Object Detection Framework with Outstanding Experiments Tracking IceVision fully supports W&B by providing a one-liner API that enables users to track their trained models and display both the predicted and ground truth bounding boxes. W&B makes visualizing and tracking different models performance a highly enjoyable task. Indeed, we are able to monitor the performance of several EfficientDet backbones by changing few lines of code and obtaining very intuitive and easy-to-interpret figures that highlights both the similarities and differences between the different backbones. For more information check the Report . Note, however, that the report refers to an older version of IceVision. This tutorial is updated for IceVision 0.7. This tutorial emphasizes the additional work required to integrated W&B. If you are new to IceVision, then we suggest that you look at the getting started with object detection tutorial . In this tutorial, we are using the fastai library training loop, the efficientdet object detection model, and a sample dataset with images of objects that you might find in a fridge. Following the usual practice with IceVision, you can use W&B with other training loops, model libraries, models and backbones. The W&B specific lines below would not need to be changed.","title":"IceVision meets W&amp;B"},{"location":"wandb_efficientdet/#install-icevision-and-icedata","text":"If on Colab run the following cell, else check the installation instructions Install from pypi... # # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation # !wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh # # Choose your installation target: cuda11 or cuda10 or cpu # !bash icevision_install.sh cuda11 ... or from icevision master # Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation ! wget https : // raw . githubusercontent . com / airctic / icevision / master / icevision_install . sh # Choose your installation target: cuda11 or cuda10 or cpu ! bash icevision_install . sh cuda11 master # Restart kernel after installation import IPython IPython . Application . instance () . kernel . do_shutdown ( True ) {'restart': True, 'status': 'ok'}","title":"Install IceVision and IceData"},{"location":"wandb_efficientdet/#imports","text":"from icevision.all import * from fastai.callback.wandb import * from fastai.callback.tracker import SaveModelCallback \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /root/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m70\u001b[0m \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading mmdet configs\u001b[0m | \u001b[36micevision.models.mmdet.download_configs\u001b[0m:\u001b[36mdownload_mmdet_configs\u001b[0m:\u001b[36m31\u001b[0m 0B [00:00, ?B/s] Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...","title":"Imports"},{"location":"wandb_efficientdet/#load-the-fridge-objects-dataset","text":"The fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Dataset url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Create the parser parser = parsers . VOCBBoxParser ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" ) # Parse annotations to create records train_records , valid_records = parser . parse () parser . class_map 0%| | 0/20380998 [00:00<?, ?B/s] 0%| | 0/128 [00:00<?, ?it/s] \u001b[1m\u001b[1mINFO \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m122\u001b[0m 0%| | 0/128 [00:00<?, ?it/s] <ClassMap: {'background': 0, 'carton': 1, 'milk_bottle': 2, 'can': 3, 'water_bottle': 4}>","title":"Load the Fridge Objects dataset"},{"location":"wandb_efficientdet/#train-and-validation-datasets","text":"# Transforms image_size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = image_size , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( image_size ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Datasets"},{"location":"wandb_efficientdet/#create-the-model","text":"In IceVision, we need to select the model type and backbone. For this tutorial, we are selecting efficientdet and the tf_lite0 backbone. Some models require additional information, such as the image_size . # Library and model selection model_type = models . ross . efficientdet backbone = model_type . backbones . tf_lite0 ( pretrained = True ) # The efficientdet model requires an img_size parameter extra_args = { 'img_size' : image_size } model = model_type . model ( backbone = backbone , num_classes = len ( parser . class_map ), ** extra_args ) Downloading: \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_lite0-f5f303a9.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientdet_lite0-f5f303a9.pth","title":"Create the model"},{"location":"wandb_efficientdet/#create-the-dataloaders","text":"The dataloaders differ somewhat across the model_types, so creating them comes after selecting the model type. # DataLoaders train_dl = model_type . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model_type . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked))","title":"Create the dataloaders"},{"location":"wandb_efficientdet/#training","text":"","title":"Training"},{"location":"wandb_efficientdet/#intialize-wb","text":"At this point, we initialize W&B. This works in the jupyter notebook, but it is more typical run W&B from within a programme. This is partly because it enables you to track the progress of your training jobs from a custom dashboard from your browser, tablet, or phone. The full interface also makes it easy to compare multiple training runs, which can be very powerful when combined with IceVision. You can easily see which model is best suited to your problem. Initializing is a single line from the W&B library. wandb . init ( project = \"icevision-fridge\" , name = \"efficientdet_tf_lite0\" , reinit = True )","title":"Intialize W&amp;B"},{"location":"wandb_efficientdet/#create-the-learner","text":"This tutorial is using fastai , but IceVision lets you us other frameworks such as pytorch-lightning . In order to use W&B within fastai, you need to specify the WandbCallback , which results in logging the metrics as well as other key parameters, as well as the SaveModelCallback , which enables W&B to log the models. Logging the model is very powerful, as it ensures that you have a copy of the best version of the model as you train. If you are using W&B on-line, however, it causes your model to be transferred to the W&B database as well as saved in a local wandb directory. learn = model_type . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )], cbs = [ WandbCallback (), SaveModelCallback ()])","title":"Create the learner"},{"location":"wandb_efficientdet/#train","text":"In this case, we use the fit_one_cycle training method from fastai, which uses a specific policy for adjusting the learning rate. This model is likely to take around 2-10 seconds per epoch, depending on your hardware. Training for 30 epochs on this small dataset typically reaches a level around 0.8 (COCOMetric), which is sufficient for our demonstration purposes and saves some time. learn . fit_one_cycle ( 30 , 1e-2 ) Could not gather input dimensions WandbCallback was not able to prepare a DataLoader for logging prediction samples -> 'Dataset' object has no attribute 'items' epoch train_loss valid_loss COCOMetric time 0 1.940700 1.267195 0.000069 00:12 1 1.685492 1.271025 0.000023 00:07 2 1.540221 1.264216 0.017094 00:07 3 1.424315 1.147959 0.155413 00:07 4 1.303290 1.082212 0.218895 00:07 5 1.184958 0.875454 0.310934 00:07 6 1.083239 0.831463 0.260008 00:07 7 0.993147 0.897370 0.278169 00:07 8 0.912498 0.779531 0.356216 00:07 9 0.844710 0.740413 0.360409 00:07 10 0.782819 0.723692 0.377731 00:07 11 0.729454 0.745544 0.323658 00:07 12 0.687574 0.653606 0.413525 00:07 13 0.651915 0.655265 0.417471 00:07 14 0.620716 0.651639 0.429272 00:07 15 0.589860 0.543764 0.468006 00:07 16 0.559970 0.516403 0.574289 00:07 17 0.531339 0.510701 0.534853 00:07 18 0.504182 0.490539 0.590217 00:07 19 0.482276 0.391926 0.669296 00:07 20 0.460442 0.391556 0.693068 00:07 21 0.444368 0.344799 0.738415 00:07 22 0.425590 0.352732 0.725862 00:07 23 0.409553 0.319861 0.769296 00:07 24 0.395036 0.308085 0.769391 00:07 25 0.383020 0.302642 0.775355 00:07 26 0.372328 0.293886 0.785769 00:07 27 0.362688 0.292389 0.796928 00:07 28 0.355098 0.289134 0.799669 00:07 29 0.347778 0.283973 0.806622 00:07 /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes Better model found at epoch 0 with valid_loss value: 1.2671946287155151. Better model found at epoch 2 with valid_loss value: 1.2642161846160889. Better model found at epoch 3 with valid_loss value: 1.1479589939117432. Better model found at epoch 4 with valid_loss value: 1.0822120904922485. Better model found at epoch 5 with valid_loss value: 0.8754537105560303. Better model found at epoch 6 with valid_loss value: 0.8314631581306458. Better model found at epoch 8 with valid_loss value: 0.7795311808586121. Better model found at epoch 9 with valid_loss value: 0.7404130697250366. Better model found at epoch 10 with valid_loss value: 0.7236919403076172. Better model found at epoch 12 with valid_loss value: 0.6536057591438293. Better model found at epoch 14 with valid_loss value: 0.6516388654708862. Better model found at epoch 15 with valid_loss value: 0.5437638759613037. Better model found at epoch 16 with valid_loss value: 0.5164031982421875. Better model found at epoch 17 with valid_loss value: 0.5107009410858154. Better model found at epoch 18 with valid_loss value: 0.4905391037464142. Better model found at epoch 19 with valid_loss value: 0.3919256627559662. Better model found at epoch 20 with valid_loss value: 0.3915559649467468. Better model found at epoch 21 with valid_loss value: 0.34479889273643494. Better model found at epoch 23 with valid_loss value: 0.3198612332344055. Better model found at epoch 24 with valid_loss value: 0.3080853819847107. Better model found at epoch 25 with valid_loss value: 0.302642285823822. Better model found at epoch 26 with valid_loss value: 0.29388612508773804. Better model found at epoch 27 with valid_loss value: 0.2923893630504608. Better model found at epoch 28 with valid_loss value: 0.2891344130039215. Better model found at epoch 29 with valid_loss value: 0.28397276997566223.","title":"Train"},{"location":"wandb_efficientdet/#show-results","text":"We can now look athe results of the training in the notebook. model_type . show_results ( model , valid_ds ) /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes","title":"Show results"},{"location":"wandb_efficientdet/#get-predictions","text":"Let's get the list of predictions from our model. We do this by creating an infer_dl - a dataloader used for inference and then getting predictions from the data loader. Please note the keep_images=True . By default, the predictions include scores, labels, and bounding boxes. In our case, we want to keep the images so that we log them to W&B. infer_dl = model_type . infer_dl ( valid_ds , batch_size = 8 ) preds = model_type . predict_from_dl ( model = model , infer_dl = infer_dl , keep_images = True ) 0%| | 0/4 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/effdet/bench.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). indices_all = cls_topk_indices_all // num_classes","title":"Get predictions"},{"location":"wandb_efficientdet/#log-results-to-wb","text":"Now comes the most important bit of this tutorial - actually logging the predictions to W&B. This takes one line specific to icevision and a second line to send the information to W&B. # Create wandb_images for each prediction wandb_images = wandb_img_preds ( preds , add_ground_truth = True ) # Log the wandb_images to wandb wandb . log ({ \"Predicted images\" : wandb_images }) After logging and finishing the training, it is good to mark the run as completed. This can take a few seconds, as we wait for the W&B processes to transfer data and finalize logging. # optional: mark the run as completed wandb . join ()","title":"Log results to W&amp;B"},{"location":"wandb_efficientdet/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"}]}