{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial does not cover basic concepts, be sure to read the Introduction Guide before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgvaz/anaconda3/envs/light2/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\n",
      "/home/lgvaz/anaconda3/envs/light2/lib/python3.7/site-packages/trains/backend_interface/metrics/events.py:27: DeprecationWarning: The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\n",
      "  @attr.attrs(cmp=False, slots=True)\n"
     ]
    }
   ],
   "source": [
    "from mantisshrimp.imports import *\n",
    "from mantisshrimp import *\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip to the next section if you just want to read on how to use custom backbones, this section is only required if you're running the complete notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue using the pets dataset and grab everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = datasets.pets.load()\n",
    "parser = datasets.pets.parser(image_dir, mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a 80% train 20% valid data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab4a3c6a41342f2bc182e089f1089c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3686.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_splitter = RandomSplitter([.8, .2])\n",
    "train_records, valid_records = parser.parse(data_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can quickly setup some transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean, imagenet_std = imagenet_stats\n",
    "\n",
    "valid_tfms = AlbuTransform(\n",
    "    [\n",
    "        A.LongestMaxSize(384),\n",
    "        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_tfms = AlbuTransform(\n",
    "    [\n",
    "        A.LongestMaxSize(384),\n",
    "        A.RandomSizedBBoxSafeCrop(320, 320, p=0.3),\n",
    "        A.HorizontalFlip(),\n",
    "        A.ShiftScaleRotate(rotate_limit=20),\n",
    "        A.RGBShift(always_apply=True),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.Blur(blur_limit=(1, 3)),\n",
    "        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup done, we're now ready to start exploring custom backbones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with FPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Pyramid Network (FPN) was an additional made to orginal Faster RCNN paper in 2017.\n",
    "\n",
    "- For more information please check this orignal paper for Faster RCNN with FPN.\n",
    "\n",
    "- These lead to improvement in performance in Faster RCNN.\n",
    "\n",
    "- Mantrisshrimp supports various Resnet styled architectures as backbones for FPN.\n",
    "\n",
    "- Is supports backbones such as \"resnet18\", \"resnet34\",\"resnet50\", \"resnet101\", \"resnet152\", \"resnext50_32x4d\", \"resnext101_32x8d\", \"wide_resnet50_2\", \"wide_resnet101_2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passing ```pretrained=True``` will create backbone trained on ImageNet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_101_backbone = MantisFasterRCNN.get_backbone_by_name(\"resnet101\", fpn=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_152_backbone = MantisFasterRCNN.get_backbone_by_name(\"resnet152\", fpn=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models without FPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The orignal implementation of Faster RCNN as in year 2014 did not have FPN.\n",
    "\n",
    "- These architectures do not use FPN and they too can have multiple backbones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mantisshrimp supports backbones \"resnet18\", \"resnet34\", \"resnet50\",\"resnet101\", \"resnet152\", \"resnext101_32x8d\", \"mobilenet\", \"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\", without fpn networks\n",
    "\n",
    "- Mantrisshrimp thus supports all architectures which can be used with FPN as well as additional CNN models as well.\n",
    "\n",
    "- For now as an example let us instantiate with mobilenetv2 backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Passing ```pretrained=True``` will create backbone trained on ImageNet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2_backbone = MantisFasterRCNN.get_backbone_by_name(\"mobilenet\", fpn=False, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can pass all the arguments that you could for torchvision Faster RCNN, E.g. Anchor boxes, iou_threshold, etc.\n",
    "- Have a look at those arguments in torchvision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the custom backbone, just pass it as the `backbone` argument to the model, simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MantisFasterRCNN(num_classes=2, backbone=mobilenet_v2_backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, continue the rest of the steps normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = model.dataloader(train_ds, shuffle=True, batch_size=8, num_workers=2)\n",
    "valid_dl = model.dataloader(valid_ds, batch_size=8, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's use fastai for training this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantisshrimp.engines.fastai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = rcnn_learner(dls=[train_dl, valid_dl], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fine_tune(20, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more customized backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note while using these customization make sure you understand them. It will give errors while training if they are not properly adjusted.\n",
    "- So, make sure your parameters work as per data and model.\n",
    "- You can pass the same arguments for torchvision FasterRCNN as well.\n",
    "- These torchvision parameters work for both models with FPN and without FPN networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what all can be customized. Parameters that are set to None, take defualt values as in torchvision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "num_classes=None,\n",
    "# transform parameters\n",
    "min_size=800, max_size=1333,\n",
    "image_mean=None, image_std=None,\n",
    "\n",
    "# RPN parameters\n",
    "rpn_anchor_generator=None, rpn_head=None, rpn_pre_nms_top_n_train=2000, \n",
    "rpn_pre_nms_top_n_test=1000, rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, \n",
    "rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "\n",
    "\n",
    "# Box parameters\n",
    "box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, \n",
    "box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, box_batch_size_per_image=512, \n",
    "box_positive_fraction=0.25, bbox_reg_weights=None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do check these in torchvision https://pytorch.org/docs/stable/_modules/torchvision/models/detection/faster_rcnn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say you need modify AnchorGenerator.\n",
    "\n",
    "(I am modifying it to default value, which would be set if it would be None here. You can experiment with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "ft_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And if we need a Region of Interest Pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "                featmap_names=['0', '1', '2', '3'],\n",
    "                output_size=7,\n",
    "                sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also if you need image mean and std specific for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imagenet mean and std it will taken automatically if not explicity given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_mean = [0.485, 0.456, 0.406] # ImageNet mean\n",
    "ft_std = [0.229, 0.224, 0.225] # ImageNet std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just pass them while you instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MantisFasterRCNN(num_class=2, backbone=resnet_101_backbone, image_mean=ft_mean, image_std=ft_std, rpn_anchor_generator=ft_anchor_generator, box_roi_pool=ft_roi_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is just a plain pytorch model, so it can be saved normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mantiss_faster_rcnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's really simple to create models with custom backbones\n",
    "- You don't have to worry how backbones should be connected with Faster RCNN, Mantisshrimp does that for you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
